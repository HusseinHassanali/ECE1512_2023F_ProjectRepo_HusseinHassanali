{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Initial Imports and Drive Mounting**"
      ],
      "metadata": {
        "id": "psdW4GfZmqQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main code adapted from here: https://github.com/VICO-UoE/DatasetCondensation"
      ],
      "metadata": {
        "id": "UFURVjYUjHH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSClkdUR8oOe",
        "outputId": "5e998938-4789-408e-e938-fd056475671f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjFzBn6CGJoc",
        "outputId": "92f93501-689a-44e6-f6b4-e915ca3f3d35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-7863e5de89a2>:12: DeprecationWarning: Please use `rotate` from the `scipy.ndimage` namespace, the `scipy.ndimage.interpolation` namespace is deprecated.\n",
            "  from scipy.ndimage.interpolation import rotate as scipyrotate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n",
            "Collecting import-ipynb\n",
            "  Downloading import_ipynb-0.1.4-py3-none-any.whl (4.1 kB)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.10/dist-packages (from import-ipynb) (7.34.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from import-ipynb) (5.9.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from IPython->import-ipynb) (67.7.2)\n",
            "Collecting jedi>=0.16 (from IPython->import-ipynb)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from IPython->import-ipynb) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from IPython->import-ipynb) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from IPython->import-ipynb) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from IPython->import-ipynb) (3.0.41)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from IPython->import-ipynb) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from IPython->import-ipynb) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from IPython->import-ipynb) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from IPython->import-ipynb) (4.9.0)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat->import-ipynb) (2.19.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->import-ipynb) (4.19.2)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.10/dist-packages (from nbformat->import-ipynb) (5.5.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->IPython->import-ipynb) (0.8.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (2023.11.2)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (0.31.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (0.13.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->IPython->import-ipynb) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython->import-ipynb) (0.2.12)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core->nbformat->import-ipynb) (4.0.0)\n",
            "Installing collected packages: jedi, import-ipynb\n",
            "Successfully installed import-ipynb-0.1.4 jedi-0.19.1\n",
            "importing Jupyter notebook from networks.ipynb\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import copy\n",
        "import argparse\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.utils import save_image\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from scipy.ndimage.interpolation import rotate as scipyrotate\n",
        "\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import cv2\n",
        "\n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "\n",
        "\n",
        "%cd \"/content/drive/MyDrive/Colab Notebooks/\"\n",
        "\n",
        "!pip install import-ipynb\n",
        "import import_ipynb\n",
        "\n",
        "from networks import MLP, ConvNet, LeNet, AlexNet, AlexNetBN, VGG11, VGG11BN, ResNet18, ResNet18BN_AP, ResNet18BN\n",
        "\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "# from utils import get_loops, get_dataset, get_network, get_eval_pool, evaluate_synset, get_daparam, match_loss, get_time, TensorDataset, epoch, DiffAugment, ParamDiffAug"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Helper Functions**"
      ],
      "metadata": {
        "id": "LjMwYcUXmznn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BM4hVMpzME2c"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, images, targets, transform=None):\n",
        "        self.data = torch.from_numpy(images).float()\n",
        "        self.targets = torch.from_numpy(targets)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.targets[index]\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "def get_dataset(dataset, data_path):\n",
        "    if dataset == 'MNIST':\n",
        "        channel = 1\n",
        "        im_size = (28, 28)\n",
        "        num_classes = 10\n",
        "        mean = [0.1307]\n",
        "        std = [0.3081]\n",
        "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n",
        "        dst_train = datasets.MNIST(data_path, train=True, download=True, transform=transform) # no augmentation\n",
        "        dst_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)\n",
        "        class_names = [str(c) for c in range(num_classes)]\n",
        "\n",
        "    elif dataset == 'MNIST Custom':\n",
        "        channel = 1\n",
        "        im_size = (28, 28)\n",
        "        num_classes = 10\n",
        "        mean = [0.1307]\n",
        "        std = [0.3081]\n",
        "\n",
        "        transform = transforms.Compose([transforms.ToTensor(), transforms.Grayscale(), transforms.Normalize(mean=mean, std=std)])\n",
        "\n",
        "        dataset_root = \"/content/drive/MyDrive/ECE1512/Project_B/Runs/MNIST Custom/\"\n",
        "        dst_train = datasets.ImageFolder(root=dataset_root, transform=transform)\n",
        "\n",
        "        dst_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)\n",
        "\n",
        "        class_names = [str(c) for c in range(num_classes)]\n",
        "\n",
        "\n",
        "    elif dataset == 'MHIST Custom':\n",
        "        channel = 3\n",
        "        im_size = (128, 128)\n",
        "        num_classes = 2\n",
        "        mean = [188, 165, 197]\n",
        "        std = [45, 58, 38]\n",
        "\n",
        "        class_names = [\"HP\", \"SSA\"]\n",
        "\n",
        "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n",
        "        transformTrain = transforms.Compose([transforms.Resize(im_size), transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n",
        "\n",
        "        dataset_root = \"/content/drive/MyDrive/ECE1512/Project_B/Runs/MHIST Custom/\"\n",
        "        dst_train = datasets.ImageFolder(root=dataset_root, transform=transformTrain)\n",
        "\n",
        "        test_images = np.load(\"/content/drive/MyDrive/ECE1512/test_images.npy\").astype(np.uint8)\n",
        "        test_labels = np.load(\"/content/drive/MyDrive/ECE1512/test_labels.npy\").astype(np.uint8)\n",
        "\n",
        "        test_images = np.asarray(test_images)\n",
        "        test_labels = np.asarray(test_labels)\n",
        "\n",
        "        test_images = np.transpose(test_images, (0, 3, 1, 2))\n",
        "\n",
        "        test_images_resized = []\n",
        "\n",
        "        for image in test_images:\n",
        "          newImg = np.transpose(image, (1, 2, 0))\n",
        "          newImg = cv2.resize(newImg, im_size)\n",
        "\n",
        "          newImg = (newImg-np.min(newImg))/(np.max(newImg)-np.min(newImg))\n",
        "\n",
        "          newImg = np.transpose(newImg, (2, 0, 1))\n",
        "          test_images_resized.append(newImg)\n",
        "\n",
        "        test_images_resized = np.asarray(test_images_resized)\n",
        "\n",
        "        print(test_images_resized.shape)\n",
        "\n",
        "        dst_test = CustomDataset(test_images_resized, test_labels, transform=transform)\n",
        "\n",
        "        class_names = [\"HP\", \"SSA\"]\n",
        "\n",
        "    elif dataset == 'MHIST':\n",
        "\n",
        "        use_existing_arrays = True\n",
        "\n",
        "        if not(use_existing_arrays):\n",
        "          csv_path = \"/content/drive/MyDrive/ECE1512/Project_B/MHIST Dataset/images/labels.csv\"\n",
        "          data_df = pd.read_csv(csv_path)\n",
        "\n",
        "          image_dir = \"/content/drive/MyDrive/ECE1512/Project_B/MHIST Dataset/images/images/\"\n",
        "\n",
        "          label_encoder = LabelEncoder()\n",
        "          data_df['Majority Vote Label'] = label_encoder.fit_transform(data_df['Majority Vote Label'])\n",
        "\n",
        "          train_data = data_df[data_df['Partition'] == 'train']\n",
        "          test_data = data_df[data_df['Partition'] == 'test']\n",
        "\n",
        "          train_images = []\n",
        "          train_labels = []\n",
        "\n",
        "          # Load images and labels for training data\n",
        "          for _, row in train_data.iterrows():\n",
        "              filename = row['Image Name']\n",
        "              label = row['Majority Vote Label']\n",
        "              image_path = os.path.join(image_dir, filename)\n",
        "\n",
        "              image = Image.open(image_path)\n",
        "              image_array = np.asarray(image)\n",
        "\n",
        "              train_images.append(image_array)\n",
        "              train_labels.append(label)\n",
        "\n",
        "          # Load images and labels for testing data\n",
        "          test_images = []\n",
        "          test_labels = []\n",
        "          for _, row in test_data.iterrows():\n",
        "              filename = row['Image Name']\n",
        "              label = row['Majority Vote Label']\n",
        "              image_path = os.path.join(image_dir, filename)\n",
        "\n",
        "              image = Image.open(image_path)\n",
        "              image_array = np.asarray(image)\n",
        "\n",
        "              test_images.append(image_array)\n",
        "              test_labels.append(label)\n",
        "        else:\n",
        "          train_images = np.load(\"/content/drive/MyDrive/ECE1512/train_images.npy\").astype(np.uint8)\n",
        "          test_images = np.load(\"/content/drive/MyDrive/ECE1512/test_images.npy\").astype(np.uint8)\n",
        "          train_labels = np.load(\"/content/drive/MyDrive/ECE1512/train_labels.npy\").astype(np.uint8)\n",
        "          test_labels = np.load(\"/content/drive/MyDrive/ECE1512/test_labels.npy\").astype(np.uint8)\n",
        "\n",
        "        train_images = np.asarray(train_images)\n",
        "        train_labels = np.asarray(train_labels)\n",
        "        test_images = np.asarray(test_images)\n",
        "        test_labels = np.asarray(test_labels)\n",
        "\n",
        "        train_images = np.transpose(train_images, (0, 3, 1, 2))\n",
        "        test_images = np.transpose(test_images, (0, 3, 1, 2))\n",
        "\n",
        "        print(\"done loading images\")\n",
        "\n",
        "        channel = 3\n",
        "        im_size = (128, 128)\n",
        "        num_classes = 2\n",
        "        mean = [188, 165, 197]\n",
        "        std = [45, 58, 38]\n",
        "\n",
        "        # print(train_images.shape)\n",
        "\n",
        "        # # Reshape the array and resize images\n",
        "        train_images_resized = []\n",
        "\n",
        "        for image in train_images:\n",
        "          newImg = np.transpose(image, (1, 2, 0))\n",
        "          newImg = cv2.resize(newImg, im_size)\n",
        "\n",
        "          newImg = (newImg-np.min(newImg))/(np.max(newImg)-np.min(newImg))\n",
        "\n",
        "\n",
        "          # newImg = (newImg - mean)/std\n",
        "\n",
        "          newImg = np.transpose(newImg, (2, 0, 1))\n",
        "          train_images_resized.append(newImg)\n",
        "\n",
        "        train_images_resized = np.asarray(train_images_resized)\n",
        "\n",
        "        test_images_resized = []\n",
        "\n",
        "        for image in test_images:\n",
        "          newImg = np.transpose(image, (1, 2, 0))\n",
        "          newImg = cv2.resize(newImg, im_size)\n",
        "\n",
        "          newImg = (newImg-np.min(newImg))/(np.max(newImg)-np.min(newImg))\n",
        "\n",
        "\n",
        "          # newImg = (newImg - mean)/std\n",
        "\n",
        "          newImg = np.transpose(newImg, (2, 0, 1))\n",
        "          test_images_resized.append(newImg)\n",
        "\n",
        "        test_images_resized = np.asarray(test_images_resized)\n",
        "\n",
        "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n",
        "\n",
        "        dst_train = CustomDataset(train_images_resized, train_labels, transform=transform)\n",
        "\n",
        "        dst_test = CustomDataset(test_images_resized, test_labels, transform=transform)\n",
        "\n",
        "        class_names = [\"HP\", \"SSA\"]\n",
        "\n",
        "    elif dataset == 'FashionMNIST':\n",
        "        channel = 1\n",
        "        im_size = (28, 28)\n",
        "        num_classes = 10\n",
        "        mean = [0.2861]\n",
        "        std = [0.3530]\n",
        "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n",
        "        dst_train = datasets.FashionMNIST(data_path, train=True, download=True, transform=transform) # no augmentation\n",
        "        dst_test = datasets.FashionMNIST(data_path, train=False, download=True, transform=transform)\n",
        "        class_names = dst_train.classes\n",
        "\n",
        "    elif dataset == 'SVHN':\n",
        "        channel = 3\n",
        "        im_size = (32, 32)\n",
        "        num_classes = 10\n",
        "        mean = [0.4377, 0.4438, 0.4728]\n",
        "        std = [0.1980, 0.2010, 0.1970]\n",
        "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n",
        "        dst_train = datasets.SVHN(data_path, split='train', download=True, transform=transform)  # no augmentation\n",
        "        dst_test = datasets.SVHN(data_path, split='test', download=True, transform=transform)\n",
        "        class_names = [str(c) for c in range(num_classes)]\n",
        "\n",
        "    elif dataset == 'CIFAR10':\n",
        "        channel = 3\n",
        "        im_size = (32, 32)\n",
        "        num_classes = 10\n",
        "        mean = [0.4914, 0.4822, 0.4465]\n",
        "        std = [0.2023, 0.1994, 0.2010]\n",
        "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n",
        "        dst_train = datasets.CIFAR10(data_path, train=True, download=True, transform=transform) # no augmentation\n",
        "        dst_test = datasets.CIFAR10(data_path, train=False, download=True, transform=transform)\n",
        "        class_names = dst_train.classes\n",
        "\n",
        "    elif dataset == 'CIFAR100':\n",
        "        channel = 3\n",
        "        im_size = (32, 32)\n",
        "        num_classes = 100\n",
        "        mean = [0.5071, 0.4866, 0.4409]\n",
        "        std = [0.2673, 0.2564, 0.2762]\n",
        "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n",
        "        dst_train = datasets.CIFAR100(data_path, train=True, download=True, transform=transform) # no augmentation\n",
        "        dst_test = datasets.CIFAR100(data_path, train=False, download=True, transform=transform)\n",
        "        class_names = dst_train.classes\n",
        "\n",
        "    elif dataset == 'TinyImageNet':\n",
        "        channel = 3\n",
        "        im_size = (64, 64)\n",
        "        num_classes = 200\n",
        "        mean = [0.485, 0.456, 0.406]\n",
        "        std = [0.229, 0.224, 0.225]\n",
        "        data = torch.load(os.path.join(data_path, 'tinyimagenet.pt'), map_location='cpu')\n",
        "\n",
        "        class_names = data['classes']\n",
        "\n",
        "        images_train = data['images_train']\n",
        "        labels_train = data['labels_train']\n",
        "        images_train = images_train.detach().float() / 255.0\n",
        "        labels_train = labels_train.detach()\n",
        "        for c in range(channel):\n",
        "            images_train[:,c] = (images_train[:,c] - mean[c])/std[c]\n",
        "        dst_train = TensorDataset(images_train, labels_train)  # no augmentation\n",
        "\n",
        "        images_val = data['images_val']\n",
        "        labels_val = data['labels_val']\n",
        "        images_val = images_val.detach().float() / 255.0\n",
        "        labels_val = labels_val.detach()\n",
        "\n",
        "        for c in range(channel):\n",
        "            images_val[:, c] = (images_val[:, c] - mean[c]) / std[c]\n",
        "\n",
        "        dst_test = TensorDataset(images_val, labels_val)  # no augmentation\n",
        "\n",
        "    else:\n",
        "        exit('unknown dataset: %s'%dataset)\n",
        "\n",
        "\n",
        "    testloader = torch.utils.data.DataLoader(dst_test, batch_size=256, shuffle=False, num_workers=0)\n",
        "    return channel, im_size, num_classes, class_names, mean, std, dst_train, dst_test, testloader\n",
        "\n",
        "\n",
        "\n",
        "class TensorDataset(Dataset):\n",
        "    def __init__(self, images, labels): # images: n x c x h x w tensor\n",
        "        self.images = images.detach().float()\n",
        "        self.labels = labels.detach()\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.images[index], self.labels[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.images.shape[0]\n",
        "\n",
        "\n",
        "\n",
        "def get_default_convnet_setting():\n",
        "    net_width, net_depth, net_act, net_norm, net_pooling = 128, 3, 'relu', 'instancenorm', 'avgpooling'\n",
        "    return net_width, net_depth, net_act, net_norm, net_pooling\n",
        "\n",
        "\n",
        "\n",
        "def get_network(model, channel, num_classes, im_size=(32, 32)):\n",
        "    torch.random.manual_seed(int(time.time() * 1000) % 100000)\n",
        "    net_width, net_depth, net_act, net_norm, net_pooling = get_default_convnet_setting()\n",
        "\n",
        "    if model == 'MLP':\n",
        "        net = MLP(channel=channel, num_classes=num_classes)\n",
        "    elif model == 'ConvNet':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
        "    elif model == 'LeNet':\n",
        "        net = LeNet(channel=channel, num_classes=num_classes)\n",
        "    elif model == 'AlexNet':\n",
        "        net = AlexNet(channel=channel, num_classes=num_classes)\n",
        "    elif model == 'AlexNetBN':\n",
        "        net = AlexNetBN(channel=channel, num_classes=num_classes)\n",
        "    elif model == 'VGG11':\n",
        "        net = VGG11( channel=channel, num_classes=num_classes)\n",
        "    elif model == 'VGG11BN':\n",
        "        net = VGG11BN(channel=channel, num_classes=num_classes)\n",
        "    elif model == 'ResNet18':\n",
        "        net = ResNet18(channel=channel, num_classes=num_classes)\n",
        "    elif model == 'ResNet18BN_AP':\n",
        "        net = ResNet18BN_AP(channel=channel, num_classes=num_classes)\n",
        "    elif model == 'ResNet18BN':\n",
        "        net = ResNet18BN(channel=channel, num_classes=num_classes)\n",
        "\n",
        "    elif model == 'ConvNetD1':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=1, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
        "    elif model == 'ConvNetD2':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=2, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
        "    elif model == 'ConvNetD3':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=3, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
        "    elif model == 'ConvNetD4':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=4, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
        "\n",
        "    elif model == 'ConvNetW32':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=32, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
        "    elif model == 'ConvNetW64':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=64, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
        "    elif model == 'ConvNetW128':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=128, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
        "    elif model == 'ConvNetW256':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=256, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
        "\n",
        "    elif model == 'ConvNetAS':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act='sigmoid', net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
        "    elif model == 'ConvNetAR':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act='relu', net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
        "    elif model == 'ConvNetAL':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act='leakyrelu', net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
        "    elif model == 'ConvNetASwish':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act='swish', net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
        "    elif model == 'ConvNetASwishBN':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act='swish', net_norm='batchnorm', net_pooling=net_pooling, im_size=im_size)\n",
        "\n",
        "    elif model == 'ConvNetNN':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm='none', net_pooling=net_pooling, im_size=im_size)\n",
        "    elif model == 'ConvNetBN':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm='batchnorm', net_pooling=net_pooling, im_size=im_size)\n",
        "    elif model == 'ConvNetLN':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm='layernorm', net_pooling=net_pooling, im_size=im_size)\n",
        "    elif model == 'ConvNetIN':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm='instancenorm', net_pooling=net_pooling, im_size=im_size)\n",
        "    elif model == 'ConvNetGN':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm='groupnorm', net_pooling=net_pooling, im_size=im_size)\n",
        "\n",
        "    elif model == 'ConvNetNP':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling='none', im_size=im_size)\n",
        "    elif model == 'ConvNetMP':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling='maxpooling', im_size=im_size)\n",
        "    elif model == 'ConvNetAP':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling='avgpooling', im_size=im_size)\n",
        "\n",
        "    else:\n",
        "        net = None\n",
        "        exit('unknown model: %s'%model)\n",
        "\n",
        "    gpu_num = torch.cuda.device_count()\n",
        "    if gpu_num>0:\n",
        "        device = 'cuda'\n",
        "        if gpu_num>1:\n",
        "            net = nn.DataParallel(net)\n",
        "    else:\n",
        "        device = 'cpu'\n",
        "    net = net.to(device)\n",
        "\n",
        "    return net\n",
        "\n",
        "\n",
        "\n",
        "def get_time():\n",
        "    return str(time.strftime(\"[%Y-%m-%d %H:%M:%S]\", time.localtime()))\n",
        "\n",
        "\n",
        "\n",
        "def distance_wb(gwr, gws):\n",
        "    shape = gwr.shape\n",
        "    if len(shape) == 4: # conv, out*in*h*w\n",
        "        gwr = gwr.reshape(shape[0], shape[1] * shape[2] * shape[3])\n",
        "        gws = gws.reshape(shape[0], shape[1] * shape[2] * shape[3])\n",
        "    elif len(shape) == 3:  # layernorm, C*h*w\n",
        "        gwr = gwr.reshape(shape[0], shape[1] * shape[2])\n",
        "        gws = gws.reshape(shape[0], shape[1] * shape[2])\n",
        "    elif len(shape) == 2: # linear, out*in\n",
        "        tmp = 'do nothing'\n",
        "    elif len(shape) == 1: # batchnorm/instancenorm, C; groupnorm x, bias\n",
        "        gwr = gwr.reshape(1, shape[0])\n",
        "        gws = gws.reshape(1, shape[0])\n",
        "        return torch.tensor(0, dtype=torch.float, device=gwr.device)\n",
        "\n",
        "    dis_weight = torch.sum(1 - torch.sum(gwr * gws, dim=-1) / (torch.norm(gwr, dim=-1) * torch.norm(gws, dim=-1) + 0.000001))\n",
        "    dis = dis_weight\n",
        "    return dis\n",
        "\n",
        "\n",
        "\n",
        "def match_loss(gw_syn, gw_real, args):\n",
        "    dis = torch.tensor(0.0).to(args.device)\n",
        "\n",
        "    if args.dis_metric == 'ours':\n",
        "        for ig in range(len(gw_real)):\n",
        "            gwr = gw_real[ig]\n",
        "            gws = gw_syn[ig]\n",
        "            dis += distance_wb(gwr, gws)\n",
        "\n",
        "    elif args.dis_metric == 'mse':\n",
        "        gw_real_vec = []\n",
        "        gw_syn_vec = []\n",
        "        for ig in range(len(gw_real)):\n",
        "            gw_real_vec.append(gw_real[ig].reshape((-1)))\n",
        "            gw_syn_vec.append(gw_syn[ig].reshape((-1)))\n",
        "        gw_real_vec = torch.cat(gw_real_vec, dim=0)\n",
        "        gw_syn_vec = torch.cat(gw_syn_vec, dim=0)\n",
        "        dis = torch.sum((gw_syn_vec - gw_real_vec)**2)\n",
        "\n",
        "    elif args.dis_metric == 'cos':\n",
        "        gw_real_vec = []\n",
        "        gw_syn_vec = []\n",
        "        for ig in range(len(gw_real)):\n",
        "            gw_real_vec.append(gw_real[ig].reshape((-1)))\n",
        "            gw_syn_vec.append(gw_syn[ig].reshape((-1)))\n",
        "        gw_real_vec = torch.cat(gw_real_vec, dim=0)\n",
        "        gw_syn_vec = torch.cat(gw_syn_vec, dim=0)\n",
        "        dis = 1 - torch.sum(gw_real_vec * gw_syn_vec, dim=-1) / (torch.norm(gw_real_vec, dim=-1) * torch.norm(gw_syn_vec, dim=-1) + 0.000001)\n",
        "\n",
        "    else:\n",
        "        exit('unknown distance function: %s'%args.dis_metric)\n",
        "\n",
        "    return dis\n",
        "\n",
        "\n",
        "\n",
        "def get_loops(ipc):\n",
        "    # Get the two hyper-parameters of outer-loop and inner-loop.\n",
        "    # The following values are empirically good.\n",
        "    if ipc == 1:\n",
        "        outer_loop, inner_loop = 1, 1\n",
        "    elif ipc == 10:\n",
        "        outer_loop, inner_loop = 10, 50\n",
        "    elif ipc == 20:\n",
        "        outer_loop, inner_loop = 20, 25\n",
        "    elif ipc == 30:\n",
        "        outer_loop, inner_loop = 30, 20\n",
        "    elif ipc == 40:\n",
        "        outer_loop, inner_loop = 40, 15\n",
        "    elif ipc == 50:\n",
        "        outer_loop, inner_loop = 50, 10\n",
        "    else:\n",
        "        outer_loop, inner_loop = 0, 0\n",
        "        exit('loop hyper-parameters are not defined for %d ipc'%ipc)\n",
        "    return outer_loop, inner_loop\n",
        "\n",
        "\n",
        "\n",
        "def epoch(mode, dataloader, net, optimizer, criterion, args, aug):\n",
        "    loss_avg, acc_avg, num_exp = 0, 0, 0\n",
        "    net = net.to(args.device)\n",
        "    criterion = criterion.to(args.device)\n",
        "\n",
        "    if mode == 'train':\n",
        "        net.train()\n",
        "    else:\n",
        "        net.eval()\n",
        "\n",
        "    for i_batch, datum in enumerate(dataloader):\n",
        "\n",
        "        if args.training_baseline and not(args.dataset==\"MHIST\"):\n",
        "          if mode == 'train':\n",
        "            img = datum[0][:, np.newaxis, :, :].float().to(args.device)\n",
        "          else:\n",
        "            img = datum[0].float().to(args.device)\n",
        "        else:\n",
        "            img = datum[0].squeeze(1).float().to(args.device)\n",
        "\n",
        "        if args.dataset==\"MNIST\":\n",
        "          img = datum[0].float().to(args.device)\n",
        "\n",
        "        if args.dataset==\"MNIST Custom\":\n",
        "          img = datum[0].float().to(args.device)\n",
        "\n",
        "        if args.dataset==\"MHIST Custom\":\n",
        "          img = datum[0].squeeze(0).float().to(args.device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # if aug:\n",
        "        #     if args.dsa:\n",
        "        #         img = DiffAugment(img, args.dsa_strategy, param=args.dsa_param)\n",
        "        #     else:\n",
        "        #         img = augment(img, args.dc_aug_param, device=args.device)\n",
        "        lab = datum[1].long().to(args.device)\n",
        "        n_b = lab.shape[0]\n",
        "\n",
        "        # save_image(img[0], \"/content/drive/MyDrive/ECE1512/Project_B/Runs/MHIST Initial/testing.png\")\n",
        "\n",
        "        output = net(img)\n",
        "        loss = criterion(output, lab)\n",
        "        acc = np.sum(np.equal(np.argmax(output.cpu().data.numpy(), axis=-1), lab.cpu().data.numpy()))\n",
        "\n",
        "        loss_avg += loss.item()*n_b\n",
        "        acc_avg += acc\n",
        "        num_exp += n_b\n",
        "\n",
        "        if mode == 'train':\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    loss_avg /= num_exp\n",
        "    acc_avg /= num_exp\n",
        "\n",
        "    return loss_avg, acc_avg\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_synset(it_eval, net, images_train, labels_train, testloader, args):\n",
        "    net = net.to(args.device)\n",
        "\n",
        "    if not(args.dataset==\"MNIST Custom\" or args.dataset==\"MHIST Custom\"):\n",
        "      images_train = images_train.to(args.device)\n",
        "      labels_train = labels_train.to(args.device)\n",
        "      dst_train = TensorDataset(images_train, labels_train)\n",
        "      trainloader = torch.utils.data.DataLoader(dst_train, batch_size=args.batch_train, shuffle=True, num_workers=0)\n",
        "    else:\n",
        "      trainloader = torch.utils.data.DataLoader(images_train, batch_size=args.batch_train, shuffle=True, num_workers=0)\n",
        "\n",
        "\n",
        "    lr = float(args.lr_net)\n",
        "    Epoch = int(args.epoch_eval_train)\n",
        "    lr_schedule = [Epoch//2+1]\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=0.0005)\n",
        "    criterion = nn.CrossEntropyLoss().to(args.device)\n",
        "\n",
        "\n",
        "    start = time.time()\n",
        "    for ep in range(Epoch+1):\n",
        "\n",
        "        if args.training_baseline:\n",
        "          shouldAug = False\n",
        "        else: shouldAug = True\n",
        "\n",
        "        loss_train, acc_train = epoch('train', trainloader, net, optimizer, criterion, args, shouldAug)\n",
        "        if ep in lr_schedule:\n",
        "            lr *= 0.1\n",
        "            optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "    time_train = time.time() - start\n",
        "\n",
        "    shouldAug = True\n",
        "    loss_test, acc_test = epoch('test', testloader, net, optimizer, criterion, args, shouldAug)\n",
        "    print('%s Evaluate_%02d: epoch = %04d train time = %d s train loss = %.6f train acc = %.4f, test acc = %.4f' % (get_time(), it_eval, Epoch, int(time_train), loss_train, acc_train, acc_test))\n",
        "\n",
        "    return net, acc_train, acc_test\n",
        "\n",
        "\n",
        "\n",
        "def augment(images, dc_aug_param, device):\n",
        "    # This can be sped up in the future.\n",
        "\n",
        "    if dc_aug_param != None and dc_aug_param['strategy'] != 'none':\n",
        "        scale = dc_aug_param['scale']\n",
        "        crop = dc_aug_param['crop']\n",
        "        rotate = dc_aug_param['rotate']\n",
        "        noise = dc_aug_param['noise']\n",
        "        strategy = dc_aug_param['strategy']\n",
        "\n",
        "        shape = images.shape\n",
        "        mean = []\n",
        "        for c in range(shape[1]):\n",
        "            mean.append(float(torch.mean(images[:,c])))\n",
        "\n",
        "        def cropfun(i):\n",
        "            im_ = torch.zeros(shape[1],shape[2]+crop*2,shape[3]+crop*2, dtype=torch.float, device=device)\n",
        "            for c in range(shape[1]):\n",
        "                im_[c] = mean[c]\n",
        "            im_[:, crop:crop+shape[2], crop:crop+shape[3]] = images[i]\n",
        "            r, c = np.random.permutation(crop*2)[0], np.random.permutation(crop*2)[0]\n",
        "            images[i] = im_[:, r:r+shape[2], c:c+shape[3]]\n",
        "\n",
        "        def scalefun(i):\n",
        "            h = int((np.random.uniform(1 - scale, 1 + scale)) * shape[2])\n",
        "            w = int((np.random.uniform(1 - scale, 1 + scale)) * shape[2])\n",
        "            tmp = F.interpolate(images[i:i + 1], [h, w], )[0]\n",
        "            mhw = max(h, w, shape[2], shape[3])\n",
        "            im_ = torch.zeros(shape[1], mhw, mhw, dtype=torch.float, device=device)\n",
        "            r = int((mhw - h) / 2)\n",
        "            c = int((mhw - w) / 2)\n",
        "            im_[:, r:r + h, c:c + w] = tmp\n",
        "            r = int((mhw - shape[2]) / 2)\n",
        "            c = int((mhw - shape[3]) / 2)\n",
        "            images[i] = im_[:, r:r + shape[2], c:c + shape[3]]\n",
        "\n",
        "        def rotatefun(i):\n",
        "            im_ = scipyrotate(images[i].cpu().data.numpy(), angle=np.random.randint(-rotate, rotate), axes=(-2, -1), cval=np.mean(mean))\n",
        "            r = int((im_.shape[-2] - shape[-2]) / 2)\n",
        "            c = int((im_.shape[-1] - shape[-1]) / 2)\n",
        "            images[i] = torch.tensor(im_[:, r:r + shape[-2], c:c + shape[-1]], dtype=torch.float, device=device)\n",
        "\n",
        "        def noisefun(i):\n",
        "            images[i] = images[i] + noise * torch.randn(shape[1:], dtype=torch.float, device=device)\n",
        "\n",
        "\n",
        "        augs = strategy.split('_')\n",
        "\n",
        "        for i in range(shape[0]):\n",
        "            choice = np.random.permutation(augs)[0] # randomly implement one augmentation\n",
        "            if choice == 'crop':\n",
        "                cropfun(i)\n",
        "            elif choice == 'scale':\n",
        "                scalefun(i)\n",
        "            elif choice == 'rotate':\n",
        "                rotatefun(i)\n",
        "            elif choice == 'noise':\n",
        "                noisefun(i)\n",
        "\n",
        "    return images\n",
        "\n",
        "\n",
        "\n",
        "def get_daparam(dataset, model, model_eval, ipc):\n",
        "    # We find that augmentation doesn't always benefit the performance.\n",
        "    # So we do augmentation for some of the settings.\n",
        "\n",
        "    dc_aug_param = dict()\n",
        "    dc_aug_param['crop'] = 4\n",
        "    dc_aug_param['scale'] = 0.2\n",
        "    dc_aug_param['rotate'] = 45\n",
        "    dc_aug_param['noise'] = 0.001\n",
        "    dc_aug_param['strategy'] = 'none'\n",
        "\n",
        "    if dataset == 'MNIST':\n",
        "        dc_aug_param['strategy'] = 'crop_scale_rotate'\n",
        "\n",
        "    if model_eval in ['ConvNetBN']: # Data augmentation makes model training with Batch Norm layer easier.\n",
        "        dc_aug_param['strategy'] = 'crop_noise'\n",
        "\n",
        "    return dc_aug_param\n",
        "\n",
        "\n",
        "def get_eval_pool(eval_mode, model, model_eval):\n",
        "    if eval_mode == 'M': # multiple architectures\n",
        "        model_eval_pool = ['MLP', 'ConvNet', 'LeNet', 'AlexNet', 'VGG11', 'ResNet18']\n",
        "    elif eval_mode == 'B':  # multiple architectures with BatchNorm for DM experiments\n",
        "        model_eval_pool = ['ConvNetBN', 'ConvNetASwishBN', 'AlexNetBN', 'VGG11BN', 'ResNet18BN']\n",
        "    elif eval_mode == 'W': # ablation study on network width\n",
        "        model_eval_pool = ['ConvNetW32', 'ConvNetW64', 'ConvNetW128', 'ConvNetW256']\n",
        "    elif eval_mode == 'D': # ablation study on network depth\n",
        "        model_eval_pool = ['ConvNetD1', 'ConvNetD2', 'ConvNetD3', 'ConvNetD4']\n",
        "    elif eval_mode == 'A': # ablation study on network activation function\n",
        "        model_eval_pool = ['ConvNetAS', 'ConvNetAR', 'ConvNetAL', 'ConvNetASwish']\n",
        "    elif eval_mode == 'P': # ablation study on network pooling layer\n",
        "        model_eval_pool = ['ConvNetNP', 'ConvNetMP', 'ConvNetAP']\n",
        "    elif eval_mode == 'N': # ablation study on network normalization layer\n",
        "        model_eval_pool = ['ConvNetNN', 'ConvNetBN', 'ConvNetLN', 'ConvNetIN', 'ConvNetGN']\n",
        "    elif eval_mode == 'S': # itself\n",
        "        if 'BN' in model:\n",
        "            print('Attention: Here I will replace BN with IN in evaluation, as the synthetic set is too small to measure BN hyper-parameters.')\n",
        "        model_eval_pool = [model[:model.index('BN')]] if 'BN' in model else [model]\n",
        "    elif eval_mode == 'SS':  # itself\n",
        "        model_eval_pool = [model]\n",
        "    else:\n",
        "        model_eval_pool = [model_eval]\n",
        "    return model_eval_pool\n",
        "\n",
        "\n",
        "class ParamDiffAug():\n",
        "    def __init__(self):\n",
        "        self.aug_mode = 'S' #'multiple or single'\n",
        "        self.prob_flip = 0.5\n",
        "        self.ratio_scale = 1.2\n",
        "        self.ratio_rotate = 15.0\n",
        "        self.ratio_crop_pad = 0.125\n",
        "        self.ratio_cutout = 0.5 # the size would be 0.5x0.5\n",
        "        self.brightness = 1.0\n",
        "        self.saturation = 2.0\n",
        "        self.contrast = 0.5\n",
        "\n",
        "\n",
        "def set_seed_DiffAug(param):\n",
        "    if param.latestseed == -1:\n",
        "        return\n",
        "    else:\n",
        "        torch.random.manual_seed(param.latestseed)\n",
        "        param.latestseed += 1\n",
        "\n",
        "\n",
        "def DiffAugment(x, strategy='', seed = -1, param = None):\n",
        "    if strategy == 'None' or strategy == 'none' or strategy == '':\n",
        "        return x\n",
        "\n",
        "    if seed == -1:\n",
        "        param.Siamese = False\n",
        "    else:\n",
        "        param.Siamese = True\n",
        "\n",
        "    param.latestseed = seed\n",
        "\n",
        "    if strategy:\n",
        "        if param.aug_mode == 'M': # original\n",
        "            for p in strategy.split('_'):\n",
        "                for f in AUGMENT_FNS[p]:\n",
        "                    x = f(x, param)\n",
        "        elif param.aug_mode == 'S':\n",
        "            pbties = strategy.split('_')\n",
        "            set_seed_DiffAug(param)\n",
        "            p = pbties[torch.randint(0, len(pbties), size=(1,)).item()]\n",
        "            for f in AUGMENT_FNS[p]:\n",
        "                x = f(x, param)\n",
        "        else:\n",
        "            exit('unknown augmentation mode: %s'%param.aug_mode)\n",
        "        x = x.contiguous()\n",
        "    return x\n",
        "\n",
        "\n",
        "# We implement the following differentiable augmentation strategies based on the code provided in https://github.com/mit-han-lab/data-efficient-gans.\n",
        "def rand_scale(x, param):\n",
        "    # x>1, max scale\n",
        "    # sx, sy: (0, +oo), 1: orignial size, 0.5: enlarge 2 times\n",
        "    ratio = param.ratio_scale\n",
        "    set_seed_DiffAug(param)\n",
        "    sx = torch.rand(x.shape[0]) * (ratio - 1.0/ratio) + 1.0/ratio\n",
        "    set_seed_DiffAug(param)\n",
        "    sy = torch.rand(x.shape[0]) * (ratio - 1.0/ratio) + 1.0/ratio\n",
        "    theta = [[[sx[i], 0,  0],\n",
        "            [0,  sy[i], 0],] for i in range(x.shape[0])]\n",
        "    theta = torch.tensor(theta, dtype=torch.float)\n",
        "    if param.Siamese: # Siamese augmentation:\n",
        "        theta[:] = theta[0]\n",
        "    grid = F.affine_grid(theta, x.shape).to(x.device)\n",
        "    x = F.grid_sample(x, grid)\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_rotate(x, param): # [-180, 180], 90: anticlockwise 90 degree\n",
        "    ratio = param.ratio_rotate\n",
        "    set_seed_DiffAug(param)\n",
        "    theta = (torch.rand(x.shape[0]) - 0.5) * 2 * ratio / 180 * float(np.pi)\n",
        "    theta = [[[torch.cos(theta[i]), torch.sin(-theta[i]), 0],\n",
        "        [torch.sin(theta[i]), torch.cos(theta[i]),  0],]  for i in range(x.shape[0])]\n",
        "    theta = torch.tensor(theta, dtype=torch.float)\n",
        "    if param.Siamese: # Siamese augmentation:\n",
        "        theta[:] = theta[0]\n",
        "    grid = F.affine_grid(theta, x.shape).to(x.device)\n",
        "    x = F.grid_sample(x, grid)\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_flip(x, param):\n",
        "    prob = param.prob_flip\n",
        "    set_seed_DiffAug(param)\n",
        "    randf = torch.rand(x.size(0), 1, 1, 1, device=x.device)\n",
        "    if param.Siamese: # Siamese augmentation:\n",
        "        randf[:] = randf[0]\n",
        "    return torch.where(randf < prob, x.flip(3), x)\n",
        "\n",
        "\n",
        "def rand_brightness(x, param):\n",
        "    ratio = param.brightness\n",
        "    set_seed_DiffAug(param)\n",
        "    randb = torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device)\n",
        "    if param.Siamese:  # Siamese augmentation:\n",
        "        randb[:] = randb[0]\n",
        "    x = x + (randb - 0.5)*ratio\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_saturation(x, param):\n",
        "    ratio = param.saturation\n",
        "    x_mean = x.mean(dim=1, keepdim=True)\n",
        "    set_seed_DiffAug(param)\n",
        "    rands = torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device)\n",
        "    if param.Siamese:  # Siamese augmentation:\n",
        "        rands[:] = rands[0]\n",
        "    x = (x - x_mean) * (rands * ratio) + x_mean\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_contrast(x, param):\n",
        "    ratio = param.contrast\n",
        "    x_mean = x.mean(dim=[1, 2, 3], keepdim=True)\n",
        "    set_seed_DiffAug(param)\n",
        "    randc = torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device)\n",
        "    if param.Siamese:  # Siamese augmentation:\n",
        "        randc[:] = randc[0]\n",
        "    x = (x - x_mean) * (randc + ratio) + x_mean\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_crop(x, param):\n",
        "    # The image is padded on its surrounding and then cropped.\n",
        "    ratio = param.ratio_crop_pad\n",
        "    shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
        "    set_seed_DiffAug(param)\n",
        "    translation_x = torch.randint(-shift_x, shift_x + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    set_seed_DiffAug(param)\n",
        "    translation_y = torch.randint(-shift_y, shift_y + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    if param.Siamese:  # Siamese augmentation:\n",
        "        translation_x[:] = translation_x[0]\n",
        "        translation_y[:] = translation_y[0]\n",
        "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(2), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(3), dtype=torch.long, device=x.device),\n",
        "    )\n",
        "    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)\n",
        "    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)\n",
        "    x_pad = F.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])\n",
        "    x = x_pad.permute(0, 2, 3, 1).contiguous()[grid_batch, grid_x, grid_y].permute(0, 3, 1, 2)\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_cutout(x, param):\n",
        "    ratio = param.ratio_cutout\n",
        "    cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
        "    set_seed_DiffAug(param)\n",
        "    offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "    set_seed_DiffAug(param)\n",
        "    offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "    if param.Siamese:  # Siamese augmentation:\n",
        "        offset_x[:] = offset_x[0]\n",
        "        offset_y[:] = offset_y[0]\n",
        "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "        torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n",
        "        torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n",
        "    )\n",
        "    grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)\n",
        "    grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)\n",
        "    mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)\n",
        "    mask[grid_batch, grid_x, grid_y] = 0\n",
        "    x = x * mask.unsqueeze(1)\n",
        "    return x\n",
        "\n",
        "\n",
        "AUGMENT_FNS = {\n",
        "    'color': [rand_brightness, rand_saturation, rand_contrast],\n",
        "    'crop': [rand_crop],\n",
        "    'cutout': [rand_cutout],\n",
        "    'flip': [rand_flip],\n",
        "    'scale': [rand_scale],\n",
        "    'rotate': [rand_rotate],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Guww8tXIIIe7"
      },
      "outputs": [],
      "source": [
        "def main(method=\"DC\", dataset=\"CIFAR10\", model=\"ConvNet\", ipc=1, eval_mode=\"S\", num_exp=5,\n",
        "         num_eval=20, epoch_eval_train=300, iteration=1000, lr_img=0.1, lr_net=0.01, batch_real=128,\n",
        "         batch_train=128, init=\"noise\", dsa_strategy=\"None\", data_path=\"/content/drive/MyDrive/ECE1512/Project_B/\",\n",
        "         save_path=\"/content/drive/MyDrive/ECE1512/Project_B/Runs/\", dis_metric=\"ours\", training_baseline=False):\n",
        "\n",
        "    parser = argparse.ArgumentParser(description='Parameter Processing')\n",
        "    parser.add_argument('--method', type=str, default=method, help='DC/DSA')\n",
        "    parser.add_argument('--dataset', type=str, default=dataset, help='dataset')\n",
        "    parser.add_argument('--model', type=str, default=model, help='model')\n",
        "    parser.add_argument('--ipc', type=int, default=ipc, help='image(s) per class')\n",
        "    parser.add_argument('--eval_mode', type=str, default=eval_mode, help='eval_mode') # S: the same to training model, M: multi architectures,  W: net width, D: net depth, A: activation function, P: pooling layer, N: normalization layer,\n",
        "    parser.add_argument('--num_exp', type=int, default=num_exp, help='the number of experiments')\n",
        "    parser.add_argument('--num_eval', type=int, default=num_eval, help='the number of evaluating randomly initialized models')\n",
        "    parser.add_argument('--epoch_eval_train', type=int, default=epoch_eval_train, help='epochs to train a model with synthetic data')\n",
        "    parser.add_argument('--Iteration', type=int, default=iteration, help='training iterations')\n",
        "    parser.add_argument('--lr_img', type=float, default=lr_img, help='learning rate for updating synthetic images')\n",
        "    parser.add_argument('--lr_net', type=float, default=lr_net, help='learning rate for updating network parameters')\n",
        "    parser.add_argument('--batch_real', type=int, default=batch_real, help='batch size for real data')\n",
        "    parser.add_argument('--batch_train', type=int, default=batch_train, help='batch size for training networks')\n",
        "    parser.add_argument('--init', type=str, default=init, help='noise/real: initialize synthetic images from random noise or randomly sampled real images.')\n",
        "    parser.add_argument('--dsa_strategy', type=str, default=dsa_strategy, help='differentiable Siamese augmentation strategy')\n",
        "    parser.add_argument('--data_path', type=str, default=data_path, help='dataset path')\n",
        "    parser.add_argument('--save_path', type=str, default=save_path, help='path to save results')\n",
        "    parser.add_argument('--dis_metric', type=str, default=dis_metric, help='distance metric')\n",
        "    parser.add_argument('--training_baseline', type=bool, default=training_baseline, help='Boolean for baseline training')\n",
        "\n",
        "\n",
        "    args, unknown = parser.parse_known_args()\n",
        "    args.outer_loop, args.inner_loop = get_loops(args.ipc)\n",
        "    args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    #args.device = 'cpu'\n",
        "\n",
        "    if args.device == 'cuda': print(\"RUNNING ON GPU\")\n",
        "\n",
        "    args.dsa_param = ParamDiffAug()\n",
        "    args.dsa = True if args.method == 'DSA' else False\n",
        "\n",
        "    if not os.path.exists(args.data_path):\n",
        "        os.mkdir(args.data_path)\n",
        "\n",
        "    if not os.path.exists(args.save_path):\n",
        "        os.mkdir(args.save_path)\n",
        "\n",
        "    eval_it_pool = np.arange(0, args.Iteration+1, 10).tolist() if args.eval_mode == 'S' or args.eval_mode == 'SS' else [args.Iteration] # The list of iterations when we evaluate models and record results.\n",
        "    print('eval_it_pool: ', eval_it_pool)\n",
        "    channel, im_size, num_classes, class_names, mean, std, dst_train, dst_test, testloader = get_dataset(args.dataset, args.data_path)\n",
        "    model_eval_pool = get_eval_pool(args.eval_mode, args.model, args.model)\n",
        "\n",
        "\n",
        "    accs_all_exps = dict() # record performances of all experiments\n",
        "    for key in model_eval_pool:\n",
        "        accs_all_exps[key] = []\n",
        "\n",
        "    data_save = []\n",
        "\n",
        "\n",
        "    for exp in range(args.num_exp):\n",
        "        print('\\n================== Exp %d ==================\\n '%exp)\n",
        "        print('Hyper-parameters: \\n', args.__dict__)\n",
        "        print('Evaluation model pool: ', model_eval_pool)\n",
        "\n",
        "        ''' organize the real dataset '''\n",
        "        images_all = []\n",
        "        labels_all = []\n",
        "        indices_class = [[] for c in range(num_classes)]\n",
        "\n",
        "        images_all = [torch.unsqueeze(dst_train[i][0], dim=0) for i in range(len(dst_train))]\n",
        "        labels_all = [dst_train[i][1] for i in range(len(dst_train))]\n",
        "        for i, lab in enumerate(labels_all):\n",
        "            indices_class[lab].append(i)\n",
        "        images_all = torch.cat(images_all, dim=0).to(args.device)\n",
        "        labels_all = torch.tensor(labels_all, dtype=torch.long, device=args.device)\n",
        "\n",
        "        for c in range(num_classes):\n",
        "            print('class c = %d: %d real images'%(c, len(indices_class[c])))\n",
        "\n",
        "        def get_images(c, n): # get random n images from class c\n",
        "            idx_shuffle = np.random.permutation(indices_class[c])[:n]\n",
        "            return images_all[idx_shuffle]\n",
        "\n",
        "        for ch in range(channel):\n",
        "            print('real images channel %d, mean = %.4f, std = %.4f'%(ch, torch.mean(images_all[:, ch]), torch.std(images_all[:, ch])))\n",
        "\n",
        "\n",
        "        ''' initialize the synthetic data '''\n",
        "        image_syn = torch.randn(size=(num_classes*args.ipc, channel, im_size[0], im_size[1]), dtype=torch.float, requires_grad=True, device=args.device)\n",
        "        label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]\n",
        "\n",
        "        if args.init == 'real':\n",
        "            print('initialize synthetic data from random real images')\n",
        "            for c in range(num_classes):\n",
        "                image_syn.data[c*args.ipc:(c+1)*args.ipc] = get_images(c, args.ipc).detach().data\n",
        "        else:\n",
        "            print('initialize synthetic data from random noise')\n",
        "\n",
        "\n",
        "        ''' training '''\n",
        "        optimizer_img = torch.optim.SGD([image_syn, ], lr=args.lr_img, momentum=0.5) # optimizer_img for synthetic data\n",
        "        optimizer_img.zero_grad()\n",
        "        criterion = nn.CrossEntropyLoss().to(args.device)\n",
        "        print('%s training begins'%get_time())\n",
        "\n",
        "        for it in range(args.Iteration+1):\n",
        "\n",
        "            ''' Evaluate synthetic data '''\n",
        "            if it in eval_it_pool:\n",
        "                for model_eval in model_eval_pool:\n",
        "                    print('-------------------------\\nEvaluation\\nmodel_train = %s, model_eval = %s, iteration = %d'%(args.model, model_eval, it))\n",
        "                    if args.dsa:\n",
        "                        args.epoch_eval_train = 1000\n",
        "                        args.dc_aug_param = None\n",
        "                        print('DSA augmentation strategy: \\n', args.dsa_strategy)\n",
        "                        print('DSA augmentation parameters: \\n', args.dsa_param.__dict__)\n",
        "                    else:\n",
        "                        args.dc_aug_param = get_daparam(args.dataset, args.model, model_eval, args.ipc) # This augmentation parameter set is only for DC method. It will be muted when args.dsa is True.\n",
        "                        print('DC augmentation parameters: \\n', args.dc_aug_param)\n",
        "\n",
        "                    if args.dsa or args.dc_aug_param['strategy'] != 'none':\n",
        "                        args.epoch_eval_train = 1000  # Training with data augmentation needs more epochs.\n",
        "                    else:\n",
        "                        args.epoch_eval_train = 300\n",
        "\n",
        "                    accs = []\n",
        "                    for it_eval in range(args.num_eval):\n",
        "                        net_eval = get_network(model_eval, channel, num_classes, im_size).to(args.device) # get a random model\n",
        "                        image_syn_eval, label_syn_eval = copy.deepcopy(image_syn.detach()), copy.deepcopy(label_syn.detach()) # avoid any unaware modification\n",
        "                        _, acc_train, acc_test = evaluate_synset(it_eval, net_eval, image_syn_eval, label_syn_eval, testloader, args)\n",
        "                        accs.append(acc_test)\n",
        "                    print('Evaluate %d random %s, mean = %.4f std = %.4f\\n-------------------------'%(len(accs), model_eval, np.mean(accs), np.std(accs)))\n",
        "\n",
        "                    if it == args.Iteration: # record the final results\n",
        "                        accs_all_exps[model_eval] += accs\n",
        "\n",
        "                ''' visualize and save '''\n",
        "                save_name = os.path.join(args.save_path, 'vis_%s_%s_%s_%dipc_exp%d_iter%d.png'%(args.method, args.dataset, args.model, args.ipc, exp, it))\n",
        "                image_syn_vis = copy.deepcopy(image_syn.detach().cpu())\n",
        "\n",
        "                count = 0\n",
        "\n",
        "                for ch in range(channel):\n",
        "                    image_syn_vis[:, ch] = image_syn_vis[:, ch]  * std[ch] + mean[ch]\n",
        "\n",
        "                if not(args.dataset == \"MHIST\"):\n",
        "                  image_syn_vis[image_syn_vis<0] = 0.0\n",
        "                  image_syn_vis[image_syn_vis>1] = 1.0\n",
        "                  save_image(image_syn_vis, save_name, nrow=args.ipc) # Trying normalize = True/False may get better visual effects.\n",
        "\n",
        "                  count = 0\n",
        "                  for synImg in image_syn_vis:\n",
        "                      save_image(synImg, save_path+str(count)+\"_\"+str(it)+\".png\")\n",
        "                      count += 1\n",
        "                else:\n",
        "\n",
        "                  image_syn_vis = (image_syn_vis - image_syn_vis.min()) / (image_syn_vis.max() - image_syn_vis.min())\n",
        "\n",
        "                  print(image_syn_vis.min())\n",
        "                  print(image_syn_vis.max())\n",
        "\n",
        "                  image_syn_vis[image_syn_vis<0] = 0.0\n",
        "                  image_syn_vis[image_syn_vis>1] = 1.0\n",
        "\n",
        "                  save_image(image_syn_vis, save_name, nrow=args.ipc) # Trying normalize = True/False may get better visual effects.\n",
        "\n",
        "                  count = 0\n",
        "                  for synImg in image_syn_vis:\n",
        "                      save_image(synImg, save_path+str(count)+\"_\"+str(it)+\".png\")\n",
        "                      count += 1\n",
        "\n",
        "\n",
        "\n",
        "            ''' Train synthetic data '''\n",
        "            net = get_network(args.model, channel, num_classes, im_size).to(args.device) # get a random model\n",
        "            net.train()\n",
        "            net_parameters = list(net.parameters())\n",
        "            optimizer_net = torch.optim.SGD(net.parameters(), lr=args.lr_net)  # optimizer_img for synthetic data\n",
        "            optimizer_net.zero_grad()\n",
        "            loss_avg = 0\n",
        "            args.dc_aug_param = None  # Mute the DC augmentation when learning synthetic data (in inner-loop epoch function) in oder to be consistent with DC paper.\n",
        "\n",
        "\n",
        "            for ol in range(args.outer_loop):\n",
        "\n",
        "                ''' freeze the running mu and sigma for BatchNorm layers '''\n",
        "                # Synthetic data batch, e.g. only 1 image/batch, is too small to obtain stable mu and sigma.\n",
        "                # So, we calculate and freeze mu and sigma for BatchNorm layer with real data batch ahead.\n",
        "                # This would make the training with BatchNorm layers easier.\n",
        "\n",
        "                BN_flag = False\n",
        "                BNSizePC = 16  # for batch normalization\n",
        "                for module in net.modules():\n",
        "                    if 'BatchNorm' in module._get_name(): #BatchNorm\n",
        "                        BN_flag = True\n",
        "                if BN_flag:\n",
        "                    img_real = torch.cat([get_images(c, BNSizePC) for c in range(num_classes)], dim=0)\n",
        "                    net.train() # for updating the mu, sigma of BatchNorm\n",
        "                    output_real = net(img_real) # get running mu, sigma\n",
        "                    for module in net.modules():\n",
        "                        if 'BatchNorm' in module._get_name():  #BatchNorm\n",
        "                            module.eval() # fix mu and sigma of every BatchNorm layer\n",
        "\n",
        "\n",
        "                ''' update synthetic data '''\n",
        "                loss = torch.tensor(0.0).to(args.device)\n",
        "                for c in range(num_classes):\n",
        "                    img_real = get_images(c, args.batch_real)\n",
        "                    lab_real = torch.ones((img_real.shape[0],), device=args.device, dtype=torch.long) * c\n",
        "                    img_syn = image_syn[c*args.ipc:(c+1)*args.ipc].reshape((args.ipc, channel, im_size[0], im_size[1]))\n",
        "                    lab_syn = torch.ones((args.ipc,), device=args.device, dtype=torch.long) * c\n",
        "\n",
        "                    if args.dsa:\n",
        "                        seed = int(time.time() * 1000) % 100000\n",
        "                        img_real = DiffAugment(img_real, args.dsa_strategy, seed=seed, param=args.dsa_param)\n",
        "                        img_syn = DiffAugment(img_syn, args.dsa_strategy, seed=seed, param=args.dsa_param)\n",
        "\n",
        "                    output_real = net(img_real)\n",
        "                    loss_real = criterion(output_real, lab_real)\n",
        "                    gw_real = torch.autograd.grad(loss_real, net_parameters)\n",
        "                    gw_real = list((_.detach().clone() for _ in gw_real))\n",
        "\n",
        "                    output_syn = net(img_syn)\n",
        "                    loss_syn = criterion(output_syn, lab_syn)\n",
        "                    gw_syn = torch.autograd.grad(loss_syn, net_parameters, create_graph=True)\n",
        "\n",
        "                    loss += match_loss(gw_syn, gw_real, args)\n",
        "\n",
        "                optimizer_img.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer_img.step()\n",
        "                loss_avg += loss.item()\n",
        "\n",
        "                if ol == args.outer_loop - 1:\n",
        "                    break\n",
        "\n",
        "\n",
        "                ''' update network '''\n",
        "                image_syn_train, label_syn_train = copy.deepcopy(image_syn.detach()), copy.deepcopy(label_syn.detach())  # avoid any unaware modification\n",
        "                dst_syn_train = TensorDataset(image_syn_train, label_syn_train)\n",
        "                trainloader = torch.utils.data.DataLoader(dst_syn_train, batch_size=args.batch_train, shuffle=True, num_workers=0)\n",
        "                for il in range(args.inner_loop):\n",
        "                    if args.dsa:\n",
        "                        shouldAug = False\n",
        "                    else:\n",
        "                        shouldAug = False\n",
        "                    epoch('train', trainloader, net, optimizer_net, criterion, args, shouldAug)\n",
        "\n",
        "\n",
        "            loss_avg /= (num_classes*args.outer_loop)\n",
        "\n",
        "            if it%10 == 0:\n",
        "                print('%s iter = %04d, loss = %.4f' % (get_time(), it, loss_avg))\n",
        "\n",
        "            if it == args.Iteration: # only record the final results\n",
        "                data_save.append([copy.deepcopy(image_syn.detach().cpu()), copy.deepcopy(label_syn.detach().cpu())])\n",
        "                torch.save({'data': data_save, 'accs_all_exps': accs_all_exps, }, os.path.join(args.save_path, 'res_%s_%s_%s_%dipc.pt'%(args.method, args.dataset, args.model, args.ipc)))\n",
        "\n",
        "\n",
        "    print('\\n==================== Final Results ====================\\n')\n",
        "    for key in model_eval_pool:\n",
        "        accs = accs_all_exps[key]\n",
        "        print('Run %d experiments, train on %s, evaluate %d random %s, mean  = %.2f%%  std = %.2f%%'%(args.num_exp, args.model, len(accs), key, np.mean(accs)*100, np.std(accs)*100))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self"
      ],
      "metadata": {
        "id": "mfSBWhSxqg8N"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Questions**"
      ],
      "metadata": {
        "id": "pRlr0294m5ld"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "QD_sRS0f2rr9"
      },
      "outputs": [],
      "source": [
        "#2A\n",
        "\n",
        "# Train MNIST using ConvNet 3 to establish baseline\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    \"dis_metric\": \"cos\",\n",
        "    \"lr_net\": 0.01,\n",
        "    \"epoch_eval_train\": 20,\n",
        "    \"batch_train\": 32,\n",
        "    \"training_baseline\": True,\n",
        "    \"dataset\": \"MNIST\"\n",
        "}\n",
        "\n",
        "args.update(args_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KA_ZeNaGkVL2",
        "outputId": "5b890876-b623-43c6-cbc1-6612702cc5df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-12-10 20:56:13] Evaluate_01: epoch = 0020 train time = 119 s train loss = 0.004321 train acc = 0.9997, test acc = 0.9950\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(ConvNet(\n",
              "   (features): Sequential(\n",
              "     (0): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3))\n",
              "     (1): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
              "     (2): ReLU(inplace=True)\n",
              "     (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "     (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "     (5): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
              "     (6): ReLU(inplace=True)\n",
              "     (7): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "     (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "     (9): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
              "     (10): ReLU(inplace=True)\n",
              "     (11): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "   )\n",
              "   (classifier): Linear(in_features=2048, out_features=10, bias=True)\n",
              " ),\n",
              " 0.9997,\n",
              " 0.995)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "# train MNIST\n",
        "\n",
        "mnist_path = \"/content/drive/MyDrive/ECE1512/Project_B/MNIST Dataset/\"\n",
        "\n",
        "MNIST_dataset = get_dataset(\"MNIST\", mnist_path)\n",
        "channel_mnist = MNIST_dataset[0]\n",
        "im_size_mnist = MNIST_dataset[1]\n",
        "num_classes_mnist = MNIST_dataset[2]\n",
        "class_names_mnist = MNIST_dataset[3]\n",
        "mean_mnist = MNIST_dataset[4]\n",
        "std_mnist = MNIST_dataset[5]\n",
        "dst_train_mnist = MNIST_dataset[6]\n",
        "dst_test_mnist = MNIST_dataset[7]\n",
        "testloader_mnist = MNIST_dataset[8]\n",
        "\n",
        "mnist_network = get_network(\"ConvNet\", channel_mnist, num_classes_mnist, im_size_mnist)\n",
        "\n",
        "evaluate_synset(1, mnist_network, dst_train_mnist.data, dst_train_mnist.targets, testloader_mnist, args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeqPH7UPtFIl",
        "outputId": "6abf6063-aa8f-48ba-d3cb-d7a948632d10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  Total KFLOPs  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                           aten::conv2d         1.04%      32.000us        58.34%       1.797ms     599.000us             3     96731.136  \n",
            "                                            aten::addmm         3.83%     118.000us         5.06%     156.000us     156.000us             1        40.960  \n",
            "                                        model_inference        17.08%     526.000us       100.00%       3.080ms       3.080ms             1            --  \n",
            "                                      aten::convolution         1.88%      58.000us        57.31%       1.765ms     588.333us             3            --  \n",
            "                                     aten::_convolution         2.34%      72.000us        55.42%       1.707ms     569.000us             3            --  \n",
            "                                aten::cudnn_convolution        45.97%       1.416ms        49.42%       1.522ms     507.333us             3            --  \n",
            "                                  cudaStreamIsCapturing         0.26%       8.000us         0.26%       8.000us       2.667us             3            --  \n",
            "                                  cudaStreamGetPriority         0.06%       2.000us         0.06%       2.000us       0.667us             3            --  \n",
            "                       cudaDeviceGetStreamPriorityRange         0.03%       1.000us         0.03%       1.000us       0.333us             3            --  \n",
            "                                       cudaLaunchKernel         6.95%     214.000us         6.95%     214.000us       8.231us            26            --  \n",
            "                                        cudaMemsetAsync         0.00%       0.000us         0.00%       0.000us       0.000us             1            --  \n",
            "                                          aten::reshape         0.29%       9.000us         0.49%      15.000us       5.000us             3            --  \n",
            "                                             aten::view         0.55%      17.000us         0.55%      17.000us       1.062us            16            --  \n",
            "                                             aten::add_         2.40%      74.000us         3.18%      98.000us      32.667us             3            --  \n",
            "                                       aten::group_norm         0.68%      21.000us        12.73%     392.000us     130.667us             3            --  \n",
            "                                aten::native_group_norm         6.72%     207.000us        12.05%     371.000us     123.667us             3            --  \n",
            "                                            aten::empty         3.28%     101.000us         3.28%     101.000us       6.733us            15            --  \n",
            "                                            aten::relu_         1.62%      50.000us         3.25%     100.000us      33.333us             3            --  \n",
            "                                       aten::clamp_min_         0.97%      30.000us         1.62%      50.000us      16.667us             3            --  \n",
            "                                       aten::avg_pool2d         1.72%      53.000us         2.40%      74.000us      24.667us             3            --  \n",
            "                                   cudaFuncSetAttribute         0.23%       7.000us         0.23%       7.000us       1.750us             4            --  \n",
            "                                    cudaLaunchKernelExC         0.45%      14.000us         0.45%      14.000us       7.000us             2            --  \n",
            "                                           aten::linear         0.39%      12.000us         6.01%     185.000us     185.000us             1            --  \n",
            "                                                aten::t         0.29%       9.000us         0.55%      17.000us      17.000us             1            --  \n",
            "                                        aten::transpose         0.19%       6.000us         0.26%       8.000us       8.000us             1            --  \n",
            "                                       aten::as_strided         0.06%       2.000us         0.06%       2.000us       2.000us             1            --  \n",
            "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.68%      21.000us         0.68%      21.000us      21.000us             1            --  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 3.080ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#MNIST FLOPS\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    \"dis_metric\": \"cos\",\n",
        "    \"lr_net\": 0.01,\n",
        "    \"epoch_eval_train\": 20,\n",
        "    \"batch_train\": 32,\n",
        "    \"training_baseline\": True,\n",
        "    \"dataset\": \"MNIST\"\n",
        "}\n",
        "\n",
        "args.update(args_dict)\n",
        "\n",
        "mnist_path = \"/content/drive/MyDrive/ECE1512/Project_B/MNIST Dataset/\"\n",
        "\n",
        "MNIST_dataset = get_dataset(\"MNIST\", mnist_path)\n",
        "channel_mnist = MNIST_dataset[0]\n",
        "im_size_mnist = MNIST_dataset[1]\n",
        "num_classes_mnist = MNIST_dataset[2]\n",
        "class_names_mnist = MNIST_dataset[3]\n",
        "mean_mnist = MNIST_dataset[4]\n",
        "std_mnist = MNIST_dataset[5]\n",
        "dst_train_mnist = MNIST_dataset[6]\n",
        "dst_test_mnist = MNIST_dataset[7]\n",
        "testloader_mnist = MNIST_dataset[8]\n",
        "\n",
        "mnist_network = get_network(\"ConvNet\", channel_mnist, num_classes_mnist, im_size_mnist)\n",
        "\n",
        "# Create dummy input\n",
        "dummy_input = torch.randn(1, 1, 28, 28).to('cuda')\n",
        "\n",
        "# Use profiler to record FLOPS\n",
        "with profile(activities=[ProfilerActivity.CPU], record_shapes=True, with_flops = True) as prof:\n",
        "    with record_function(\"model_inference\"):\n",
        "        mnist_network(dummy_input)\n",
        "\n",
        "# Print the FLOPS\n",
        "print(prof.key_averages().table(sort_by=\"flops\", row_limit=-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKyhyyOol2sP",
        "outputId": "587d99ab-d916-40c4-c9dd-249081d6a8e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done loading images\n",
            "[2023-12-10 22:17:49] Evaluate_01: epoch = 0020 train time = 27 s train loss = 0.138041 train acc = 0.9706, test acc = 0.7410\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(ConvNet(\n",
              "   (features): Sequential(\n",
              "     (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "     (1): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
              "     (2): ReLU(inplace=True)\n",
              "     (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "     (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "     (5): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
              "     (6): ReLU(inplace=True)\n",
              "     (7): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "     (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "     (9): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
              "     (10): ReLU(inplace=True)\n",
              "     (11): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "   )\n",
              "   (classifier): Linear(in_features=32768, out_features=2, bias=True)\n",
              " ),\n",
              " 0.9705747126436781,\n",
              " 0.7410440122824974)"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ],
      "source": [
        "# Train MHIST using ConvNet to establish baseline\n",
        "mhist_path = \"/content/drive/MyDrive/ECE1512/Project_B/MHIST Dataset/\"\n",
        "\n",
        "MHIST_dataset = get_dataset(\"MHIST\", mhist_path)\n",
        "channel_mhist = MHIST_dataset[0]\n",
        "im_size_mhist = MHIST_dataset[1]\n",
        "num_classes_mhist = MHIST_dataset[2]\n",
        "class_names_mhist = MHIST_dataset[3]\n",
        "mean_mhist = MHIST_dataset[4]\n",
        "std_mhist = MHIST_dataset[5]\n",
        "dst_train_mhist = MHIST_dataset[6]\n",
        "dst_test_mhist = MHIST_dataset[7]\n",
        "testloader_mhist = MHIST_dataset[8]\n",
        "\n",
        "mhist_network = get_network(\"ConvNet\", channel_mhist, num_classes_mhist, im_size_mhist)\n",
        "\n",
        "evaluate_synset(1, mhist_network, dst_train_mhist.data, dst_train_mhist.targets, testloader_mhist, args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "WPMtUZW1waKL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ae727ce-4a90-4ac1-ceb1-fe9570911ff0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done loading images\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  Total KFLOPs  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                           aten::conv2d         0.10%      20.000us        65.57%      13.020ms       4.340ms             3   1623195.648  \n",
            "                                            aten::addmm        27.26%       5.412ms        27.53%       5.467ms       5.467ms             1       131.072  \n",
            "                                        model_inference         3.33%     662.000us       100.00%      19.856ms      19.856ms             1            --  \n",
            "                                      aten::convolution         0.32%      64.000us        65.47%      13.000ms       4.333ms             3            --  \n",
            "                                     aten::_convolution         0.66%     132.000us        65.15%      12.936ms       4.312ms             3            --  \n",
            "                                aten::cudnn_convolution        61.99%      12.309ms        63.68%      12.645ms       4.215ms             3            --  \n",
            "                                  cudaStreamIsCapturing         0.10%      20.000us         0.10%      20.000us       6.667us             3            --  \n",
            "                                  cudaStreamGetPriority         0.16%      32.000us         0.16%      32.000us      10.667us             3            --  \n",
            "                       cudaDeviceGetStreamPriorityRange         0.01%       2.000us         0.01%       2.000us       0.667us             3            --  \n",
            "                                       cudaLaunchKernel         2.01%     400.000us         2.01%     400.000us      14.815us            27            --  \n",
            "                                          aten::reshape         0.05%      10.000us         0.10%      19.000us       6.333us             3            --  \n",
            "                                             aten::view         0.10%      20.000us         0.10%      20.000us       1.250us            16            --  \n",
            "                                             aten::add_         0.49%      98.000us         0.71%     140.000us      46.667us             3            --  \n",
            "                                       aten::group_norm         0.19%      38.000us         2.23%     443.000us     147.667us             3            --  \n",
            "                                aten::native_group_norm         1.25%     248.000us         2.04%     405.000us     135.000us             3            --  \n",
            "                                            aten::empty         0.45%      89.000us         0.45%      89.000us       5.933us            15            --  \n",
            "                                            aten::relu_         0.30%      59.000us         0.58%     115.000us      38.333us             3            --  \n",
            "                                       aten::clamp_min_         0.18%      35.000us         0.28%      56.000us      18.667us             3            --  \n",
            "                                       aten::avg_pool2d         0.47%      93.000us         0.57%     114.000us      38.000us             3            --  \n",
            "                                   cudaFuncSetAttribute         0.12%      23.000us         0.12%      23.000us       5.750us             4            --  \n",
            "                                    cudaLaunchKernelExC         0.21%      41.000us         0.21%      41.000us      20.500us             2            --  \n",
            "                                           aten::linear         0.07%      13.000us        27.67%       5.495ms       5.495ms             1            --  \n",
            "                                                aten::t         0.04%       7.000us         0.08%      15.000us      15.000us             1            --  \n",
            "                                        aten::transpose         0.03%       6.000us         0.04%       8.000us       8.000us             1            --  \n",
            "                                       aten::as_strided         0.01%       2.000us         0.01%       2.000us       2.000us             1            --  \n",
            "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.11%      21.000us         0.11%      21.000us      21.000us             1            --  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 19.856ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#MHIST FLOPS\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    \"dis_metric\": \"cos\",\n",
        "    \"lr_net\": 0.01,\n",
        "    \"epoch_eval_train\": 20,\n",
        "    \"batch_train\": 32,\n",
        "    \"training_baseline\": True,\n",
        "    \"dataset\": \"MHIST\"\n",
        "}\n",
        "\n",
        "args.update(args_dict)\n",
        "\n",
        "mhist_path = \"/content/drive/MyDrive/ECE1512/Project_B/MHIST Dataset/\"\n",
        "\n",
        "MHIST_dataset = get_dataset(\"MHIST\", mhist_path)\n",
        "channel_mhist = MHIST_dataset[0]\n",
        "im_size_mhist = MHIST_dataset[1]\n",
        "num_classes_mhist = MHIST_dataset[2]\n",
        "class_names_mhist = MHIST_dataset[3]\n",
        "mean_mhist = MHIST_dataset[4]\n",
        "std_mhist = MHIST_dataset[5]\n",
        "dst_train_mhist = MHIST_dataset[6]\n",
        "dst_test_mhist = MHIST_dataset[7]\n",
        "testloader_mhist = MHIST_dataset[8]\n",
        "\n",
        "mhist_network = get_network(\"ConvNet\", channel_mhist, num_classes_mhist, im_size_mhist)\n",
        "\n",
        "# Create dummy input\n",
        "dummy_input = torch.randn(1, 3, 128, 128).to('cuda')\n",
        "\n",
        "# Use profiler to record FLOPS\n",
        "with profile(activities=[ProfilerActivity.CPU], record_shapes=True, with_flops = True) as prof:\n",
        "    with record_function(\"model_inference\"):\n",
        "        mhist_network(dummy_input)\n",
        "\n",
        "# Print the FLOPS\n",
        "print(prof.key_averages().table(sort_by=\"flops\", row_limit=-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7py1nBzny40",
        "outputId": "8fe1c0df-bb73-462a-ebe7-cfe5ab06a156"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RUNNING ON GPU\n",
            "eval_it_pool:  [0]\n",
            "\n",
            "================== Exp 0 ==================\n",
            " \n",
            "Hyper-parameters: \n",
            " {'method': 'DC', 'dataset': 'MNIST', 'model': 'ConvNet', 'ipc': 10, 'eval_mode': 'S', 'num_exp': 1, 'num_eval': 20, 'epoch_eval_train': 300, 'Iteration': 10, 'lr_img': 0.1, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'real', 'dsa_strategy': 'None', 'data_path': '/content/drive/MyDrive/ECE1512/Project_B/', 'save_path': '/content/drive/MyDrive/ECE1512/Project_B/Runs/MNIST Initial/', 'dis_metric': 'ours', 'training_baseline': False, 'outer_loop': 10, 'inner_loop': 50, 'device': 'cuda', 'dsa_param': <__main__.ParamDiffAug object at 0x7cd95210c970>, 'dsa': False}\n",
            "Evaluation model pool:  ['ConvNet']\n",
            "class c = 0: 5923 real images\n",
            "class c = 1: 6742 real images\n",
            "class c = 2: 5958 real images\n",
            "class c = 3: 6131 real images\n",
            "class c = 4: 5842 real images\n",
            "class c = 5: 5421 real images\n",
            "class c = 6: 5918 real images\n",
            "class c = 7: 6265 real images\n",
            "class c = 8: 5851 real images\n",
            "class c = 9: 5949 real images\n",
            "real images channel 0, mean = -0.0001, std = 1.0000\n",
            "initialize synthetic data from random real images\n",
            "[2023-12-10 16:06:59] training begins\n",
            "-------------------------\n",
            "Evaluation\n",
            "model_train = ConvNet, model_eval = ConvNet, iteration = 0\n",
            "DC augmentation parameters: \n",
            " {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'crop_scale_rotate'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-936200a5d27c>:88: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
            "  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-12-10 16:07:40] Evaluate_00: epoch = 1000 train time = 39 s train loss = 0.003873 train acc = 1.0000, test acc = 0.9558\n",
            "[2023-12-10 16:08:14] Evaluate_01: epoch = 1000 train time = 31 s train loss = 0.005113 train acc = 1.0000, test acc = 0.9577\n",
            "[2023-12-10 16:08:48] Evaluate_02: epoch = 1000 train time = 31 s train loss = 0.006244 train acc = 1.0000, test acc = 0.9538\n",
            "[2023-12-10 16:09:21] Evaluate_03: epoch = 1000 train time = 31 s train loss = 0.003648 train acc = 1.0000, test acc = 0.9559\n",
            "[2023-12-10 16:09:55] Evaluate_04: epoch = 1000 train time = 31 s train loss = 0.005302 train acc = 1.0000, test acc = 0.9524\n",
            "[2023-12-10 16:10:29] Evaluate_05: epoch = 1000 train time = 31 s train loss = 0.005753 train acc = 1.0000, test acc = 0.9581\n",
            "[2023-12-10 16:11:03] Evaluate_06: epoch = 1000 train time = 31 s train loss = 0.008147 train acc = 1.0000, test acc = 0.9568\n",
            "[2023-12-10 16:11:36] Evaluate_07: epoch = 1000 train time = 31 s train loss = 0.005655 train acc = 1.0000, test acc = 0.9565\n",
            "[2023-12-10 16:12:10] Evaluate_08: epoch = 1000 train time = 31 s train loss = 0.003631 train acc = 1.0000, test acc = 0.9549\n",
            "[2023-12-10 16:12:44] Evaluate_09: epoch = 1000 train time = 31 s train loss = 0.006405 train acc = 1.0000, test acc = 0.9547\n",
            "[2023-12-10 16:13:17] Evaluate_10: epoch = 1000 train time = 31 s train loss = 0.007421 train acc = 1.0000, test acc = 0.9530\n",
            "[2023-12-10 16:13:51] Evaluate_11: epoch = 1000 train time = 31 s train loss = 0.013556 train acc = 1.0000, test acc = 0.9549\n",
            "[2023-12-10 16:14:25] Evaluate_12: epoch = 1000 train time = 31 s train loss = 0.005612 train acc = 1.0000, test acc = 0.9593\n",
            "[2023-12-10 16:14:58] Evaluate_13: epoch = 1000 train time = 31 s train loss = 0.005515 train acc = 1.0000, test acc = 0.9553\n",
            "[2023-12-10 16:15:32] Evaluate_14: epoch = 1000 train time = 31 s train loss = 0.010411 train acc = 1.0000, test acc = 0.9565\n",
            "[2023-12-10 16:16:05] Evaluate_15: epoch = 1000 train time = 31 s train loss = 0.003831 train acc = 1.0000, test acc = 0.9573\n",
            "[2023-12-10 16:16:38] Evaluate_16: epoch = 1000 train time = 31 s train loss = 0.004845 train acc = 1.0000, test acc = 0.9556\n",
            "[2023-12-10 16:17:12] Evaluate_17: epoch = 1000 train time = 31 s train loss = 0.007721 train acc = 1.0000, test acc = 0.9537\n",
            "[2023-12-10 16:17:45] Evaluate_18: epoch = 1000 train time = 31 s train loss = 0.006064 train acc = 1.0000, test acc = 0.9567\n",
            "[2023-12-10 16:18:18] Evaluate_19: epoch = 1000 train time = 31 s train loss = 0.006028 train acc = 1.0000, test acc = 0.9551\n",
            "Evaluate 20 random ConvNet, mean = 0.9557 std = 0.0017\n",
            "-------------------------\n",
            "[2023-12-10 16:18:22] iter = 0000, loss = 44.5712\n",
            "[2023-12-10 16:18:56] iter = 0010, loss = 36.1227\n",
            "\n",
            "==================== Final Results ====================\n",
            "\n",
            "Run 1 experiments, train on ConvNet, evaluate 0 random ConvNet, mean  = nan%  std = nan%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n",
            "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        }
      ],
      "source": [
        "#2B\n",
        "\n",
        "#MNIST\n",
        "\n",
        "main(dataset=\"MNIST\", model=\"ConvNet\", iteration = 10, batch_real=256, batch_train=256, num_exp=1,\n",
        "     ipc=10, init=\"real\", save_path=\"/content/drive/MyDrive/ECE1512/Project_B/Runs/MNIST Initial/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2hJMdW_oy5C",
        "outputId": "73bc24e5-0f68-426f-aff9-7f6017a43eaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RUNNING ON GPU\n",
            "eval_it_pool:  [0]\n",
            "done loading images\n",
            "\n",
            "================== Exp 0 ==================\n",
            " \n",
            "Hyper-parameters: \n",
            " {'method': 'DC', 'dataset': 'MHIST', 'model': 'ConvNet', 'ipc': 50, 'eval_mode': 'S', 'num_exp': 1, 'num_eval': 20, 'epoch_eval_train': 300, 'Iteration': 10, 'lr_img': 0.1, 'lr_net': 0.01, 'batch_real': 128, 'batch_train': 128, 'init': 'real', 'dsa_strategy': 'None', 'data_path': '/content/drive/MyDrive/ECE1512/Project_B/', 'save_path': '/content/drive/MyDrive/ECE1512/Project_B/Runs/MHIST Initial/', 'dis_metric': 'ours', 'training_baseline': False, 'outer_loop': 50, 'inner_loop': 10, 'device': 'cuda', 'dsa_param': <__main__.ParamDiffAug object at 0x7baa2dced210>, 'dsa': False}\n",
            "Evaluation model pool:  ['ConvNet']\n",
            "class c = 0: 1545 real images\n",
            "class c = 1: 630 real images\n",
            "real images channel 0, mean = 0.7195, std = 0.1973\n",
            "real images channel 1, mean = 0.6218, std = 0.2513\n",
            "real images channel 2, mean = 0.7605, std = 0.1662\n",
            "initialize synthetic data from random real images\n",
            "[2023-12-10 22:19:08] training begins\n",
            "-------------------------\n",
            "Evaluation\n",
            "model_train = ConvNet, model_eval = ConvNet, iteration = 0\n",
            "DC augmentation parameters: \n",
            " {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}\n",
            "[2023-12-10 22:19:27] Evaluate_00: epoch = 0300 train time = 18 s train loss = 0.000001 train acc = 1.0000, test acc = 0.5241\n",
            "[2023-12-10 22:19:45] Evaluate_01: epoch = 0300 train time = 18 s train loss = 0.000008 train acc = 1.0000, test acc = 0.5251\n",
            "[2023-12-10 22:20:03] Evaluate_02: epoch = 0300 train time = 18 s train loss = 0.000009 train acc = 1.0000, test acc = 0.5589\n",
            "[2023-12-10 22:20:22] Evaluate_03: epoch = 0300 train time = 18 s train loss = 0.000006 train acc = 1.0000, test acc = 0.5312\n",
            "[2023-12-10 22:20:40] Evaluate_04: epoch = 0300 train time = 18 s train loss = 0.000000 train acc = 1.0000, test acc = 0.5189\n",
            "[2023-12-10 22:20:59] Evaluate_05: epoch = 0300 train time = 18 s train loss = 0.000003 train acc = 1.0000, test acc = 0.5271\n",
            "[2023-12-10 22:21:17] Evaluate_06: epoch = 0300 train time = 18 s train loss = 0.000004 train acc = 1.0000, test acc = 0.5681\n",
            "[2023-12-10 22:21:35] Evaluate_07: epoch = 0300 train time = 18 s train loss = 0.000005 train acc = 1.0000, test acc = 0.4637\n",
            "[2023-12-10 22:21:54] Evaluate_08: epoch = 0300 train time = 18 s train loss = 0.000003 train acc = 1.0000, test acc = 0.5220\n",
            "[2023-12-10 22:22:12] Evaluate_09: epoch = 0300 train time = 18 s train loss = 0.000002 train acc = 1.0000, test acc = 0.5363\n",
            "[2023-12-10 22:22:30] Evaluate_10: epoch = 0300 train time = 18 s train loss = 0.000003 train acc = 1.0000, test acc = 0.5159\n",
            "[2023-12-10 22:22:49] Evaluate_11: epoch = 0300 train time = 18 s train loss = 0.000015 train acc = 1.0000, test acc = 0.4985\n",
            "[2023-12-10 22:23:07] Evaluate_12: epoch = 0300 train time = 18 s train loss = 0.000004 train acc = 1.0000, test acc = 0.5353\n",
            "[2023-12-10 22:23:26] Evaluate_13: epoch = 0300 train time = 18 s train loss = 0.000017 train acc = 1.0000, test acc = 0.5148\n",
            "[2023-12-10 22:23:44] Evaluate_14: epoch = 0300 train time = 18 s train loss = 0.000019 train acc = 1.0000, test acc = 0.5404\n",
            "[2023-12-10 22:24:02] Evaluate_15: epoch = 0300 train time = 18 s train loss = 0.000015 train acc = 1.0000, test acc = 0.5548\n",
            "[2023-12-10 22:24:21] Evaluate_16: epoch = 0300 train time = 18 s train loss = 0.000005 train acc = 1.0000, test acc = 0.5281\n",
            "[2023-12-10 22:24:39] Evaluate_17: epoch = 0300 train time = 18 s train loss = 0.000008 train acc = 1.0000, test acc = 0.5312\n",
            "[2023-12-10 22:24:58] Evaluate_18: epoch = 0300 train time = 18 s train loss = 0.000000 train acc = 1.0000, test acc = 0.5384\n",
            "[2023-12-10 22:25:16] Evaluate_19: epoch = 0300 train time = 18 s train loss = 0.000004 train acc = 1.0000, test acc = 0.5138\n",
            "Evaluate 20 random ConvNet, mean = 0.5273 std = 0.0216\n",
            "-------------------------\n",
            "tensor(0.)\n",
            "tensor(1.)\n",
            "[2023-12-10 22:26:08] iter = 0000, loss = 208.3594\n",
            "[2023-12-10 22:34:34] iter = 0010, loss = 174.5578\n",
            "\n",
            "==================== Final Results ====================\n",
            "\n",
            "Run 1 experiments, train on ConvNet, evaluate 0 random ConvNet, mean  = nan%  std = nan%\n"
          ]
        }
      ],
      "source": [
        "#MHIST\n",
        "\n",
        "main(dataset=\"MHIST\", model=\"ConvNet\", iteration = 10, batch_real=128, batch_train=128, num_exp=1,\n",
        "     ipc=50, init=\"real\", save_path=\"/content/drive/MyDrive/ECE1512/Project_B/Runs/MHIST Initial/\")\n",
        "\n",
        "mhist_path = \"/content/drive/MyDrive/ECE1512/Project_B/MHIST Dataset/\"\n",
        "\n",
        "MHIST_dataset = get_dataset(\"MHIST\", mhist_path)\n",
        "channel_mhist = MHIST_dataset[0]\n",
        "im_size_mhist = MHIST_dataset[1]\n",
        "num_classes_mhist = MHIST_dataset[2]\n",
        "class_names_mhist = MHIST_dataset[3]\n",
        "mean_mhist = MHIST_dataset[4]\n",
        "std_mhist = MHIST_dataset[5]\n",
        "dst_train_mhist = MHIST_dataset[6]\n",
        "dst_test_mhist = MHIST_dataset[7]\n",
        "testloader_mhist = MHIST_dataset[8]\n",
        "\n",
        "mhist_network = get_network(\"ConvNet\", channel_mhist, num_classes_mhist, im_size_mhist)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2D MNIST (Longer)\n",
        "\n",
        "main(dataset=\"MNIST\", model=\"ConvNet\", iteration = 10, batch_real=256, batch_train=256, num_exp=1,\n",
        "     ipc=10, save_path=\"/content/drive/MyDrive/ECE1512/Project_B/Runs/MNIST Noise/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DU-3UBND2xWz",
        "outputId": "d8a0456c-64d4-4765-f4cd-998e55259d26"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RUNNING ON GPU\n",
            "eval_it_pool:  [0, 10]\n",
            "\n",
            "================== Exp 0 ==================\n",
            " \n",
            "Hyper-parameters: \n",
            " {'method': 'DC', 'dataset': 'MNIST', 'model': 'ConvNet', 'ipc': 10, 'eval_mode': 'S', 'num_exp': 1, 'num_eval': 20, 'epoch_eval_train': 300, 'Iteration': 10, 'lr_img': 0.1, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'noise', 'dsa_strategy': 'None', 'data_path': '/content/drive/MyDrive/ECE1512/Project_B/', 'save_path': '/content/drive/MyDrive/ECE1512/Project_B/Runs/MNIST Noise/', 'dis_metric': 'ours', 'training_baseline': False, 'outer_loop': 10, 'inner_loop': 50, 'device': 'cuda', 'dsa_param': <__main__.ParamDiffAug object at 0x7c157558a230>, 'dsa': False}\n",
            "Evaluation model pool:  ['ConvNet']\n",
            "class c = 0: 5923 real images\n",
            "class c = 1: 6742 real images\n",
            "class c = 2: 5958 real images\n",
            "class c = 3: 6131 real images\n",
            "class c = 4: 5842 real images\n",
            "class c = 5: 5421 real images\n",
            "class c = 6: 5918 real images\n",
            "class c = 7: 6265 real images\n",
            "class c = 8: 5851 real images\n",
            "class c = 9: 5949 real images\n",
            "real images channel 0, mean = -0.0001, std = 1.0000\n",
            "initialize synthetic data from random noise\n",
            "[2023-12-10 23:42:43] training begins\n",
            "-------------------------\n",
            "Evaluation\n",
            "model_train = ConvNet, model_eval = ConvNet, iteration = 0\n",
            "DC augmentation parameters: \n",
            " {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'crop_scale_rotate'}\n",
            "[2023-12-10 23:42:52] Evaluate_00: epoch = 1000 train time = 6 s train loss = 0.001651 train acc = 1.0000, test acc = 0.1572\n",
            "[2023-12-10 23:43:00] Evaluate_01: epoch = 1000 train time = 6 s train loss = 0.001652 train acc = 1.0000, test acc = 0.1592\n",
            "[2023-12-10 23:43:08] Evaluate_02: epoch = 1000 train time = 6 s train loss = 0.001650 train acc = 1.0000, test acc = 0.1655\n",
            "[2023-12-10 23:43:17] Evaluate_03: epoch = 1000 train time = 6 s train loss = 0.001616 train acc = 1.0000, test acc = 0.1860\n",
            "[2023-12-10 23:43:25] Evaluate_04: epoch = 1000 train time = 6 s train loss = 0.001605 train acc = 1.0000, test acc = 0.0747\n",
            "[2023-12-10 23:43:34] Evaluate_05: epoch = 1000 train time = 6 s train loss = 0.001622 train acc = 1.0000, test acc = 0.1553\n",
            "[2023-12-10 23:43:42] Evaluate_06: epoch = 1000 train time = 6 s train loss = 0.001615 train acc = 1.0000, test acc = 0.0781\n",
            "[2023-12-10 23:43:51] Evaluate_07: epoch = 1000 train time = 6 s train loss = 0.001625 train acc = 1.0000, test acc = 0.1693\n",
            "[2023-12-10 23:43:59] Evaluate_08: epoch = 1000 train time = 6 s train loss = 0.001621 train acc = 1.0000, test acc = 0.1643\n",
            "[2023-12-10 23:44:08] Evaluate_09: epoch = 1000 train time = 6 s train loss = 0.001624 train acc = 1.0000, test acc = 0.1220\n",
            "[2023-12-10 23:44:16] Evaluate_10: epoch = 1000 train time = 6 s train loss = 0.001636 train acc = 1.0000, test acc = 0.1323\n",
            "[2023-12-10 23:44:25] Evaluate_11: epoch = 1000 train time = 6 s train loss = 0.001653 train acc = 1.0000, test acc = 0.1564\n",
            "[2023-12-10 23:44:33] Evaluate_12: epoch = 1000 train time = 6 s train loss = 0.001637 train acc = 1.0000, test acc = 0.1185\n",
            "[2023-12-10 23:44:41] Evaluate_13: epoch = 1000 train time = 6 s train loss = 0.001610 train acc = 1.0000, test acc = 0.1123\n",
            "[2023-12-10 23:44:50] Evaluate_14: epoch = 1000 train time = 6 s train loss = 0.001623 train acc = 1.0000, test acc = 0.1300\n",
            "[2023-12-10 23:44:58] Evaluate_15: epoch = 1000 train time = 6 s train loss = 0.001651 train acc = 1.0000, test acc = 0.1707\n",
            "[2023-12-10 23:45:07] Evaluate_16: epoch = 1000 train time = 6 s train loss = 0.001547 train acc = 1.0000, test acc = 0.1419\n",
            "[2023-12-10 23:45:15] Evaluate_17: epoch = 1000 train time = 6 s train loss = 0.001630 train acc = 1.0000, test acc = 0.2021\n",
            "[2023-12-10 23:45:24] Evaluate_18: epoch = 1000 train time = 6 s train loss = 0.001606 train acc = 1.0000, test acc = 0.1408\n",
            "[2023-12-10 23:45:32] Evaluate_19: epoch = 1000 train time = 6 s train loss = 0.001623 train acc = 1.0000, test acc = 0.1487\n",
            "Evaluate 20 random ConvNet, mean = 0.1443 std = 0.0314\n",
            "-------------------------\n",
            "[2023-12-10 23:45:38] iter = 0000, loss = 223.2862\n",
            "-------------------------\n",
            "Evaluation\n",
            "model_train = ConvNet, model_eval = ConvNet, iteration = 10\n",
            "DC augmentation parameters: \n",
            " {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'crop_scale_rotate'}\n",
            "[2023-12-10 23:46:32] Evaluate_00: epoch = 1000 train time = 6 s train loss = 0.001557 train acc = 1.0000, test acc = 0.8626\n",
            "[2023-12-10 23:46:41] Evaluate_01: epoch = 1000 train time = 6 s train loss = 0.001507 train acc = 1.0000, test acc = 0.8671\n",
            "[2023-12-10 23:46:49] Evaluate_02: epoch = 1000 train time = 6 s train loss = 0.001508 train acc = 1.0000, test acc = 0.8538\n",
            "[2023-12-10 23:46:58] Evaluate_03: epoch = 1000 train time = 6 s train loss = 0.001576 train acc = 1.0000, test acc = 0.8570\n",
            "[2023-12-10 23:47:06] Evaluate_04: epoch = 1000 train time = 6 s train loss = 0.001500 train acc = 1.0000, test acc = 0.8703\n",
            "[2023-12-10 23:47:15] Evaluate_05: epoch = 1000 train time = 6 s train loss = 0.001559 train acc = 1.0000, test acc = 0.8637\n",
            "[2023-12-10 23:47:23] Evaluate_06: epoch = 1000 train time = 6 s train loss = 0.001564 train acc = 1.0000, test acc = 0.8624\n",
            "[2023-12-10 23:47:32] Evaluate_07: epoch = 1000 train time = 6 s train loss = 0.001570 train acc = 1.0000, test acc = 0.8624\n",
            "[2023-12-10 23:47:40] Evaluate_08: epoch = 1000 train time = 6 s train loss = 0.001594 train acc = 1.0000, test acc = 0.8571\n",
            "[2023-12-10 23:47:49] Evaluate_09: epoch = 1000 train time = 6 s train loss = 0.001582 train acc = 1.0000, test acc = 0.8421\n",
            "[2023-12-10 23:47:57] Evaluate_10: epoch = 1000 train time = 6 s train loss = 0.001527 train acc = 1.0000, test acc = 0.8581\n",
            "[2023-12-10 23:48:06] Evaluate_11: epoch = 1000 train time = 6 s train loss = 0.001526 train acc = 1.0000, test acc = 0.8383\n",
            "[2023-12-10 23:48:14] Evaluate_12: epoch = 1000 train time = 6 s train loss = 0.001591 train acc = 1.0000, test acc = 0.8494\n",
            "[2023-12-10 23:48:22] Evaluate_13: epoch = 1000 train time = 6 s train loss = 0.001543 train acc = 1.0000, test acc = 0.8521\n",
            "[2023-12-10 23:48:31] Evaluate_14: epoch = 1000 train time = 6 s train loss = 0.001589 train acc = 1.0000, test acc = 0.8613\n",
            "[2023-12-10 23:48:39] Evaluate_15: epoch = 1000 train time = 6 s train loss = 0.001514 train acc = 1.0000, test acc = 0.8598\n",
            "[2023-12-10 23:48:48] Evaluate_16: epoch = 1000 train time = 6 s train loss = 0.001572 train acc = 1.0000, test acc = 0.8616\n",
            "[2023-12-10 23:48:57] Evaluate_17: epoch = 1000 train time = 6 s train loss = 0.001515 train acc = 1.0000, test acc = 0.8487\n",
            "[2023-12-10 23:49:05] Evaluate_18: epoch = 1000 train time = 6 s train loss = 0.001561 train acc = 1.0000, test acc = 0.8647\n",
            "[2023-12-10 23:49:13] Evaluate_19: epoch = 1000 train time = 6 s train loss = 0.001551 train acc = 1.0000, test acc = 0.8632\n",
            "Evaluate 20 random ConvNet, mean = 0.8578 std = 0.0080\n",
            "-------------------------\n",
            "[2023-12-10 23:49:19] iter = 0010, loss = 93.8560\n",
            "\n",
            "==================== Final Results ====================\n",
            "\n",
            "Run 1 experiments, train on ConvNet, evaluate 20 random ConvNet, mean  = 85.78%  std = 0.80%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2D MHIST (Longer)\n",
        "\n",
        "main(dataset=\"MHIST\", model=\"ConvNet\", iteration = 10, batch_real=128, batch_train=128, num_exp=1,\n",
        "     ipc=50, init=\"noise\", save_path=\"/content/drive/MyDrive/ECE1512/Project_B/Runs/MHIST Noise/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMlZsT-I20gd",
        "outputId": "14ec4f06-1b24-40b7-f9f4-51666bf2e816"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RUNNING ON GPU\n",
            "eval_it_pool:  [0, 10]\n",
            "done loading images\n",
            "\n",
            "================== Exp 0 ==================\n",
            " \n",
            "Hyper-parameters: \n",
            " {'method': 'DC', 'dataset': 'MHIST', 'model': 'ConvNet', 'ipc': 50, 'eval_mode': 'S', 'num_exp': 1, 'num_eval': 20, 'epoch_eval_train': 300, 'Iteration': 10, 'lr_img': 0.1, 'lr_net': 0.01, 'batch_real': 128, 'batch_train': 128, 'init': 'noise', 'dsa_strategy': 'None', 'data_path': '/content/drive/MyDrive/ECE1512/Project_B/', 'save_path': '/content/drive/MyDrive/ECE1512/Project_B/Runs/MHIST Noise/', 'dis_metric': 'ours', 'training_baseline': False, 'outer_loop': 50, 'inner_loop': 10, 'device': 'cuda', 'dsa_param': <__main__.ParamDiffAug object at 0x7c1575bf80d0>, 'dsa': False}\n",
            "Evaluation model pool:  ['ConvNet']\n",
            "class c = 0: 1545 real images\n",
            "class c = 1: 630 real images\n",
            "real images channel 0, mean = 0.7195, std = 0.1973\n",
            "real images channel 1, mean = 0.6218, std = 0.2513\n",
            "real images channel 2, mean = 0.7605, std = 0.1662\n",
            "initialize synthetic data from random noise\n",
            "[2023-12-10 22:58:19] training begins\n",
            "-------------------------\n",
            "Evaluation\n",
            "model_train = ConvNet, model_eval = ConvNet, iteration = 0\n",
            "DC augmentation parameters: \n",
            " {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}\n",
            "[2023-12-10 22:58:39] Evaluate_00: epoch = 0300 train time = 19 s train loss = 0.000000 train acc = 1.0000, test acc = 0.3849\n",
            "[2023-12-10 22:58:58] Evaluate_01: epoch = 0300 train time = 18 s train loss = 0.000005 train acc = 1.0000, test acc = 0.5783\n",
            "[2023-12-10 22:59:16] Evaluate_02: epoch = 0300 train time = 18 s train loss = 0.000000 train acc = 1.0000, test acc = 0.5568\n",
            "[2023-12-10 22:59:35] Evaluate_03: epoch = 0300 train time = 18 s train loss = 0.000004 train acc = 1.0000, test acc = 0.3767\n",
            "[2023-12-10 22:59:54] Evaluate_04: epoch = 0300 train time = 18 s train loss = 0.000000 train acc = 1.0000, test acc = 0.5906\n",
            "[2023-12-10 23:00:13] Evaluate_05: epoch = 0300 train time = 18 s train loss = 0.000002 train acc = 1.0000, test acc = 0.5466\n",
            "[2023-12-10 23:00:31] Evaluate_06: epoch = 0300 train time = 18 s train loss = 0.000000 train acc = 1.0000, test acc = 0.4401\n",
            "[2023-12-10 23:00:50] Evaluate_07: epoch = 0300 train time = 18 s train loss = 0.000003 train acc = 1.0000, test acc = 0.3838\n",
            "[2023-12-10 23:01:09] Evaluate_08: epoch = 0300 train time = 18 s train loss = 0.000000 train acc = 1.0000, test acc = 0.5793\n",
            "[2023-12-10 23:01:27] Evaluate_09: epoch = 0300 train time = 18 s train loss = 0.000019 train acc = 1.0000, test acc = 0.5394\n",
            "[2023-12-10 23:01:46] Evaluate_10: epoch = 0300 train time = 18 s train loss = 0.000003 train acc = 1.0000, test acc = 0.6182\n",
            "[2023-12-10 23:02:04] Evaluate_11: epoch = 0300 train time = 18 s train loss = 0.000004 train acc = 1.0000, test acc = 0.6192\n",
            "[2023-12-10 23:02:23] Evaluate_12: epoch = 0300 train time = 18 s train loss = 0.000001 train acc = 1.0000, test acc = 0.5957\n",
            "[2023-12-10 23:02:42] Evaluate_13: epoch = 0300 train time = 18 s train loss = 0.000000 train acc = 1.0000, test acc = 0.5875\n",
            "[2023-12-10 23:03:00] Evaluate_14: epoch = 0300 train time = 18 s train loss = 0.000000 train acc = 1.0000, test acc = 0.4974\n",
            "[2023-12-10 23:03:19] Evaluate_15: epoch = 0300 train time = 18 s train loss = 0.000000 train acc = 1.0000, test acc = 0.5281\n",
            "[2023-12-10 23:03:37] Evaluate_16: epoch = 0300 train time = 18 s train loss = 0.000000 train acc = 1.0000, test acc = 0.4882\n",
            "[2023-12-10 23:03:56] Evaluate_17: epoch = 0300 train time = 18 s train loss = 0.000000 train acc = 1.0000, test acc = 0.5138\n",
            "[2023-12-10 23:04:15] Evaluate_18: epoch = 0300 train time = 18 s train loss = 0.000000 train acc = 1.0000, test acc = 0.4514\n",
            "[2023-12-10 23:04:33] Evaluate_19: epoch = 0300 train time = 18 s train loss = 0.000004 train acc = 1.0000, test acc = 0.6223\n",
            "Evaluate 20 random ConvNet, mean = 0.5249 std = 0.0788\n",
            "-------------------------\n",
            "tensor(0.)\n",
            "tensor(1.)\n",
            "[2023-12-10 23:05:26] iter = 0000, loss = 325.5974\n",
            "-------------------------\n",
            "Evaluation\n",
            "model_train = ConvNet, model_eval = ConvNet, iteration = 10\n",
            "DC augmentation parameters: \n",
            " {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}\n",
            "[2023-12-10 23:13:23] Evaluate_00: epoch = 0300 train time = 18 s train loss = 0.000000 train acc = 1.0000, test acc = 0.5783\n",
            "[2023-12-10 23:13:42] Evaluate_01: epoch = 0300 train time = 18 s train loss = 0.000000 train acc = 1.0000, test acc = 0.5824\n",
            "[2023-12-10 23:14:01] Evaluate_02: epoch = 0300 train time = 18 s train loss = 0.000001 train acc = 1.0000, test acc = 0.3869\n",
            "[2023-12-10 23:14:19] Evaluate_03: epoch = 0300 train time = 18 s train loss = 0.000003 train acc = 1.0000, test acc = 0.5210\n",
            "[2023-12-10 23:14:38] Evaluate_04: epoch = 0300 train time = 18 s train loss = 0.000005 train acc = 1.0000, test acc = 0.5107\n",
            "[2023-12-10 23:14:56] Evaluate_05: epoch = 0300 train time = 18 s train loss = 0.000000 train acc = 1.0000, test acc = 0.5415\n",
            "[2023-12-10 23:15:15] Evaluate_06: epoch = 0300 train time = 18 s train loss = 0.000001 train acc = 1.0000, test acc = 0.5793\n",
            "[2023-12-10 23:15:34] Evaluate_07: epoch = 0300 train time = 18 s train loss = 0.000019 train acc = 1.0000, test acc = 0.5210\n",
            "[2023-12-10 23:15:52] Evaluate_08: epoch = 0300 train time = 18 s train loss = 0.000000 train acc = 1.0000, test acc = 0.5394\n",
            "[2023-12-10 23:16:11] Evaluate_09: epoch = 0300 train time = 18 s train loss = 0.000000 train acc = 1.0000, test acc = 0.4893\n",
            "[2023-12-10 23:16:29] Evaluate_10: epoch = 0300 train time = 18 s train loss = 0.000005 train acc = 1.0000, test acc = 0.4616\n",
            "[2023-12-10 23:16:48] Evaluate_11: epoch = 0300 train time = 18 s train loss = 0.000001 train acc = 1.0000, test acc = 0.5916\n",
            "[2023-12-10 23:17:07] Evaluate_12: epoch = 0300 train time = 18 s train loss = 0.000002 train acc = 1.0000, test acc = 0.5906\n",
            "[2023-12-10 23:17:25] Evaluate_13: epoch = 0300 train time = 18 s train loss = 0.000000 train acc = 1.0000, test acc = 0.5742\n",
            "[2023-12-10 23:17:44] Evaluate_14: epoch = 0300 train time = 18 s train loss = 0.000000 train acc = 1.0000, test acc = 0.4913\n",
            "[2023-12-10 23:18:02] Evaluate_15: epoch = 0300 train time = 18 s train loss = 0.000009 train acc = 1.0000, test acc = 0.5384\n",
            "[2023-12-10 23:18:21] Evaluate_16: epoch = 0300 train time = 18 s train loss = 0.000005 train acc = 1.0000, test acc = 0.5834\n",
            "[2023-12-10 23:18:40] Evaluate_17: epoch = 0300 train time = 18 s train loss = 0.000000 train acc = 1.0000, test acc = 0.5844\n",
            "[2023-12-10 23:18:58] Evaluate_18: epoch = 0300 train time = 18 s train loss = 0.000000 train acc = 1.0000, test acc = 0.4616\n",
            "[2023-12-10 23:19:17] Evaluate_19: epoch = 0300 train time = 18 s train loss = 0.000009 train acc = 1.0000, test acc = 0.5578\n",
            "Evaluate 20 random ConvNet, mean = 0.5342 std = 0.0534\n",
            "-------------------------\n",
            "tensor(0.)\n",
            "tensor(1.)\n",
            "[2023-12-10 23:20:09] iter = 0010, loss = 342.5468\n",
            "\n",
            "==================== Final Results ====================\n",
            "\n",
            "Run 1 experiments, train on ConvNet, evaluate 20 random ConvNet, mean  = 53.42%  std = 5.34%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2D MNIST Extreme\n",
        "\n",
        "main(dataset=\"MNIST\", model=\"ConvNet\", iteration = 100, batch_real=128, batch_train=128, num_exp=1,\n",
        "     ipc=10, init=\"noise\", save_path=\"/content/drive/MyDrive/ECE1512/Project_B/Runs/MNIST Noise Extreme/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LaAUkwbLtoY",
        "outputId": "07e42c3d-d99f-4ba4-ab7c-ce7a5d9cc122"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RUNNING ON GPU\n",
            "eval_it_pool:  [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
            "\n",
            "================== Exp 0 ==================\n",
            " \n",
            "Hyper-parameters: \n",
            " {'method': 'DC', 'dataset': 'MNIST', 'model': 'ConvNet', 'ipc': 10, 'eval_mode': 'S', 'num_exp': 1, 'num_eval': 20, 'epoch_eval_train': 300, 'Iteration': 100, 'lr_img': 0.1, 'lr_net': 0.01, 'batch_real': 128, 'batch_train': 128, 'init': 'noise', 'dsa_strategy': 'None', 'data_path': '/content/drive/MyDrive/ECE1512/Project_B/', 'save_path': '/content/drive/MyDrive/ECE1512/Project_B/Runs/MNIST Noise Extreme/', 'dis_metric': 'ours', 'training_baseline': False, 'outer_loop': 10, 'inner_loop': 50, 'device': 'cuda', 'dsa_param': <__main__.ParamDiffAug object at 0x7d6e44b70a90>, 'dsa': False}\n",
            "Evaluation model pool:  ['ConvNet']\n",
            "class c = 0: 5923 real images\n",
            "class c = 1: 6742 real images\n",
            "class c = 2: 5958 real images\n",
            "class c = 3: 6131 real images\n",
            "class c = 4: 5842 real images\n",
            "class c = 5: 5421 real images\n",
            "class c = 6: 5918 real images\n",
            "class c = 7: 6265 real images\n",
            "class c = 8: 5851 real images\n",
            "class c = 9: 5949 real images\n",
            "real images channel 0, mean = -0.0001, std = 1.0000\n",
            "initialize synthetic data from random noise\n",
            "[2023-12-11 01:24:48] training begins\n",
            "-------------------------\n",
            "Evaluation\n",
            "model_train = ConvNet, model_eval = ConvNet, iteration = 0\n",
            "DC augmentation parameters: \n",
            " {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'crop_scale_rotate'}\n",
            "[2023-12-11 01:24:55] Evaluate_00: epoch = 1000 train time = 4 s train loss = 0.001647 train acc = 1.0000, test acc = 0.0519\n",
            "[2023-12-11 01:25:01] Evaluate_01: epoch = 1000 train time = 3 s train loss = 0.001630 train acc = 1.0000, test acc = 0.0974\n",
            "[2023-12-11 01:25:07] Evaluate_02: epoch = 1000 train time = 4 s train loss = 0.001597 train acc = 1.0000, test acc = 0.0907\n",
            "[2023-12-11 01:25:13] Evaluate_03: epoch = 1000 train time = 3 s train loss = 0.001594 train acc = 1.0000, test acc = 0.0976\n",
            "[2023-12-11 01:25:19] Evaluate_04: epoch = 1000 train time = 4 s train loss = 0.001633 train acc = 1.0000, test acc = 0.1289\n",
            "[2023-12-11 01:25:25] Evaluate_05: epoch = 1000 train time = 4 s train loss = 0.001606 train acc = 1.0000, test acc = 0.1021\n",
            "[2023-12-11 01:25:31] Evaluate_06: epoch = 1000 train time = 3 s train loss = 0.001629 train acc = 1.0000, test acc = 0.1031\n",
            "[2023-12-11 01:25:37] Evaluate_07: epoch = 1000 train time = 3 s train loss = 0.001649 train acc = 1.0000, test acc = 0.1171\n",
            "[2023-12-11 01:25:43] Evaluate_08: epoch = 1000 train time = 3 s train loss = 0.001628 train acc = 1.0000, test acc = 0.0781\n",
            "[2023-12-11 01:25:49] Evaluate_09: epoch = 1000 train time = 3 s train loss = 0.001637 train acc = 1.0000, test acc = 0.0920\n",
            "[2023-12-11 01:25:55] Evaluate_10: epoch = 1000 train time = 3 s train loss = 0.001592 train acc = 1.0000, test acc = 0.0893\n",
            "[2023-12-11 01:26:01] Evaluate_11: epoch = 1000 train time = 4 s train loss = 0.001616 train acc = 1.0000, test acc = 0.0795\n",
            "[2023-12-11 01:26:07] Evaluate_12: epoch = 1000 train time = 4 s train loss = 0.001591 train acc = 1.0000, test acc = 0.1249\n",
            "[2023-12-11 01:26:13] Evaluate_13: epoch = 1000 train time = 3 s train loss = 0.001609 train acc = 1.0000, test acc = 0.0509\n",
            "[2023-12-11 01:26:19] Evaluate_14: epoch = 1000 train time = 3 s train loss = 0.001634 train acc = 1.0000, test acc = 0.1275\n",
            "[2023-12-11 01:26:26] Evaluate_15: epoch = 1000 train time = 4 s train loss = 0.001632 train acc = 1.0000, test acc = 0.1307\n",
            "[2023-12-11 01:26:32] Evaluate_16: epoch = 1000 train time = 4 s train loss = 0.001615 train acc = 1.0000, test acc = 0.0502\n",
            "[2023-12-11 01:26:38] Evaluate_17: epoch = 1000 train time = 4 s train loss = 0.001617 train acc = 1.0000, test acc = 0.1000\n",
            "[2023-12-11 01:26:44] Evaluate_18: epoch = 1000 train time = 3 s train loss = 0.001597 train acc = 1.0000, test acc = 0.0869\n",
            "[2023-12-11 01:26:50] Evaluate_19: epoch = 1000 train time = 3 s train loss = 0.001610 train acc = 1.0000, test acc = 0.0696\n",
            "Evaluate 20 random ConvNet, mean = 0.0934 std = 0.0246\n",
            "-------------------------\n",
            "[2023-12-11 01:26:54] iter = 0000, loss = 215.9529\n",
            "-------------------------\n",
            "Evaluation\n",
            "model_train = ConvNet, model_eval = ConvNet, iteration = 10\n",
            "DC augmentation parameters: \n",
            " {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'crop_scale_rotate'}\n",
            "[2023-12-11 01:27:29] Evaluate_00: epoch = 1000 train time = 4 s train loss = 0.001562 train acc = 1.0000, test acc = 0.8577\n",
            "[2023-12-11 01:27:35] Evaluate_01: epoch = 1000 train time = 4 s train loss = 0.001590 train acc = 1.0000, test acc = 0.8320\n",
            "[2023-12-11 01:27:41] Evaluate_02: epoch = 1000 train time = 4 s train loss = 0.001581 train acc = 1.0000, test acc = 0.8450\n",
            "[2023-12-11 01:27:47] Evaluate_03: epoch = 1000 train time = 4 s train loss = 0.001586 train acc = 1.0000, test acc = 0.8575\n",
            "[2023-12-11 01:27:53] Evaluate_04: epoch = 1000 train time = 4 s train loss = 0.001568 train acc = 1.0000, test acc = 0.8388\n",
            "[2023-12-11 01:27:59] Evaluate_05: epoch = 1000 train time = 4 s train loss = 0.001565 train acc = 1.0000, test acc = 0.8514\n",
            "[2023-12-11 01:28:05] Evaluate_06: epoch = 1000 train time = 4 s train loss = 0.001571 train acc = 1.0000, test acc = 0.8375\n",
            "[2023-12-11 01:28:12] Evaluate_07: epoch = 1000 train time = 4 s train loss = 0.001595 train acc = 1.0000, test acc = 0.8461\n",
            "[2023-12-11 01:28:18] Evaluate_08: epoch = 1000 train time = 4 s train loss = 0.001565 train acc = 1.0000, test acc = 0.8545\n",
            "[2023-12-11 01:28:24] Evaluate_09: epoch = 1000 train time = 4 s train loss = 0.001557 train acc = 1.0000, test acc = 0.8418\n",
            "[2023-12-11 01:28:30] Evaluate_10: epoch = 1000 train time = 4 s train loss = 0.001584 train acc = 1.0000, test acc = 0.8366\n",
            "[2023-12-11 01:28:36] Evaluate_11: epoch = 1000 train time = 4 s train loss = 0.001596 train acc = 1.0000, test acc = 0.8396\n",
            "[2023-12-11 01:28:42] Evaluate_12: epoch = 1000 train time = 4 s train loss = 0.001586 train acc = 1.0000, test acc = 0.8414\n",
            "[2023-12-11 01:28:48] Evaluate_13: epoch = 1000 train time = 4 s train loss = 0.001553 train acc = 1.0000, test acc = 0.8512\n",
            "[2023-12-11 01:28:55] Evaluate_14: epoch = 1000 train time = 4 s train loss = 0.001591 train acc = 1.0000, test acc = 0.8592\n",
            "[2023-12-11 01:29:01] Evaluate_15: epoch = 1000 train time = 4 s train loss = 0.001583 train acc = 1.0000, test acc = 0.8604\n",
            "[2023-12-11 01:29:07] Evaluate_16: epoch = 1000 train time = 4 s train loss = 0.001601 train acc = 1.0000, test acc = 0.8330\n",
            "[2023-12-11 01:29:13] Evaluate_17: epoch = 1000 train time = 3 s train loss = 0.001551 train acc = 1.0000, test acc = 0.8500\n",
            "[2023-12-11 01:29:19] Evaluate_18: epoch = 1000 train time = 3 s train loss = 0.001542 train acc = 1.0000, test acc = 0.8339\n",
            "[2023-12-11 01:29:25] Evaluate_19: epoch = 1000 train time = 4 s train loss = 0.001562 train acc = 1.0000, test acc = 0.8263\n",
            "Evaluate 20 random ConvNet, mean = 0.8447 std = 0.0099\n",
            "-------------------------\n",
            "[2023-12-11 01:29:29] iter = 0010, loss = 98.1671\n",
            "-------------------------\n",
            "Evaluation\n",
            "model_train = ConvNet, model_eval = ConvNet, iteration = 20\n",
            "DC augmentation parameters: \n",
            " {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'crop_scale_rotate'}\n",
            "[2023-12-11 01:30:04] Evaluate_00: epoch = 1000 train time = 4 s train loss = 0.001568 train acc = 1.0000, test acc = 0.9001\n",
            "[2023-12-11 01:30:10] Evaluate_01: epoch = 1000 train time = 4 s train loss = 0.001593 train acc = 1.0000, test acc = 0.8987\n",
            "[2023-12-11 01:30:16] Evaluate_02: epoch = 1000 train time = 4 s train loss = 0.001571 train acc = 1.0000, test acc = 0.8951\n",
            "[2023-12-11 01:30:22] Evaluate_03: epoch = 1000 train time = 4 s train loss = 0.001563 train acc = 1.0000, test acc = 0.8951\n",
            "[2023-12-11 01:30:28] Evaluate_04: epoch = 1000 train time = 4 s train loss = 0.001538 train acc = 1.0000, test acc = 0.9050\n",
            "[2023-12-11 01:30:34] Evaluate_05: epoch = 1000 train time = 4 s train loss = 0.001586 train acc = 1.0000, test acc = 0.8871\n",
            "[2023-12-11 01:30:41] Evaluate_06: epoch = 1000 train time = 4 s train loss = 0.001563 train acc = 1.0000, test acc = 0.8922\n",
            "[2023-12-11 01:30:47] Evaluate_07: epoch = 1000 train time = 4 s train loss = 0.001591 train acc = 1.0000, test acc = 0.8919\n",
            "[2023-12-11 01:30:53] Evaluate_08: epoch = 1000 train time = 4 s train loss = 0.001562 train acc = 1.0000, test acc = 0.9002\n",
            "[2023-12-11 01:30:59] Evaluate_09: epoch = 1000 train time = 4 s train loss = 0.001574 train acc = 1.0000, test acc = 0.8928\n",
            "[2023-12-11 01:31:05] Evaluate_10: epoch = 1000 train time = 4 s train loss = 0.001596 train acc = 1.0000, test acc = 0.8946\n",
            "[2023-12-11 01:31:11] Evaluate_11: epoch = 1000 train time = 4 s train loss = 0.001590 train acc = 1.0000, test acc = 0.8932\n",
            "[2023-12-11 01:31:17] Evaluate_12: epoch = 1000 train time = 4 s train loss = 0.001564 train acc = 1.0000, test acc = 0.8896\n",
            "[2023-12-11 01:31:24] Evaluate_13: epoch = 1000 train time = 4 s train loss = 0.001519 train acc = 1.0000, test acc = 0.9018\n",
            "[2023-12-11 01:31:30] Evaluate_14: epoch = 1000 train time = 4 s train loss = 0.001526 train acc = 1.0000, test acc = 0.8938\n",
            "[2023-12-11 01:31:36] Evaluate_15: epoch = 1000 train time = 4 s train loss = 0.001570 train acc = 1.0000, test acc = 0.8860\n",
            "[2023-12-11 01:31:42] Evaluate_16: epoch = 1000 train time = 4 s train loss = 0.001553 train acc = 1.0000, test acc = 0.9015\n",
            "[2023-12-11 01:31:48] Evaluate_17: epoch = 1000 train time = 4 s train loss = 0.001611 train acc = 1.0000, test acc = 0.8914\n",
            "[2023-12-11 01:31:54] Evaluate_18: epoch = 1000 train time = 4 s train loss = 0.001591 train acc = 1.0000, test acc = 0.8989\n",
            "[2023-12-11 01:32:00] Evaluate_19: epoch = 1000 train time = 4 s train loss = 0.001623 train acc = 1.0000, test acc = 0.8986\n",
            "Evaluate 20 random ConvNet, mean = 0.8954 std = 0.0050\n",
            "-------------------------\n",
            "[2023-12-11 01:32:04] iter = 0020, loss = 75.4063\n",
            "-------------------------\n",
            "Evaluation\n",
            "model_train = ConvNet, model_eval = ConvNet, iteration = 30\n",
            "DC augmentation parameters: \n",
            " {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'crop_scale_rotate'}\n",
            "[2023-12-11 01:32:39] Evaluate_00: epoch = 1000 train time = 4 s train loss = 0.001568 train acc = 1.0000, test acc = 0.9139\n",
            "[2023-12-11 01:32:45] Evaluate_01: epoch = 1000 train time = 4 s train loss = 0.001633 train acc = 1.0000, test acc = 0.9155\n",
            "[2023-12-11 01:32:51] Evaluate_02: epoch = 1000 train time = 4 s train loss = 0.001586 train acc = 1.0000, test acc = 0.9066\n",
            "[2023-12-11 01:32:58] Evaluate_03: epoch = 1000 train time = 4 s train loss = 0.001608 train acc = 1.0000, test acc = 0.9147\n",
            "[2023-12-11 01:33:04] Evaluate_04: epoch = 1000 train time = 4 s train loss = 0.001561 train acc = 1.0000, test acc = 0.9118\n",
            "[2023-12-11 01:33:10] Evaluate_05: epoch = 1000 train time = 4 s train loss = 0.001577 train acc = 1.0000, test acc = 0.9160\n",
            "[2023-12-11 01:33:16] Evaluate_06: epoch = 1000 train time = 4 s train loss = 0.001587 train acc = 1.0000, test acc = 0.9069\n",
            "[2023-12-11 01:33:22] Evaluate_07: epoch = 1000 train time = 3 s train loss = 0.001592 train acc = 1.0000, test acc = 0.9128\n",
            "[2023-12-11 01:33:28] Evaluate_08: epoch = 1000 train time = 4 s train loss = 0.001618 train acc = 1.0000, test acc = 0.9158\n",
            "[2023-12-11 01:33:34] Evaluate_09: epoch = 1000 train time = 4 s train loss = 0.001635 train acc = 1.0000, test acc = 0.9164\n",
            "[2023-12-11 01:33:40] Evaluate_10: epoch = 1000 train time = 4 s train loss = 0.001588 train acc = 1.0000, test acc = 0.9150\n",
            "[2023-12-11 01:33:47] Evaluate_11: epoch = 1000 train time = 3 s train loss = 0.001557 train acc = 1.0000, test acc = 0.9127\n",
            "[2023-12-11 01:33:53] Evaluate_12: epoch = 1000 train time = 4 s train loss = 0.001584 train acc = 1.0000, test acc = 0.9102\n",
            "[2023-12-11 01:33:59] Evaluate_13: epoch = 1000 train time = 4 s train loss = 0.001629 train acc = 1.0000, test acc = 0.9103\n",
            "[2023-12-11 01:34:05] Evaluate_14: epoch = 1000 train time = 4 s train loss = 0.001606 train acc = 1.0000, test acc = 0.9191\n",
            "[2023-12-11 01:34:11] Evaluate_15: epoch = 1000 train time = 4 s train loss = 0.001630 train acc = 1.0000, test acc = 0.9132\n",
            "[2023-12-11 01:34:17] Evaluate_16: epoch = 1000 train time = 4 s train loss = 0.001575 train acc = 1.0000, test acc = 0.9182\n",
            "[2023-12-11 01:34:23] Evaluate_17: epoch = 1000 train time = 4 s train loss = 0.001556 train acc = 1.0000, test acc = 0.9096\n",
            "[2023-12-11 01:34:29] Evaluate_18: epoch = 1000 train time = 4 s train loss = 0.001593 train acc = 1.0000, test acc = 0.9142\n",
            "[2023-12-11 01:34:36] Evaluate_19: epoch = 1000 train time = 4 s train loss = 0.001528 train acc = 1.0000, test acc = 0.9093\n",
            "Evaluate 20 random ConvNet, mean = 0.9131 std = 0.0034\n",
            "-------------------------\n",
            "[2023-12-11 01:34:39] iter = 0030, loss = 65.0477\n",
            "-------------------------\n",
            "Evaluation\n",
            "model_train = ConvNet, model_eval = ConvNet, iteration = 40\n",
            "DC augmentation parameters: \n",
            " {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'crop_scale_rotate'}\n",
            "[2023-12-11 01:35:15] Evaluate_00: epoch = 1000 train time = 4 s train loss = 0.001608 train acc = 1.0000, test acc = 0.9342\n",
            "[2023-12-11 01:35:21] Evaluate_01: epoch = 1000 train time = 4 s train loss = 0.001622 train acc = 1.0000, test acc = 0.9304\n",
            "[2023-12-11 01:35:27] Evaluate_02: epoch = 1000 train time = 4 s train loss = 0.001670 train acc = 1.0000, test acc = 0.9299\n",
            "[2023-12-11 01:35:33] Evaluate_03: epoch = 1000 train time = 4 s train loss = 0.001733 train acc = 1.0000, test acc = 0.9330\n",
            "[2023-12-11 01:35:39] Evaluate_04: epoch = 1000 train time = 4 s train loss = 0.001671 train acc = 1.0000, test acc = 0.9362\n",
            "[2023-12-11 01:35:45] Evaluate_05: epoch = 1000 train time = 4 s train loss = 0.001652 train acc = 1.0000, test acc = 0.9294\n",
            "[2023-12-11 01:35:51] Evaluate_06: epoch = 1000 train time = 4 s train loss = 0.001636 train acc = 1.0000, test acc = 0.9284\n",
            "[2023-12-11 01:35:58] Evaluate_07: epoch = 1000 train time = 4 s train loss = 0.001547 train acc = 1.0000, test acc = 0.9303\n",
            "[2023-12-11 01:36:04] Evaluate_08: epoch = 1000 train time = 4 s train loss = 0.001685 train acc = 1.0000, test acc = 0.9279\n",
            "[2023-12-11 01:36:10] Evaluate_09: epoch = 1000 train time = 4 s train loss = 0.001652 train acc = 1.0000, test acc = 0.9318\n",
            "[2023-12-11 01:36:16] Evaluate_10: epoch = 1000 train time = 4 s train loss = 0.001614 train acc = 1.0000, test acc = 0.9313\n",
            "[2023-12-11 01:36:22] Evaluate_11: epoch = 1000 train time = 4 s train loss = 0.001662 train acc = 1.0000, test acc = 0.9317\n",
            "[2023-12-11 01:36:28] Evaluate_12: epoch = 1000 train time = 4 s train loss = 0.001671 train acc = 1.0000, test acc = 0.9323\n",
            "[2023-12-11 01:36:34] Evaluate_13: epoch = 1000 train time = 4 s train loss = 0.001659 train acc = 1.0000, test acc = 0.9326\n",
            "[2023-12-11 01:36:41] Evaluate_14: epoch = 1000 train time = 4 s train loss = 0.001652 train acc = 1.0000, test acc = 0.9314\n",
            "[2023-12-11 01:36:47] Evaluate_15: epoch = 1000 train time = 4 s train loss = 0.001627 train acc = 1.0000, test acc = 0.9332\n",
            "[2023-12-11 01:36:53] Evaluate_16: epoch = 1000 train time = 4 s train loss = 0.001615 train acc = 1.0000, test acc = 0.9355\n",
            "[2023-12-11 01:36:59] Evaluate_17: epoch = 1000 train time = 4 s train loss = 0.001619 train acc = 1.0000, test acc = 0.9281\n",
            "[2023-12-11 01:37:05] Evaluate_18: epoch = 1000 train time = 4 s train loss = 0.001645 train acc = 1.0000, test acc = 0.9280\n",
            "[2023-12-11 01:37:11] Evaluate_19: epoch = 1000 train time = 4 s train loss = 0.001690 train acc = 1.0000, test acc = 0.9326\n",
            "Evaluate 20 random ConvNet, mean = 0.9314 std = 0.0023\n",
            "-------------------------\n",
            "[2023-12-11 01:37:15] iter = 0040, loss = 58.3660\n",
            "-------------------------\n",
            "Evaluation\n",
            "model_train = ConvNet, model_eval = ConvNet, iteration = 50\n",
            "DC augmentation parameters: \n",
            " {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'crop_scale_rotate'}\n",
            "[2023-12-11 01:37:50] Evaluate_00: epoch = 1000 train time = 4 s train loss = 0.001660 train acc = 1.0000, test acc = 0.9340\n",
            "[2023-12-11 01:37:56] Evaluate_01: epoch = 1000 train time = 4 s train loss = 0.001661 train acc = 1.0000, test acc = 0.9359\n",
            "[2023-12-11 01:38:02] Evaluate_02: epoch = 1000 train time = 4 s train loss = 0.001673 train acc = 1.0000, test acc = 0.9348\n",
            "[2023-12-11 01:38:09] Evaluate_03: epoch = 1000 train time = 4 s train loss = 0.001679 train acc = 1.0000, test acc = 0.9329\n",
            "[2023-12-11 01:38:15] Evaluate_04: epoch = 1000 train time = 4 s train loss = 0.001704 train acc = 1.0000, test acc = 0.9357\n",
            "[2023-12-11 01:38:21] Evaluate_05: epoch = 1000 train time = 4 s train loss = 0.001664 train acc = 1.0000, test acc = 0.9365\n",
            "[2023-12-11 01:38:27] Evaluate_06: epoch = 1000 train time = 4 s train loss = 0.001697 train acc = 1.0000, test acc = 0.9384\n",
            "[2023-12-11 01:38:33] Evaluate_07: epoch = 1000 train time = 4 s train loss = 0.001725 train acc = 1.0000, test acc = 0.9339\n",
            "[2023-12-11 01:38:39] Evaluate_08: epoch = 1000 train time = 4 s train loss = 0.001716 train acc = 1.0000, test acc = 0.9351\n",
            "[2023-12-11 01:38:46] Evaluate_09: epoch = 1000 train time = 4 s train loss = 0.001653 train acc = 1.0000, test acc = 0.9356\n",
            "[2023-12-11 01:38:52] Evaluate_10: epoch = 1000 train time = 4 s train loss = 0.001610 train acc = 1.0000, test acc = 0.9397\n",
            "[2023-12-11 01:38:58] Evaluate_11: epoch = 1000 train time = 4 s train loss = 0.001776 train acc = 1.0000, test acc = 0.9340\n",
            "[2023-12-11 01:39:04] Evaluate_12: epoch = 1000 train time = 4 s train loss = 0.001704 train acc = 1.0000, test acc = 0.9373\n",
            "[2023-12-11 01:39:10] Evaluate_13: epoch = 1000 train time = 4 s train loss = 0.001748 train acc = 1.0000, test acc = 0.9346\n",
            "[2023-12-11 01:39:16] Evaluate_14: epoch = 1000 train time = 4 s train loss = 0.001716 train acc = 1.0000, test acc = 0.9352\n",
            "[2023-12-11 01:39:22] Evaluate_15: epoch = 1000 train time = 4 s train loss = 0.001706 train acc = 1.0000, test acc = 0.9340\n",
            "[2023-12-11 01:39:29] Evaluate_16: epoch = 1000 train time = 4 s train loss = 0.001641 train acc = 1.0000, test acc = 0.9364\n",
            "[2023-12-11 01:39:35] Evaluate_17: epoch = 1000 train time = 4 s train loss = 0.001721 train acc = 1.0000, test acc = 0.9364\n",
            "[2023-12-11 01:39:41] Evaluate_18: epoch = 1000 train time = 3 s train loss = 0.001628 train acc = 1.0000, test acc = 0.9373\n",
            "[2023-12-11 01:39:47] Evaluate_19: epoch = 1000 train time = 4 s train loss = 0.001657 train acc = 1.0000, test acc = 0.9329\n",
            "Evaluate 20 random ConvNet, mean = 0.9355 std = 0.0017\n",
            "-------------------------\n",
            "[2023-12-11 01:39:51] iter = 0050, loss = 54.9421\n",
            "-------------------------\n",
            "Evaluation\n",
            "model_train = ConvNet, model_eval = ConvNet, iteration = 60\n",
            "DC augmentation parameters: \n",
            " {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'crop_scale_rotate'}\n",
            "[2023-12-11 01:40:26] Evaluate_00: epoch = 1000 train time = 4 s train loss = 0.001684 train acc = 1.0000, test acc = 0.9410\n",
            "[2023-12-11 01:40:32] Evaluate_01: epoch = 1000 train time = 4 s train loss = 0.001637 train acc = 1.0000, test acc = 0.9411\n",
            "[2023-12-11 01:40:38] Evaluate_02: epoch = 1000 train time = 4 s train loss = 0.001685 train acc = 1.0000, test acc = 0.9405\n",
            "[2023-12-11 01:40:44] Evaluate_03: epoch = 1000 train time = 4 s train loss = 0.001703 train acc = 1.0000, test acc = 0.9409\n",
            "[2023-12-11 01:40:50] Evaluate_04: epoch = 1000 train time = 4 s train loss = 0.001744 train acc = 1.0000, test acc = 0.9383\n",
            "[2023-12-11 01:40:56] Evaluate_05: epoch = 1000 train time = 4 s train loss = 0.001687 train acc = 1.0000, test acc = 0.9405\n",
            "[2023-12-11 01:41:02] Evaluate_06: epoch = 1000 train time = 4 s train loss = 0.001731 train acc = 1.0000, test acc = 0.9399\n",
            "[2023-12-11 01:41:09] Evaluate_07: epoch = 1000 train time = 4 s train loss = 0.001695 train acc = 1.0000, test acc = 0.9393\n",
            "[2023-12-11 01:41:15] Evaluate_08: epoch = 1000 train time = 4 s train loss = 0.001685 train acc = 1.0000, test acc = 0.9357\n",
            "[2023-12-11 01:41:21] Evaluate_09: epoch = 1000 train time = 4 s train loss = 0.001679 train acc = 1.0000, test acc = 0.9386\n",
            "[2023-12-11 01:41:27] Evaluate_10: epoch = 1000 train time = 4 s train loss = 0.001692 train acc = 1.0000, test acc = 0.9395\n",
            "[2023-12-11 01:41:33] Evaluate_11: epoch = 1000 train time = 4 s train loss = 0.001623 train acc = 1.0000, test acc = 0.9403\n",
            "[2023-12-11 01:41:39] Evaluate_12: epoch = 1000 train time = 4 s train loss = 0.001687 train acc = 1.0000, test acc = 0.9403\n",
            "[2023-12-11 01:41:46] Evaluate_13: epoch = 1000 train time = 4 s train loss = 0.001660 train acc = 1.0000, test acc = 0.9391\n",
            "[2023-12-11 01:41:52] Evaluate_14: epoch = 1000 train time = 4 s train loss = 0.001709 train acc = 1.0000, test acc = 0.9402\n",
            "[2023-12-11 01:41:58] Evaluate_15: epoch = 1000 train time = 4 s train loss = 0.001645 train acc = 1.0000, test acc = 0.9375\n",
            "[2023-12-11 01:42:04] Evaluate_16: epoch = 1000 train time = 4 s train loss = 0.001666 train acc = 1.0000, test acc = 0.9365\n",
            "[2023-12-11 01:42:10] Evaluate_17: epoch = 1000 train time = 4 s train loss = 0.001740 train acc = 1.0000, test acc = 0.9382\n",
            "[2023-12-11 01:42:16] Evaluate_18: epoch = 1000 train time = 4 s train loss = 0.001714 train acc = 1.0000, test acc = 0.9392\n",
            "[2023-12-11 01:42:22] Evaluate_19: epoch = 1000 train time = 4 s train loss = 0.001681 train acc = 1.0000, test acc = 0.9413\n",
            "Evaluate 20 random ConvNet, mean = 0.9394 std = 0.0015\n",
            "-------------------------\n",
            "[2023-12-11 01:42:26] iter = 0060, loss = 49.9408\n",
            "-------------------------\n",
            "Evaluation\n",
            "model_train = ConvNet, model_eval = ConvNet, iteration = 70\n",
            "DC augmentation parameters: \n",
            " {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'crop_scale_rotate'}\n",
            "[2023-12-11 01:43:01] Evaluate_00: epoch = 1000 train time = 4 s train loss = 0.001740 train acc = 1.0000, test acc = 0.9429\n",
            "[2023-12-11 01:43:07] Evaluate_01: epoch = 1000 train time = 4 s train loss = 0.001735 train acc = 1.0000, test acc = 0.9432\n",
            "[2023-12-11 01:43:13] Evaluate_02: epoch = 1000 train time = 4 s train loss = 0.001736 train acc = 1.0000, test acc = 0.9412\n",
            "[2023-12-11 01:43:20] Evaluate_03: epoch = 1000 train time = 4 s train loss = 0.001714 train acc = 1.0000, test acc = 0.9384\n",
            "[2023-12-11 01:43:26] Evaluate_04: epoch = 1000 train time = 4 s train loss = 0.001731 train acc = 1.0000, test acc = 0.9403\n",
            "[2023-12-11 01:43:32] Evaluate_05: epoch = 1000 train time = 4 s train loss = 0.001731 train acc = 1.0000, test acc = 0.9429\n",
            "[2023-12-11 01:43:38] Evaluate_06: epoch = 1000 train time = 4 s train loss = 0.001736 train acc = 1.0000, test acc = 0.9429\n",
            "[2023-12-11 01:43:44] Evaluate_07: epoch = 1000 train time = 4 s train loss = 0.001682 train acc = 1.0000, test acc = 0.9431\n",
            "[2023-12-11 01:43:50] Evaluate_08: epoch = 1000 train time = 4 s train loss = 0.001771 train acc = 1.0000, test acc = 0.9415\n",
            "[2023-12-11 01:43:57] Evaluate_09: epoch = 1000 train time = 4 s train loss = 0.001735 train acc = 1.0000, test acc = 0.9421\n",
            "[2023-12-11 01:44:03] Evaluate_10: epoch = 1000 train time = 4 s train loss = 0.001677 train acc = 1.0000, test acc = 0.9423\n",
            "[2023-12-11 01:44:09] Evaluate_11: epoch = 1000 train time = 4 s train loss = 0.001758 train acc = 1.0000, test acc = 0.9395\n",
            "[2023-12-11 01:44:15] Evaluate_12: epoch = 1000 train time = 4 s train loss = 0.001690 train acc = 1.0000, test acc = 0.9445\n",
            "[2023-12-11 01:44:21] Evaluate_13: epoch = 1000 train time = 4 s train loss = 0.001704 train acc = 1.0000, test acc = 0.9414\n",
            "[2023-12-11 01:44:27] Evaluate_14: epoch = 1000 train time = 4 s train loss = 0.001718 train acc = 1.0000, test acc = 0.9422\n",
            "[2023-12-11 01:44:33] Evaluate_15: epoch = 1000 train time = 4 s train loss = 0.001709 train acc = 1.0000, test acc = 0.9410\n",
            "[2023-12-11 01:44:40] Evaluate_16: epoch = 1000 train time = 4 s train loss = 0.001770 train acc = 1.0000, test acc = 0.9416\n",
            "[2023-12-11 01:44:46] Evaluate_17: epoch = 1000 train time = 4 s train loss = 0.001725 train acc = 1.0000, test acc = 0.9420\n",
            "[2023-12-11 01:44:52] Evaluate_18: epoch = 1000 train time = 4 s train loss = 0.001700 train acc = 1.0000, test acc = 0.9436\n",
            "[2023-12-11 01:44:58] Evaluate_19: epoch = 1000 train time = 4 s train loss = 0.001663 train acc = 1.0000, test acc = 0.9424\n",
            "Evaluate 20 random ConvNet, mean = 0.9419 std = 0.0014\n",
            "-------------------------\n",
            "[2023-12-11 01:45:02] iter = 0070, loss = 49.9727\n",
            "-------------------------\n",
            "Evaluation\n",
            "model_train = ConvNet, model_eval = ConvNet, iteration = 80\n",
            "DC augmentation parameters: \n",
            " {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'crop_scale_rotate'}\n",
            "[2023-12-11 01:45:37] Evaluate_00: epoch = 1000 train time = 4 s train loss = 0.001765 train acc = 1.0000, test acc = 0.9445\n",
            "[2023-12-11 01:45:43] Evaluate_01: epoch = 1000 train time = 4 s train loss = 0.001759 train acc = 1.0000, test acc = 0.9439\n",
            "[2023-12-11 01:45:49] Evaluate_02: epoch = 1000 train time = 4 s train loss = 0.001801 train acc = 1.0000, test acc = 0.9436\n",
            "[2023-12-11 01:45:55] Evaluate_03: epoch = 1000 train time = 4 s train loss = 0.001651 train acc = 1.0000, test acc = 0.9441\n",
            "[2023-12-11 01:46:01] Evaluate_04: epoch = 1000 train time = 4 s train loss = 0.001692 train acc = 1.0000, test acc = 0.9411\n",
            "[2023-12-11 01:46:07] Evaluate_05: epoch = 1000 train time = 4 s train loss = 0.001806 train acc = 1.0000, test acc = 0.9429\n",
            "[2023-12-11 01:46:14] Evaluate_06: epoch = 1000 train time = 4 s train loss = 0.001652 train acc = 1.0000, test acc = 0.9425\n",
            "[2023-12-11 01:46:20] Evaluate_07: epoch = 1000 train time = 4 s train loss = 0.001717 train acc = 1.0000, test acc = 0.9420\n",
            "[2023-12-11 01:46:26] Evaluate_08: epoch = 1000 train time = 4 s train loss = 0.001746 train acc = 1.0000, test acc = 0.9445\n",
            "[2023-12-11 01:46:32] Evaluate_09: epoch = 1000 train time = 4 s train loss = 0.001715 train acc = 1.0000, test acc = 0.9446\n",
            "[2023-12-11 01:46:38] Evaluate_10: epoch = 1000 train time = 4 s train loss = 0.001738 train acc = 1.0000, test acc = 0.9429\n",
            "[2023-12-11 01:46:44] Evaluate_11: epoch = 1000 train time = 4 s train loss = 0.001745 train acc = 1.0000, test acc = 0.9436\n",
            "[2023-12-11 01:46:50] Evaluate_12: epoch = 1000 train time = 4 s train loss = 0.001749 train acc = 1.0000, test acc = 0.9459\n",
            "[2023-12-11 01:46:57] Evaluate_13: epoch = 1000 train time = 4 s train loss = 0.001735 train acc = 1.0000, test acc = 0.9452\n",
            "[2023-12-11 01:47:03] Evaluate_14: epoch = 1000 train time = 4 s train loss = 0.001736 train acc = 1.0000, test acc = 0.9412\n",
            "[2023-12-11 01:47:09] Evaluate_15: epoch = 1000 train time = 4 s train loss = 0.001753 train acc = 1.0000, test acc = 0.9450\n",
            "[2023-12-11 01:47:15] Evaluate_16: epoch = 1000 train time = 4 s train loss = 0.001713 train acc = 1.0000, test acc = 0.9392\n",
            "[2023-12-11 01:47:21] Evaluate_17: epoch = 1000 train time = 4 s train loss = 0.001759 train acc = 1.0000, test acc = 0.9439\n",
            "[2023-12-11 01:47:27] Evaluate_18: epoch = 1000 train time = 4 s train loss = 0.001714 train acc = 1.0000, test acc = 0.9426\n",
            "[2023-12-11 01:47:33] Evaluate_19: epoch = 1000 train time = 4 s train loss = 0.001736 train acc = 1.0000, test acc = 0.9431\n",
            "Evaluate 20 random ConvNet, mean = 0.9433 std = 0.0016\n",
            "-------------------------\n",
            "[2023-12-11 01:47:37] iter = 0080, loss = 48.5885\n",
            "-------------------------\n",
            "Evaluation\n",
            "model_train = ConvNet, model_eval = ConvNet, iteration = 90\n",
            "DC augmentation parameters: \n",
            " {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'crop_scale_rotate'}\n",
            "[2023-12-11 01:48:12] Evaluate_00: epoch = 1000 train time = 4 s train loss = 0.001778 train acc = 1.0000, test acc = 0.9471\n",
            "[2023-12-11 01:48:18] Evaluate_01: epoch = 1000 train time = 4 s train loss = 0.001801 train acc = 1.0000, test acc = 0.9470\n",
            "[2023-12-11 01:48:24] Evaluate_02: epoch = 1000 train time = 4 s train loss = 0.001784 train acc = 1.0000, test acc = 0.9462\n",
            "[2023-12-11 01:48:30] Evaluate_03: epoch = 1000 train time = 4 s train loss = 0.001739 train acc = 1.0000, test acc = 0.9471\n",
            "[2023-12-11 01:48:37] Evaluate_04: epoch = 1000 train time = 4 s train loss = 0.001805 train acc = 1.0000, test acc = 0.9471\n",
            "[2023-12-11 01:48:43] Evaluate_05: epoch = 1000 train time = 4 s train loss = 0.001777 train acc = 1.0000, test acc = 0.9460\n",
            "[2023-12-11 01:48:49] Evaluate_06: epoch = 1000 train time = 4 s train loss = 0.001758 train acc = 1.0000, test acc = 0.9452\n",
            "[2023-12-11 01:48:55] Evaluate_07: epoch = 1000 train time = 4 s train loss = 0.001772 train acc = 1.0000, test acc = 0.9439\n",
            "[2023-12-11 01:49:01] Evaluate_08: epoch = 1000 train time = 4 s train loss = 0.001779 train acc = 1.0000, test acc = 0.9441\n",
            "[2023-12-11 01:49:07] Evaluate_09: epoch = 1000 train time = 4 s train loss = 0.001778 train acc = 1.0000, test acc = 0.9479\n",
            "[2023-12-11 01:49:13] Evaluate_10: epoch = 1000 train time = 4 s train loss = 0.001827 train acc = 1.0000, test acc = 0.9463\n",
            "[2023-12-11 01:49:20] Evaluate_11: epoch = 1000 train time = 4 s train loss = 0.001739 train acc = 1.0000, test acc = 0.9457\n",
            "[2023-12-11 01:49:26] Evaluate_12: epoch = 1000 train time = 4 s train loss = 0.001696 train acc = 1.0000, test acc = 0.9431\n",
            "[2023-12-11 01:49:32] Evaluate_13: epoch = 1000 train time = 4 s train loss = 0.001789 train acc = 1.0000, test acc = 0.9479\n",
            "[2023-12-11 01:49:38] Evaluate_14: epoch = 1000 train time = 4 s train loss = 0.001757 train acc = 1.0000, test acc = 0.9458\n",
            "[2023-12-11 01:49:44] Evaluate_15: epoch = 1000 train time = 4 s train loss = 0.001745 train acc = 1.0000, test acc = 0.9470\n",
            "[2023-12-11 01:49:50] Evaluate_16: epoch = 1000 train time = 4 s train loss = 0.001852 train acc = 1.0000, test acc = 0.9462\n",
            "[2023-12-11 01:49:56] Evaluate_17: epoch = 1000 train time = 3 s train loss = 0.001813 train acc = 1.0000, test acc = 0.9446\n",
            "[2023-12-11 01:50:02] Evaluate_18: epoch = 1000 train time = 4 s train loss = 0.001779 train acc = 1.0000, test acc = 0.9444\n",
            "[2023-12-11 01:50:09] Evaluate_19: epoch = 1000 train time = 4 s train loss = 0.001787 train acc = 1.0000, test acc = 0.9465\n",
            "Evaluate 20 random ConvNet, mean = 0.9460 std = 0.0013\n",
            "-------------------------\n",
            "[2023-12-11 01:50:12] iter = 0090, loss = 46.2178\n",
            "-------------------------\n",
            "Evaluation\n",
            "model_train = ConvNet, model_eval = ConvNet, iteration = 100\n",
            "DC augmentation parameters: \n",
            " {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'crop_scale_rotate'}\n",
            "[2023-12-11 01:50:47] Evaluate_00: epoch = 1000 train time = 4 s train loss = 0.001866 train acc = 1.0000, test acc = 0.9477\n",
            "[2023-12-11 01:50:54] Evaluate_01: epoch = 1000 train time = 4 s train loss = 0.001864 train acc = 1.0000, test acc = 0.9497\n",
            "[2023-12-11 01:51:00] Evaluate_02: epoch = 1000 train time = 4 s train loss = 0.001743 train acc = 1.0000, test acc = 0.9470\n",
            "[2023-12-11 01:51:06] Evaluate_03: epoch = 1000 train time = 4 s train loss = 0.001769 train acc = 1.0000, test acc = 0.9488\n",
            "[2023-12-11 01:51:12] Evaluate_04: epoch = 1000 train time = 4 s train loss = 0.001758 train acc = 1.0000, test acc = 0.9493\n",
            "[2023-12-11 01:51:18] Evaluate_05: epoch = 1000 train time = 4 s train loss = 0.001793 train acc = 1.0000, test acc = 0.9488\n",
            "[2023-12-11 01:51:24] Evaluate_06: epoch = 1000 train time = 4 s train loss = 0.001783 train acc = 1.0000, test acc = 0.9471\n",
            "[2023-12-11 01:51:31] Evaluate_07: epoch = 1000 train time = 4 s train loss = 0.001773 train acc = 1.0000, test acc = 0.9517\n",
            "[2023-12-11 01:51:37] Evaluate_08: epoch = 1000 train time = 4 s train loss = 0.001851 train acc = 1.0000, test acc = 0.9441\n",
            "[2023-12-11 01:51:43] Evaluate_09: epoch = 1000 train time = 4 s train loss = 0.001797 train acc = 1.0000, test acc = 0.9488\n",
            "[2023-12-11 01:51:49] Evaluate_10: epoch = 1000 train time = 4 s train loss = 0.001796 train acc = 1.0000, test acc = 0.9501\n",
            "[2023-12-11 01:51:55] Evaluate_11: epoch = 1000 train time = 3 s train loss = 0.001753 train acc = 1.0000, test acc = 0.9498\n",
            "[2023-12-11 01:52:01] Evaluate_12: epoch = 1000 train time = 4 s train loss = 0.001848 train acc = 1.0000, test acc = 0.9478\n",
            "[2023-12-11 01:52:07] Evaluate_13: epoch = 1000 train time = 4 s train loss = 0.001780 train acc = 1.0000, test acc = 0.9495\n",
            "[2023-12-11 01:52:14] Evaluate_14: epoch = 1000 train time = 4 s train loss = 0.001829 train acc = 1.0000, test acc = 0.9494\n",
            "[2023-12-11 01:52:20] Evaluate_15: epoch = 1000 train time = 4 s train loss = 0.001789 train acc = 1.0000, test acc = 0.9469\n",
            "[2023-12-11 01:52:26] Evaluate_16: epoch = 1000 train time = 4 s train loss = 0.001781 train acc = 1.0000, test acc = 0.9516\n",
            "[2023-12-11 01:52:32] Evaluate_17: epoch = 1000 train time = 4 s train loss = 0.001742 train acc = 1.0000, test acc = 0.9495\n",
            "[2023-12-11 01:52:38] Evaluate_18: epoch = 1000 train time = 4 s train loss = 0.001815 train acc = 1.0000, test acc = 0.9467\n",
            "[2023-12-11 01:52:44] Evaluate_19: epoch = 1000 train time = 4 s train loss = 0.001789 train acc = 1.0000, test acc = 0.9484\n",
            "Evaluate 20 random ConvNet, mean = 0.9486 std = 0.0017\n",
            "-------------------------\n",
            "[2023-12-11 01:52:48] iter = 0100, loss = 48.4405\n",
            "\n",
            "==================== Final Results ====================\n",
            "\n",
            "Run 1 experiments, train on ConvNet, evaluate 20 random ConvNet, mean  = 94.86%  std = 0.17%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2D MHIST Extreme\n",
        "\n",
        "main(dataset=\"MHIST\", model=\"ConvNet\", iteration = 100, batch_real=128, batch_train=128, num_exp=1,\n",
        "     ipc=50, init=\"noise\", save_path=\"/content/drive/MyDrive/ECE1512/Project_B/Runs/MHIST Noise Extreme/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "chlbWP_NL4On",
        "outputId": "164189f8-9118-4553-f2d7-d17594bbafcc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RUNNING ON GPU\n",
            "eval_it_pool:  [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
            "done loading images\n",
            "\n",
            "================== Exp 0 ==================\n",
            " \n",
            "Hyper-parameters: \n",
            " {'method': 'DC', 'dataset': 'MHIST', 'model': 'ConvNet', 'ipc': 50, 'eval_mode': 'S', 'num_exp': 1, 'num_eval': 20, 'epoch_eval_train': 300, 'Iteration': 100, 'lr_img': 0.1, 'lr_net': 0.01, 'batch_real': 128, 'batch_train': 128, 'init': 'noise', 'dsa_strategy': 'None', 'data_path': '/content/drive/MyDrive/ECE1512/Project_B/', 'save_path': '/content/drive/MyDrive/ECE1512/Project_B/Runs/MHIST Noise Extreme/', 'dis_metric': 'ours', 'training_baseline': False, 'outer_loop': 50, 'inner_loop': 10, 'device': 'cuda', 'dsa_param': <__main__.ParamDiffAug object at 0x7d6e6e638340>, 'dsa': False}\n",
            "Evaluation model pool:  ['ConvNet']\n",
            "class c = 0: 1545 real images\n",
            "class c = 1: 630 real images\n",
            "real images channel 0, mean = 0.7195, std = 0.1973\n",
            "real images channel 1, mean = 0.6218, std = 0.2513\n",
            "real images channel 2, mean = 0.7605, std = 0.1662\n",
            "initialize synthetic data from random noise\n",
            "[2023-12-11 00:06:22] training begins\n",
            "-------------------------\n",
            "Evaluation\n",
            "model_train = ConvNet, model_eval = ConvNet, iteration = 0\n",
            "DC augmentation parameters: \n",
            " {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-106c806b81ac>:88: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
            "  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-12-11 00:06:40] Evaluate_00: epoch = 0300 train time = 17 s train loss = 0.000005 train acc = 1.0000, test acc = 0.5189\n",
            "[2023-12-11 00:06:50] Evaluate_01: epoch = 0300 train time = 10 s train loss = 0.000000 train acc = 1.0000, test acc = 0.3470\n",
            "[2023-12-11 00:07:00] Evaluate_02: epoch = 0300 train time = 10 s train loss = 0.000000 train acc = 1.0000, test acc = 0.3644\n",
            "[2023-12-11 00:07:11] Evaluate_03: epoch = 0300 train time = 10 s train loss = 0.000008 train acc = 1.0000, test acc = 0.4811\n",
            "[2023-12-11 00:07:21] Evaluate_04: epoch = 0300 train time = 10 s train loss = 0.000000 train acc = 1.0000, test acc = 0.5241\n",
            "[2023-12-11 00:07:31] Evaluate_05: epoch = 0300 train time = 10 s train loss = 0.000000 train acc = 1.0000, test acc = 0.5670\n",
            "[2023-12-11 00:07:42] Evaluate_06: epoch = 0300 train time = 10 s train loss = 0.000000 train acc = 1.0000, test acc = 0.4596\n",
            "[2023-12-11 00:07:52] Evaluate_07: epoch = 0300 train time = 10 s train loss = 0.000000 train acc = 1.0000, test acc = 0.3613\n",
            "[2023-12-11 00:08:03] Evaluate_08: epoch = 0300 train time = 10 s train loss = 0.000000 train acc = 1.0000, test acc = 0.6305\n",
            "[2023-12-11 00:08:13] Evaluate_09: epoch = 0300 train time = 10 s train loss = 0.000006 train acc = 1.0000, test acc = 0.4207\n",
            "[2023-12-11 00:08:23] Evaluate_10: epoch = 0300 train time = 10 s train loss = 0.000000 train acc = 1.0000, test acc = 0.4688\n",
            "[2023-12-11 00:08:33] Evaluate_11: epoch = 0300 train time = 10 s train loss = 0.000000 train acc = 1.0000, test acc = 0.5353\n",
            "[2023-12-11 00:08:44] Evaluate_12: epoch = 0300 train time = 10 s train loss = 0.000000 train acc = 1.0000, test acc = 0.3357\n",
            "[2023-12-11 00:08:54] Evaluate_13: epoch = 0300 train time = 10 s train loss = 0.000015 train acc = 1.0000, test acc = 0.5128\n",
            "[2023-12-11 00:09:04] Evaluate_14: epoch = 0300 train time = 10 s train loss = 0.000000 train acc = 1.0000, test acc = 0.3408\n",
            "[2023-12-11 00:09:15] Evaluate_15: epoch = 0300 train time = 10 s train loss = 0.000000 train acc = 1.0000, test acc = 0.4084\n",
            "[2023-12-11 00:09:25] Evaluate_16: epoch = 0300 train time = 10 s train loss = 0.000001 train acc = 1.0000, test acc = 0.6244\n",
            "[2023-12-11 00:09:36] Evaluate_17: epoch = 0300 train time = 10 s train loss = 0.000000 train acc = 1.0000, test acc = 0.5496\n",
            "[2023-12-11 00:09:46] Evaluate_18: epoch = 0300 train time = 10 s train loss = 0.000000 train acc = 1.0000, test acc = 0.3654\n",
            "[2023-12-11 00:09:56] Evaluate_19: epoch = 0300 train time = 10 s train loss = 0.000015 train acc = 1.0000, test acc = 0.3685\n",
            "Evaluate 20 random ConvNet, mean = 0.4592 std = 0.0935\n",
            "-------------------------\n",
            "tensor(0.)\n",
            "tensor(1.)\n",
            "[2023-12-11 00:10:26] iter = 0000, loss = 331.4677\n",
            "-------------------------\n",
            "Evaluation\n",
            "model_train = ConvNet, model_eval = ConvNet, iteration = 10\n",
            "DC augmentation parameters: \n",
            " {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}\n",
            "[2023-12-11 00:14:56] Evaluate_00: epoch = 0300 train time = 10 s train loss = 0.000006 train acc = 1.0000, test acc = 0.5322\n",
            "[2023-12-11 00:15:07] Evaluate_01: epoch = 0300 train time = 10 s train loss = 0.000012 train acc = 1.0000, test acc = 0.6141\n",
            "[2023-12-11 00:15:17] Evaluate_02: epoch = 0300 train time = 10 s train loss = 0.000006 train acc = 1.0000, test acc = 0.4667\n",
            "[2023-12-11 00:15:27] Evaluate_03: epoch = 0300 train time = 10 s train loss = 0.000001 train acc = 1.0000, test acc = 0.6469\n",
            "[2023-12-11 00:15:38] Evaluate_04: epoch = 0300 train time = 10 s train loss = 0.000004 train acc = 1.0000, test acc = 0.4882\n",
            "[2023-12-11 00:15:48] Evaluate_05: epoch = 0300 train time = 10 s train loss = 0.000015 train acc = 1.0000, test acc = 0.4381\n",
            "[2023-12-11 00:15:58] Evaluate_06: epoch = 0300 train time = 10 s train loss = 0.000011 train acc = 1.0000, test acc = 0.3941\n",
            "[2023-12-11 00:16:08] Evaluate_07: epoch = 0300 train time = 10 s train loss = 0.000165 train acc = 1.0000, test acc = 0.3941\n",
            "[2023-12-11 00:16:19] Evaluate_08: epoch = 0300 train time = 10 s train loss = 0.000000 train acc = 1.0000, test acc = 0.4555\n",
            "[2023-12-11 00:16:29] Evaluate_09: epoch = 0300 train time = 10 s train loss = 0.000005 train acc = 1.0000, test acc = 0.4350\n",
            "[2023-12-11 00:16:39] Evaluate_10: epoch = 0300 train time = 10 s train loss = 0.000075 train acc = 1.0000, test acc = 0.5455\n",
            "[2023-12-11 00:16:50] Evaluate_11: epoch = 0300 train time = 10 s train loss = 0.000009 train acc = 1.0000, test acc = 0.5619\n",
            "[2023-12-11 00:17:00] Evaluate_12: epoch = 0300 train time = 10 s train loss = 0.000004 train acc = 1.0000, test acc = 0.5374\n",
            "[2023-12-11 00:17:10] Evaluate_13: epoch = 0300 train time = 10 s train loss = 0.000005 train acc = 1.0000, test acc = 0.3920\n",
            "[2023-12-11 00:17:21] Evaluate_14: epoch = 0300 train time = 10 s train loss = 0.000034 train acc = 1.0000, test acc = 0.4289\n",
            "[2023-12-11 00:17:31] Evaluate_15: epoch = 0300 train time = 10 s train loss = 0.000002 train acc = 1.0000, test acc = 0.5200\n",
            "[2023-12-11 00:17:41] Evaluate_16: epoch = 0300 train time = 10 s train loss = 0.000006 train acc = 1.0000, test acc = 0.4371\n",
            "[2023-12-11 00:17:52] Evaluate_17: epoch = 0300 train time = 10 s train loss = 0.000004 train acc = 1.0000, test acc = 0.5998\n",
            "[2023-12-11 00:18:02] Evaluate_18: epoch = 0300 train time = 10 s train loss = 0.000007 train acc = 1.0000, test acc = 0.4278\n",
            "[2023-12-11 00:18:12] Evaluate_19: epoch = 0300 train time = 10 s train loss = 0.000002 train acc = 1.0000, test acc = 0.5763\n",
            "Evaluate 20 random ConvNet, mean = 0.4946 std = 0.0769\n",
            "-------------------------\n",
            "tensor(0.)\n",
            "tensor(1.)\n",
            "[2023-12-11 00:18:42] iter = 0010, loss = 302.6858\n",
            "-------------------------\n",
            "Evaluation\n",
            "model_train = ConvNet, model_eval = ConvNet, iteration = 20\n",
            "DC augmentation parameters: \n",
            " {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}\n",
            "[2023-12-11 00:23:12] Evaluate_00: epoch = 0300 train time = 10 s train loss = 0.000028 train acc = 1.0000, test acc = 0.4432\n",
            "[2023-12-11 00:23:22] Evaluate_01: epoch = 0300 train time = 10 s train loss = 0.000091 train acc = 1.0000, test acc = 0.4104\n",
            "[2023-12-11 00:23:33] Evaluate_02: epoch = 0300 train time = 10 s train loss = 0.000027 train acc = 1.0000, test acc = 0.5015\n",
            "[2023-12-11 00:23:43] Evaluate_03: epoch = 0300 train time = 10 s train loss = 0.000036 train acc = 1.0000, test acc = 0.4708\n",
            "[2023-12-11 00:23:53] Evaluate_04: epoch = 0300 train time = 10 s train loss = 0.000027 train acc = 1.0000, test acc = 0.5548\n",
            "[2023-12-11 00:24:04] Evaluate_05: epoch = 0300 train time = 10 s train loss = 0.000019 train acc = 1.0000, test acc = 0.4821\n",
            "[2023-12-11 00:24:14] Evaluate_06: epoch = 0300 train time = 10 s train loss = 0.000024 train acc = 1.0000, test acc = 0.5251\n",
            "[2023-12-11 00:24:24] Evaluate_07: epoch = 0300 train time = 10 s train loss = 0.000024 train acc = 1.0000, test acc = 0.4626\n",
            "[2023-12-11 00:24:34] Evaluate_08: epoch = 0300 train time = 10 s train loss = 0.000015 train acc = 1.0000, test acc = 0.5589\n",
            "[2023-12-11 00:24:45] Evaluate_09: epoch = 0300 train time = 10 s train loss = 0.000016 train acc = 1.0000, test acc = 0.6029\n",
            "[2023-12-11 00:24:55] Evaluate_10: epoch = 0300 train time = 10 s train loss = 0.000013 train acc = 1.0000, test acc = 0.5701\n",
            "[2023-12-11 00:25:05] Evaluate_11: epoch = 0300 train time = 10 s train loss = 0.000007 train acc = 1.0000, test acc = 0.4893\n",
            "[2023-12-11 00:25:16] Evaluate_12: epoch = 0300 train time = 10 s train loss = 0.000052 train acc = 1.0000, test acc = 0.5824\n",
            "[2023-12-11 00:25:26] Evaluate_13: epoch = 0300 train time = 10 s train loss = 0.000027 train acc = 1.0000, test acc = 0.5865\n",
            "[2023-12-11 00:25:36] Evaluate_14: epoch = 0300 train time = 10 s train loss = 0.000008 train acc = 1.0000, test acc = 0.5230\n",
            "[2023-12-11 00:25:47] Evaluate_15: epoch = 0300 train time = 10 s train loss = 0.000016 train acc = 1.0000, test acc = 0.5322\n",
            "[2023-12-11 00:25:57] Evaluate_16: epoch = 0300 train time = 10 s train loss = 0.000025 train acc = 1.0000, test acc = 0.5138\n",
            "[2023-12-11 00:26:07] Evaluate_17: epoch = 0300 train time = 10 s train loss = 0.000005 train acc = 1.0000, test acc = 0.4637\n",
            "[2023-12-11 00:26:18] Evaluate_18: epoch = 0300 train time = 10 s train loss = 0.000021 train acc = 1.0000, test acc = 0.5404\n",
            "[2023-12-11 00:26:28] Evaluate_19: epoch = 0300 train time = 10 s train loss = 0.000025 train acc = 1.0000, test acc = 0.6356\n",
            "Evaluate 20 random ConvNet, mean = 0.5225 std = 0.0565\n",
            "-------------------------\n",
            "tensor(0.)\n",
            "tensor(1.)\n",
            "[2023-12-11 00:26:58] iter = 0020, loss = 277.6305\n",
            "-------------------------\n",
            "Evaluation\n",
            "model_train = ConvNet, model_eval = ConvNet, iteration = 30\n",
            "DC augmentation parameters: \n",
            " {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}\n",
            "[2023-12-11 00:31:28] Evaluate_00: epoch = 0300 train time = 10 s train loss = 0.000006 train acc = 1.0000, test acc = 0.6182\n",
            "[2023-12-11 00:31:38] Evaluate_01: epoch = 0300 train time = 10 s train loss = 0.000024 train acc = 1.0000, test acc = 0.5896\n",
            "[2023-12-11 00:31:48] Evaluate_02: epoch = 0300 train time = 10 s train loss = 0.000023 train acc = 1.0000, test acc = 0.6305\n",
            "[2023-12-11 00:31:59] Evaluate_03: epoch = 0300 train time = 10 s train loss = 0.000004 train acc = 1.0000, test acc = 0.4115\n",
            "[2023-12-11 00:32:09] Evaluate_04: epoch = 0300 train time = 10 s train loss = 0.000004 train acc = 1.0000, test acc = 0.6090\n",
            "[2023-12-11 00:32:19] Evaluate_05: epoch = 0300 train time = 10 s train loss = 0.000007 train acc = 1.0000, test acc = 0.4985\n",
            "[2023-12-11 00:32:29] Evaluate_06: epoch = 0300 train time = 10 s train loss = 0.000008 train acc = 1.0000, test acc = 0.6551\n",
            "[2023-12-11 00:32:40] Evaluate_07: epoch = 0300 train time = 10 s train loss = 0.000001 train acc = 1.0000, test acc = 0.6315\n",
            "[2023-12-11 00:32:50] Evaluate_08: epoch = 0300 train time = 10 s train loss = 0.000007 train acc = 1.0000, test acc = 0.5486\n",
            "[2023-12-11 00:33:00] Evaluate_09: epoch = 0300 train time = 10 s train loss = 0.000038 train acc = 1.0000, test acc = 0.5363\n",
            "[2023-12-11 00:33:11] Evaluate_10: epoch = 0300 train time = 10 s train loss = 0.000004 train acc = 1.0000, test acc = 0.6305\n",
            "[2023-12-11 00:33:21] Evaluate_11: epoch = 0300 train time = 10 s train loss = 0.000004 train acc = 1.0000, test acc = 0.6520\n",
            "[2023-12-11 00:33:31] Evaluate_12: epoch = 0300 train time = 10 s train loss = 0.000005 train acc = 1.0000, test acc = 0.6407\n",
            "[2023-12-11 00:33:42] Evaluate_13: epoch = 0300 train time = 10 s train loss = 0.000006 train acc = 1.0000, test acc = 0.6244\n",
            "[2023-12-11 00:33:52] Evaluate_14: epoch = 0300 train time = 10 s train loss = 0.000025 train acc = 1.0000, test acc = 0.5537\n",
            "[2023-12-11 00:34:02] Evaluate_15: epoch = 0300 train time = 10 s train loss = 0.000018 train acc = 1.0000, test acc = 0.5589\n",
            "[2023-12-11 00:34:13] Evaluate_16: epoch = 0300 train time = 10 s train loss = 0.000024 train acc = 1.0000, test acc = 0.5650\n",
            "[2023-12-11 00:34:23] Evaluate_17: epoch = 0300 train time = 10 s train loss = 0.000003 train acc = 1.0000, test acc = 0.5036\n",
            "[2023-12-11 00:34:33] Evaluate_18: epoch = 0300 train time = 10 s train loss = 0.000070 train acc = 1.0000, test acc = 0.4831\n",
            "[2023-12-11 00:34:43] Evaluate_19: epoch = 0300 train time = 10 s train loss = 0.000005 train acc = 1.0000, test acc = 0.6356\n",
            "Evaluate 20 random ConvNet, mean = 0.5788 std = 0.0649\n",
            "-------------------------\n",
            "tensor(0.)\n",
            "tensor(1.)\n",
            "[2023-12-11 00:35:14] iter = 0030, loss = 265.3425\n",
            "-------------------------\n",
            "Evaluation\n",
            "model_train = ConvNet, model_eval = ConvNet, iteration = 40\n",
            "DC augmentation parameters: \n",
            " {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}\n",
            "[2023-12-11 00:39:43] Evaluate_00: epoch = 0300 train time = 10 s train loss = 0.000019 train acc = 1.0000, test acc = 0.6029\n",
            "[2023-12-11 00:39:54] Evaluate_01: epoch = 0300 train time = 10 s train loss = 0.000021 train acc = 1.0000, test acc = 0.5599\n",
            "[2023-12-11 00:40:04] Evaluate_02: epoch = 0300 train time = 10 s train loss = 0.000006 train acc = 1.0000, test acc = 0.4933\n",
            "[2023-12-11 00:40:14] Evaluate_03: epoch = 0300 train time = 10 s train loss = 0.000037 train acc = 1.0000, test acc = 0.3808\n",
            "[2023-12-11 00:40:25] Evaluate_04: epoch = 0300 train time = 10 s train loss = 0.000003 train acc = 1.0000, test acc = 0.4340\n",
            "[2023-12-11 00:40:35] Evaluate_05: epoch = 0300 train time = 10 s train loss = 0.000011 train acc = 1.0000, test acc = 0.4063\n",
            "[2023-12-11 00:40:45] Evaluate_06: epoch = 0300 train time = 10 s train loss = 0.000005 train acc = 1.0000, test acc = 0.6684\n",
            "[2023-12-11 00:40:55] Evaluate_07: epoch = 0300 train time = 10 s train loss = 0.000005 train acc = 1.0000, test acc = 0.5322\n",
            "[2023-12-11 00:41:06] Evaluate_08: epoch = 0300 train time = 10 s train loss = 0.000008 train acc = 1.0000, test acc = 0.5200\n",
            "[2023-12-11 00:41:16] Evaluate_09: epoch = 0300 train time = 10 s train loss = 0.000005 train acc = 1.0000, test acc = 0.6377\n",
            "[2023-12-11 00:41:26] Evaluate_10: epoch = 0300 train time = 10 s train loss = 0.000003 train acc = 1.0000, test acc = 0.6162\n",
            "[2023-12-11 00:41:37] Evaluate_11: epoch = 0300 train time = 10 s train loss = 0.000004 train acc = 1.0000, test acc = 0.6264\n",
            "[2023-12-11 00:41:47] Evaluate_12: epoch = 0300 train time = 10 s train loss = 0.000004 train acc = 1.0000, test acc = 0.5629\n",
            "[2023-12-11 00:41:57] Evaluate_13: epoch = 0300 train time = 10 s train loss = 0.000002 train acc = 1.0000, test acc = 0.4667\n",
            "[2023-12-11 00:42:08] Evaluate_14: epoch = 0300 train time = 10 s train loss = 0.000038 train acc = 1.0000, test acc = 0.5343\n",
            "[2023-12-11 00:42:18] Evaluate_15: epoch = 0300 train time = 10 s train loss = 0.000001 train acc = 1.0000, test acc = 0.4811\n",
            "[2023-12-11 00:42:28] Evaluate_16: epoch = 0300 train time = 10 s train loss = 0.000012 train acc = 1.0000, test acc = 0.4770\n",
            "[2023-12-11 00:42:39] Evaluate_17: epoch = 0300 train time = 10 s train loss = 0.000002 train acc = 1.0000, test acc = 0.6192\n",
            "[2023-12-11 00:42:49] Evaluate_18: epoch = 0300 train time = 10 s train loss = 0.000049 train acc = 1.0000, test acc = 0.4514\n",
            "[2023-12-11 00:42:59] Evaluate_19: epoch = 0300 train time = 10 s train loss = 0.000018 train acc = 1.0000, test acc = 0.4115\n",
            "Evaluate 20 random ConvNet, mean = 0.5241 std = 0.0836\n",
            "-------------------------\n",
            "tensor(0.)\n",
            "tensor(1.)\n",
            "[2023-12-11 00:43:29] iter = 0040, loss = 275.9434\n",
            "-------------------------\n",
            "Evaluation\n",
            "model_train = ConvNet, model_eval = ConvNet, iteration = 50\n",
            "DC augmentation parameters: \n",
            " {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}\n",
            "[2023-12-11 00:47:59] Evaluate_00: epoch = 0300 train time = 10 s train loss = 0.000013 train acc = 1.0000, test acc = 0.5230\n",
            "[2023-12-11 00:48:09] Evaluate_01: epoch = 0300 train time = 10 s train loss = 0.000008 train acc = 1.0000, test acc = 0.5415\n",
            "[2023-12-11 00:48:20] Evaluate_02: epoch = 0300 train time = 10 s train loss = 0.000001 train acc = 1.0000, test acc = 0.6776\n",
            "[2023-12-11 00:48:30] Evaluate_03: epoch = 0300 train time = 10 s train loss = 0.000032 train acc = 1.0000, test acc = 0.6254\n",
            "[2023-12-11 00:48:40] Evaluate_04: epoch = 0300 train time = 10 s train loss = 0.000007 train acc = 1.0000, test acc = 0.6346\n",
            "[2023-12-11 00:48:50] Evaluate_05: epoch = 0300 train time = 10 s train loss = 0.000013 train acc = 1.0000, test acc = 0.5711\n",
            "[2023-12-11 00:49:01] Evaluate_06: epoch = 0300 train time = 10 s train loss = 0.000006 train acc = 1.0000, test acc = 0.5814\n",
            "[2023-12-11 00:49:11] Evaluate_07: epoch = 0300 train time = 10 s train loss = 0.000008 train acc = 1.0000, test acc = 0.5773\n",
            "[2023-12-11 00:49:21] Evaluate_08: epoch = 0300 train time = 10 s train loss = 0.000015 train acc = 1.0000, test acc = 0.6520\n",
            "[2023-12-11 00:49:32] Evaluate_09: epoch = 0300 train time = 10 s train loss = 0.000005 train acc = 1.0000, test acc = 0.5855\n",
            "[2023-12-11 00:49:42] Evaluate_10: epoch = 0300 train time = 10 s train loss = 0.000013 train acc = 1.0000, test acc = 0.6325\n",
            "[2023-12-11 00:49:52] Evaluate_11: epoch = 0300 train time = 10 s train loss = 0.000002 train acc = 1.0000, test acc = 0.6407\n",
            "[2023-12-11 00:50:03] Evaluate_12: epoch = 0300 train time = 10 s train loss = 0.000004 train acc = 1.0000, test acc = 0.5998\n",
            "[2023-12-11 00:50:13] Evaluate_13: epoch = 0300 train time = 10 s train loss = 0.000007 train acc = 1.0000, test acc = 0.6039\n",
            "[2023-12-11 00:50:23] Evaluate_14: epoch = 0300 train time = 10 s train loss = 0.000015 train acc = 1.0000, test acc = 0.6008\n",
            "[2023-12-11 00:50:33] Evaluate_15: epoch = 0300 train time = 10 s train loss = 0.000002 train acc = 1.0000, test acc = 0.6366\n",
            "[2023-12-11 00:50:44] Evaluate_16: epoch = 0300 train time = 10 s train loss = 0.000026 train acc = 1.0000, test acc = 0.6244\n",
            "[2023-12-11 00:50:54] Evaluate_17: epoch = 0300 train time = 10 s train loss = 0.000012 train acc = 1.0000, test acc = 0.6325\n",
            "[2023-12-11 00:51:04] Evaluate_18: epoch = 0300 train time = 10 s train loss = 0.000005 train acc = 1.0000, test acc = 0.3951\n",
            "[2023-12-11 00:51:15] Evaluate_19: epoch = 0300 train time = 10 s train loss = 0.000000 train acc = 1.0000, test acc = 0.6141\n",
            "Evaluate 20 random ConvNet, mean = 0.5975 std = 0.0590\n",
            "-------------------------\n",
            "tensor(0.)\n",
            "tensor(1.)\n",
            "[2023-12-11 00:51:45] iter = 0050, loss = 271.6075\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-a1984f38ae6c>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#2D MHIST Extreme\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m main(dataset=\"MHIST\", model=\"ConvNet\", iteration = 100, batch_real=128, batch_train=128, num_exp=1,\n\u001b[0m\u001b[1;32m      4\u001b[0m      ipc=50, init=\"noise\", save_path=\"/content/drive/MyDrive/ECE1512/Project_B/Runs/MHIST Noise Extreme/\")\n",
            "\u001b[0;32m<ipython-input-5-106c806b81ac>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(method, dataset, model, ipc, eval_mode, num_exp, num_eval, epoch_eval_train, iteration, lr_img, lr_net, batch_real, batch_train, init, dsa_strategy, data_path, save_path, dis_metric, training_baseline)\u001b[0m\n\u001b[1;32m    228\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                 \u001b[0moptimizer_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                 \u001b[0mloss_avg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mol\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mouter_loop\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2E - Train on synthetic set\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    \"dis_metric\": \"cos\",\n",
        "    \"lr_net\": 0.01,\n",
        "    \"epoch_eval_train\": 20,\n",
        "    \"batch_train\": 32,\n",
        "    \"training_baseline\": True,\n",
        "    \"dataset\": \"MNIST Custom\"\n",
        "}\n",
        "\n",
        "args.update(args_dict)\n",
        "\n",
        "# train MNIST\n",
        "\n",
        "mnist_path = \"/content/drive/MyDrive/ECE1512/Project_B/MNIST Dataset/\"\n",
        "\n",
        "MNIST_dataset = get_dataset(\"MNIST Custom\", mnist_path)\n",
        "channel_mnist = MNIST_dataset[0]\n",
        "im_size_mnist = MNIST_dataset[1]\n",
        "num_classes_mnist = MNIST_dataset[2]\n",
        "class_names_mnist = MNIST_dataset[3]\n",
        "mean_mnist = MNIST_dataset[4]\n",
        "std_mnist = MNIST_dataset[5]\n",
        "dst_train_mnist = MNIST_dataset[6]\n",
        "dst_test_mnist = MNIST_dataset[7]\n",
        "testloader_mnist = MNIST_dataset[8]\n",
        "\n",
        "mnist_network = get_network(\"ConvNet\", channel_mnist, num_classes_mnist, im_size_mnist)\n",
        "\n",
        "evaluate_synset(1, mnist_network, dst_train_mnist, dst_train_mnist.targets, testloader_mnist, args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4WEN1ekU9wV",
        "outputId": "8d80fe03-7d87-47ea-ac68-b6a4d2913340"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-12-11 01:12:17] Evaluate_01: epoch = 0020 train time = 9 s train loss = 0.025028 train acc = 1.0000, test acc = 0.9063\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(ConvNet(\n",
              "   (features): Sequential(\n",
              "     (0): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3))\n",
              "     (1): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
              "     (2): ReLU(inplace=True)\n",
              "     (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "     (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "     (5): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
              "     (6): ReLU(inplace=True)\n",
              "     (7): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "     (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "     (9): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
              "     (10): ReLU(inplace=True)\n",
              "     (11): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "   )\n",
              "   (classifier): Linear(in_features=2048, out_features=10, bias=True)\n",
              " ),\n",
              " 1.0,\n",
              " 0.9063)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2E - train on synthetic set (MHIST)\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    \"dis_metric\": \"cos\",\n",
        "    \"lr_net\": 0.01,\n",
        "    \"epoch_eval_train\": 20,\n",
        "    \"batch_train\": 32,\n",
        "    \"training_baseline\": True,\n",
        "    \"dataset\": \"MHIST Custom\"\n",
        "}\n",
        "\n",
        "args.update(args_dict)\n",
        "\n",
        "mhist_path = \"/content/drive/MyDrive/ECE1512/Project_B/MHIST Dataset/\"\n",
        "\n",
        "MHIST_dataset = get_dataset(\"MHIST Custom\", mhist_path)\n",
        "channel_mhist = MHIST_dataset[0]\n",
        "im_size_mhist = MHIST_dataset[1]\n",
        "num_classes_mhist = MHIST_dataset[2]\n",
        "class_names_mhist = MHIST_dataset[3]\n",
        "mean_mhist = MHIST_dataset[4]\n",
        "std_mhist = MHIST_dataset[5]\n",
        "dst_train_mhist = MHIST_dataset[6]\n",
        "dst_test_mhist = MHIST_dataset[7]\n",
        "testloader_mhist = MHIST_dataset[8]\n",
        "\n",
        "mhist_network = get_network(\"ConvNet\", channel_mhist, num_classes_mhist, im_size_mhist)\n",
        "\n",
        "evaluate_synset(1, mhist_network, dst_train_mhist, dst_train_mhist.targets, testloader_mhist, args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gihNEIteb0zL",
        "outputId": "e660fb1a-45c3-4a07-8191-73332d81eaad"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-12-11 01:21:43] Evaluate_01: epoch = 0020 train time = 4 s train loss = 0.693508 train acc = 0.5000, test acc = 0.3685\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(ConvNet(\n",
              "   (features): Sequential(\n",
              "     (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "     (1): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
              "     (2): ReLU(inplace=True)\n",
              "     (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "     (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "     (5): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
              "     (6): ReLU(inplace=True)\n",
              "     (7): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "     (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "     (9): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
              "     (10): ReLU(inplace=True)\n",
              "     (11): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "   )\n",
              "   (classifier): Linear(in_features=32768, out_features=2, bias=True)\n",
              " ),\n",
              " 0.5,\n",
              " 0.368474923234391)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3 Cross architecture performance\n",
        "\n",
        "# Train MNIST using ResNet for comparison\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    \"dis_metric\": \"cos\",\n",
        "    \"lr_net\": 0.01,\n",
        "    \"epoch_eval_train\": 20,\n",
        "    \"batch_train\": 32,\n",
        "    \"training_baseline\": True,\n",
        "    \"dataset\": \"MNIST Custom\"\n",
        "}\n",
        "\n",
        "args.update(args_dict)\n",
        "\n",
        "# train MNIST\n",
        "\n",
        "mnist_path = \"/content/drive/MyDrive/ECE1512/Project_B/MNIST Dataset/\"\n",
        "\n",
        "MNIST_dataset = get_dataset(\"MNIST Custom\", mnist_path)\n",
        "channel_mnist = MNIST_dataset[0]\n",
        "im_size_mnist = MNIST_dataset[1]\n",
        "num_classes_mnist = MNIST_dataset[2]\n",
        "class_names_mnist = MNIST_dataset[3]\n",
        "mean_mnist = MNIST_dataset[4]\n",
        "std_mnist = MNIST_dataset[5]\n",
        "dst_train_mnist = MNIST_dataset[6]\n",
        "dst_test_mnist = MNIST_dataset[7]\n",
        "testloader_mnist = MNIST_dataset[8]\n",
        "\n",
        "mnist_network = get_network(\"ResNet18\", channel_mnist, num_classes_mnist, im_size_mnist)\n",
        "\n",
        "evaluate_synset(1, mnist_network, dst_train_mnist, dst_train_mnist.targets, testloader_mnist, args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZEgfn1IquMS",
        "outputId": "2b08ecb5-7a03-4628-fd87-b12d8ddc876d"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-12-11 02:31:39] Evaluate_01: epoch = 0020 train time = 3 s train loss = 0.177401 train acc = 1.0000, test acc = 0.8144\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(ResNet(\n",
              "   (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "   (bn1): GroupNorm(64, 64, eps=1e-05, affine=True)\n",
              "   (layer1): Sequential(\n",
              "     (0): BasicBlock(\n",
              "       (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn1): GroupNorm(64, 64, eps=1e-05, affine=True)\n",
              "       (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): GroupNorm(64, 64, eps=1e-05, affine=True)\n",
              "       (shortcut): Sequential()\n",
              "     )\n",
              "     (1): BasicBlock(\n",
              "       (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn1): GroupNorm(64, 64, eps=1e-05, affine=True)\n",
              "       (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): GroupNorm(64, 64, eps=1e-05, affine=True)\n",
              "       (shortcut): Sequential()\n",
              "     )\n",
              "   )\n",
              "   (layer2): Sequential(\n",
              "     (0): BasicBlock(\n",
              "       (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "       (bn1): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
              "       (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
              "       (shortcut): Sequential(\n",
              "         (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "         (1): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
              "       )\n",
              "     )\n",
              "     (1): BasicBlock(\n",
              "       (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn1): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
              "       (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
              "       (shortcut): Sequential()\n",
              "     )\n",
              "   )\n",
              "   (layer3): Sequential(\n",
              "     (0): BasicBlock(\n",
              "       (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "       (bn1): GroupNorm(256, 256, eps=1e-05, affine=True)\n",
              "       (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): GroupNorm(256, 256, eps=1e-05, affine=True)\n",
              "       (shortcut): Sequential(\n",
              "         (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "         (1): GroupNorm(256, 256, eps=1e-05, affine=True)\n",
              "       )\n",
              "     )\n",
              "     (1): BasicBlock(\n",
              "       (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn1): GroupNorm(256, 256, eps=1e-05, affine=True)\n",
              "       (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): GroupNorm(256, 256, eps=1e-05, affine=True)\n",
              "       (shortcut): Sequential()\n",
              "     )\n",
              "   )\n",
              "   (layer4): Sequential(\n",
              "     (0): BasicBlock(\n",
              "       (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "       (bn1): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
              "       (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
              "       (shortcut): Sequential(\n",
              "         (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "         (1): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
              "       )\n",
              "     )\n",
              "     (1): BasicBlock(\n",
              "       (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn1): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
              "       (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
              "       (shortcut): Sequential()\n",
              "     )\n",
              "   )\n",
              "   (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
              " ),\n",
              " 1.0,\n",
              " 0.8144)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RESNET MNIST FLOPS\n",
        "\n",
        "# Train MNIST using ResNet for comparison\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    \"dis_metric\": \"cos\",\n",
        "    \"lr_net\": 0.01,\n",
        "    \"epoch_eval_train\": 20,\n",
        "    \"batch_train\": 32,\n",
        "    \"training_baseline\": True,\n",
        "    \"dataset\": \"MNIST Custom\"\n",
        "}\n",
        "\n",
        "args.update(args_dict)\n",
        "\n",
        "# train MNIST\n",
        "\n",
        "mnist_path = \"/content/drive/MyDrive/ECE1512/Project_B/MNIST Dataset/\"\n",
        "\n",
        "MNIST_dataset = get_dataset(\"MNIST Custom\", mnist_path)\n",
        "channel_mnist = MNIST_dataset[0]\n",
        "im_size_mnist = MNIST_dataset[1]\n",
        "num_classes_mnist = MNIST_dataset[2]\n",
        "class_names_mnist = MNIST_dataset[3]\n",
        "mean_mnist = MNIST_dataset[4]\n",
        "std_mnist = MNIST_dataset[5]\n",
        "dst_train_mnist = MNIST_dataset[6]\n",
        "dst_test_mnist = MNIST_dataset[7]\n",
        "testloader_mnist = MNIST_dataset[8]\n",
        "\n",
        "mnist_network = get_network(\"ResNet18\", channel_mnist, num_classes_mnist, im_size_mnist)\n",
        "\n",
        "# Create dummy input\n",
        "dummy_input = torch.randn(1, 1, 28, 28).to('cuda')\n",
        "\n",
        "# Use profiler to record FLOPS\n",
        "with profile(activities=[ProfilerActivity.CPU], record_shapes=True, with_flops = True) as prof:\n",
        "    with record_function(\"model_inference\"):\n",
        "        mnist_network(dummy_input)\n",
        "\n",
        "# Print the FLOPS\n",
        "print(prof.key_averages().table(sort_by=\"flops\", row_limit=-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stLLRw3t4KL8",
        "outputId": "6123b6c7-6001-4704-e528-fe9418ebcb7c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  Total KFLOPs  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                           aten::conv2d         6.23%       2.759ms        80.29%      35.543ms       1.777ms            20    911591.424  \n",
            "                                            aten::addmm         0.43%     192.000us         0.54%     240.000us     240.000us             1        10.240  \n",
            "                                        model_inference         3.93%       1.740ms       100.00%      44.267ms      44.267ms             1            --  \n",
            "                                      aten::convolution         0.63%     278.000us        80.10%      35.460ms       1.773ms            20            --  \n",
            "                                     aten::_convolution         0.52%     232.000us        79.48%      35.182ms       1.759ms            20            --  \n",
            "                                aten::cudnn_convolution        76.85%      34.021ms        78.95%      34.950ms       1.748ms            20            --  \n",
            "                                  cudaStreamIsCapturing         0.08%      35.000us         0.08%      35.000us       1.591us            22            --  \n",
            "                                  cudaStreamGetPriority         0.00%       2.000us         0.00%       2.000us       0.100us            20            --  \n",
            "                       cudaDeviceGetStreamPriorityRange         0.00%       2.000us         0.00%       2.000us       0.100us            20            --  \n",
            "                                       cudaLaunchKernel         2.46%       1.088ms         2.46%       1.088ms       8.635us           126            --  \n",
            "                                        cudaMemsetAsync         0.11%      47.000us         0.11%      47.000us       5.875us             8            --  \n",
            "                                       aten::group_norm         0.29%     130.000us         5.43%       2.404ms     120.200us            20            --  \n",
            "                                aten::native_group_norm         2.96%       1.312ms         5.14%       2.274ms     113.700us            20            --  \n",
            "                                            aten::empty         1.17%     517.000us         1.17%     517.000us       5.170us           100            --  \n",
            "                                             aten::view         0.09%      39.000us         0.09%      39.000us       0.481us            81            --  \n",
            "                                             aten::relu         0.80%     354.000us         2.66%       1.176ms      69.176us            17            --  \n",
            "                                        aten::clamp_min         0.86%     380.000us         1.86%     822.000us      48.353us            17            --  \n",
            "                                   cudaFuncSetAttribute         0.07%      32.000us         0.07%      32.000us       0.941us            34            --  \n",
            "                                    cudaLaunchKernelExC         0.26%     116.000us         0.26%     116.000us       6.824us            17            --  \n",
            "                                             aten::add_         0.74%     326.000us         0.90%     400.000us      50.000us             8            --  \n",
            "                                             cudaMalloc         1.27%     563.000us         1.27%     563.000us     281.500us             2            --  \n",
            "                                       aten::avg_pool2d         0.10%      43.000us         0.12%      51.000us      51.000us             1            --  \n",
            "                                           aten::linear         0.03%      13.000us         0.61%     269.000us     269.000us             1            --  \n",
            "                                                aten::t         0.02%       8.000us         0.04%      16.000us      16.000us             1            --  \n",
            "                                        aten::transpose         0.01%       6.000us         0.02%       8.000us       8.000us             1            --  \n",
            "                                       aten::as_strided         0.00%       2.000us         0.00%       2.000us       2.000us             1            --  \n",
            "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.07%      30.000us         0.07%      30.000us      30.000us             1            --  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 44.267ms\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2E\n",
        "\n",
        "# Train MHIST using ResNet18 for comparison\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    \"dis_metric\": \"cos\",\n",
        "    \"lr_net\": 0.01,\n",
        "    \"epoch_eval_train\": 20,\n",
        "    \"batch_train\": 32,\n",
        "    \"training_baseline\": True,\n",
        "    \"dataset\": \"MHIST Custom\"\n",
        "}\n",
        "\n",
        "args.update(args_dict)\n",
        "\n",
        "# Train MHIST using ResNet18 to compare performance\n",
        "\n",
        "# set im_size to 32, 32 before running\n",
        "\n",
        "mhist_path = \"/content/drive/MyDrive/ECE1512/Project_B/MHIST Dataset/\"\n",
        "\n",
        "MHIST_dataset = get_dataset(\"MHIST Custom\", mhist_path)\n",
        "channel_mhist = MHIST_dataset[0]\n",
        "im_size_mhist = MHIST_dataset[1]\n",
        "num_classes_mhist = MHIST_dataset[2]\n",
        "class_names_mhist = MHIST_dataset[3]\n",
        "mean_mhist = MHIST_dataset[4]\n",
        "std_mhist = MHIST_dataset[5]\n",
        "dst_train_mhist = MHIST_dataset[6]\n",
        "dst_test_mhist = MHIST_dataset[7]\n",
        "testloader_mhist = MHIST_dataset[8]\n",
        "\n",
        "mhist_network = get_network(\"ResNet18\", channel_mhist, num_classes_mhist, im_size_mhist)\n",
        "\n",
        "evaluate_synset(1, mhist_network, dst_train_mhist, dst_train_mhist.targets, testloader_mhist, args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8D4BVi0t29X",
        "outputId": "31216c6d-d885-4983-fae9-b70109f78da1"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(977, 3, 32, 32)\n",
            "[2023-12-11 02:44:10] Evaluate_01: epoch = 0020 train time = 5 s train loss = 0.723770 train acc = 0.5000, test acc = 0.3685\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(ResNet(\n",
              "   (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "   (bn1): GroupNorm(64, 64, eps=1e-05, affine=True)\n",
              "   (layer1): Sequential(\n",
              "     (0): BasicBlock(\n",
              "       (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn1): GroupNorm(64, 64, eps=1e-05, affine=True)\n",
              "       (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): GroupNorm(64, 64, eps=1e-05, affine=True)\n",
              "       (shortcut): Sequential()\n",
              "     )\n",
              "     (1): BasicBlock(\n",
              "       (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn1): GroupNorm(64, 64, eps=1e-05, affine=True)\n",
              "       (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): GroupNorm(64, 64, eps=1e-05, affine=True)\n",
              "       (shortcut): Sequential()\n",
              "     )\n",
              "   )\n",
              "   (layer2): Sequential(\n",
              "     (0): BasicBlock(\n",
              "       (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "       (bn1): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
              "       (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
              "       (shortcut): Sequential(\n",
              "         (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "         (1): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
              "       )\n",
              "     )\n",
              "     (1): BasicBlock(\n",
              "       (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn1): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
              "       (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
              "       (shortcut): Sequential()\n",
              "     )\n",
              "   )\n",
              "   (layer3): Sequential(\n",
              "     (0): BasicBlock(\n",
              "       (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "       (bn1): GroupNorm(256, 256, eps=1e-05, affine=True)\n",
              "       (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): GroupNorm(256, 256, eps=1e-05, affine=True)\n",
              "       (shortcut): Sequential(\n",
              "         (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "         (1): GroupNorm(256, 256, eps=1e-05, affine=True)\n",
              "       )\n",
              "     )\n",
              "     (1): BasicBlock(\n",
              "       (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn1): GroupNorm(256, 256, eps=1e-05, affine=True)\n",
              "       (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): GroupNorm(256, 256, eps=1e-05, affine=True)\n",
              "       (shortcut): Sequential()\n",
              "     )\n",
              "   )\n",
              "   (layer4): Sequential(\n",
              "     (0): BasicBlock(\n",
              "       (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "       (bn1): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
              "       (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
              "       (shortcut): Sequential(\n",
              "         (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "         (1): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
              "       )\n",
              "     )\n",
              "     (1): BasicBlock(\n",
              "       (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn1): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
              "       (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
              "       (shortcut): Sequential()\n",
              "     )\n",
              "   )\n",
              "   (classifier): Linear(in_features=512, out_features=2, bias=True)\n",
              " ),\n",
              " 0.5,\n",
              " 0.368474923234391)"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RESNET MHIST FLOPS\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    \"dis_metric\": \"cos\",\n",
        "    \"lr_net\": 0.01,\n",
        "    \"epoch_eval_train\": 20,\n",
        "    \"batch_train\": 32,\n",
        "    \"training_baseline\": True,\n",
        "    \"dataset\": \"MHIST Custom\"\n",
        "}\n",
        "\n",
        "args.update(args_dict)\n",
        "\n",
        "# Train MHIST using ResNet18 to compare performance\n",
        "\n",
        "mhist_path = \"/content/drive/MyDrive/ECE1512/Project_B/MHIST Dataset/\"\n",
        "\n",
        "MHIST_dataset = get_dataset(\"MHIST Custom\", mhist_path)\n",
        "channel_mhist = MHIST_dataset[0]\n",
        "im_size_mhist = MHIST_dataset[1]\n",
        "num_classes_mhist = MHIST_dataset[2]\n",
        "class_names_mhist = MHIST_dataset[3]\n",
        "mean_mhist = MHIST_dataset[4]\n",
        "std_mhist = MHIST_dataset[5]\n",
        "dst_train_mhist = MHIST_dataset[6]\n",
        "dst_test_mhist = MHIST_dataset[7]\n",
        "testloader_mhist = MHIST_dataset[8]\n",
        "\n",
        "mhist_network = get_network(\"ResNet18\", channel_mhist, num_classes_mhist, im_size_mhist)\n",
        "\n",
        "# Create dummy input\n",
        "dummy_input = torch.randn(1, 3, 32, 32).to('cuda')\n",
        "\n",
        "# Use profiler to record FLOPS\n",
        "with profile(activities=[ProfilerActivity.CPU], record_shapes=True, with_flops = True) as prof:\n",
        "    with record_function(\"model_inference\"):\n",
        "        mhist_network(dummy_input)\n",
        "\n",
        "# Print the FLOPS\n",
        "print(prof.key_averages().table(sort_by=\"flops\", row_limit=-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GUIXhpd4iP6",
        "outputId": "58209024-f851-42e8-ff34-48d4a420cd4a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(977, 3, 128, 128)\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  Total KFLOPs  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                           aten::conv2d         0.76%     264.000us        83.32%      29.015ms       1.451ms            20   1110835.200  \n",
            "                                            aten::addmm         0.49%     172.000us         0.60%     208.000us     208.000us             1         2.048  \n",
            "                                        model_inference         6.04%       2.104ms       100.00%      34.823ms      34.823ms             1            --  \n",
            "                                      aten::convolution         0.70%     244.000us        83.09%      28.936ms       1.447ms            20            --  \n",
            "                                     aten::_convolution         0.61%     211.000us        82.39%      28.692ms       1.435ms            20            --  \n",
            "                                aten::cudnn_convolution        79.10%      27.545ms        81.79%      28.481ms       1.424ms            20            --  \n",
            "                                  cudaStreamIsCapturing         0.10%      34.000us         0.10%      34.000us       1.619us            21            --  \n",
            "                                  cudaStreamGetPriority         0.00%       1.000us         0.00%       1.000us       0.050us            20            --  \n",
            "                       cudaDeviceGetStreamPriorityRange         0.00%       1.000us         0.00%       1.000us       0.050us            20            --  \n",
            "                                       cudaLaunchKernel         2.79%     970.000us         2.79%     970.000us       7.760us           125            --  \n",
            "                                       aten::group_norm         0.36%     127.000us         7.13%       2.484ms     124.200us            20            --  \n",
            "                                aten::native_group_norm         3.90%       1.359ms         6.77%       2.357ms     117.850us            20            --  \n",
            "                                            aten::empty         1.61%     559.000us         1.61%     559.000us       5.590us           100            --  \n",
            "                                             aten::view         0.11%      38.000us         0.11%      38.000us       0.469us            81            --  \n",
            "                                             aten::relu         0.53%     184.000us         1.44%     503.000us      29.588us            17            --  \n",
            "                                        aten::clamp_min         0.65%     228.000us         0.92%     319.000us      18.765us            17            --  \n",
            "                                   cudaFuncSetAttribute         0.25%      86.000us         0.25%      86.000us       2.529us            34            --  \n",
            "                                    cudaLaunchKernelExC         0.34%     118.000us         0.34%     118.000us       6.941us            17            --  \n",
            "                                             aten::add_         0.55%     192.000us         0.69%     240.000us      30.000us             8            --  \n",
            "                                             cudaMalloc         0.73%     253.000us         0.73%     253.000us     253.000us             1            --  \n",
            "                                        cudaMemsetAsync         0.13%      44.000us         0.13%      44.000us       6.286us             7            --  \n",
            "                                       aten::avg_pool2d         0.12%      41.000us         0.14%      49.000us      49.000us             1            --  \n",
            "                                           aten::linear         0.04%      13.000us         0.68%     236.000us     236.000us             1            --  \n",
            "                                                aten::t         0.02%       8.000us         0.04%      15.000us      15.000us             1            --  \n",
            "                                        aten::transpose         0.01%       5.000us         0.02%       7.000us       7.000us             1            --  \n",
            "                                       aten::as_strided         0.01%       2.000us         0.01%       2.000us       2.000us             1            --  \n",
            "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.06%      20.000us         0.06%      20.000us      20.000us             1            --  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 34.823ms\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3 Cross architecture performance\n",
        "\n",
        "# Train MNIST using MLP for comparison\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    \"dis_metric\": \"cos\",\n",
        "    \"lr_net\": 0.01,\n",
        "    \"epoch_eval_train\": 20,\n",
        "    \"batch_train\": 32,\n",
        "    \"training_baseline\": True,\n",
        "    \"dataset\": \"MNIST Custom\"\n",
        "}\n",
        "\n",
        "args.update(args_dict)\n",
        "\n",
        "# train MNIST\n",
        "\n",
        "mnist_path = \"/content/drive/MyDrive/ECE1512/Project_B/MNIST Dataset/\"\n",
        "\n",
        "MNIST_dataset = get_dataset(\"MNIST Custom\", mnist_path)\n",
        "channel_mnist = MNIST_dataset[0]\n",
        "im_size_mnist = MNIST_dataset[1]\n",
        "num_classes_mnist = MNIST_dataset[2]\n",
        "class_names_mnist = MNIST_dataset[3]\n",
        "mean_mnist = MNIST_dataset[4]\n",
        "std_mnist = MNIST_dataset[5]\n",
        "dst_train_mnist = MNIST_dataset[6]\n",
        "dst_test_mnist = MNIST_dataset[7]\n",
        "testloader_mnist = MNIST_dataset[8]\n",
        "\n",
        "mnist_network = get_network(\"MLP\", channel_mnist, num_classes_mnist, im_size_mnist)\n",
        "\n",
        "evaluate_synset(1, mnist_network, dst_train_mnist, dst_train_mnist.targets, testloader_mnist, args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Utqjo793yAMv",
        "outputId": "538b26a5-8c10-41c0-f423-33bbc615b0bc"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-12-11 02:50:44] Evaluate_01: epoch = 0020 train time = 2 s train loss = 0.176212 train acc = 0.9800, test acc = 0.7492\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(MLP(\n",
              "   (fc_1): Linear(in_features=784, out_features=128, bias=True)\n",
              "   (fc_2): Linear(in_features=128, out_features=128, bias=True)\n",
              "   (fc_3): Linear(in_features=128, out_features=10, bias=True)\n",
              " ),\n",
              " 0.98,\n",
              " 0.7492)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#MLP MNIST FLOPS\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    \"dis_metric\": \"cos\",\n",
        "    \"lr_net\": 0.01,\n",
        "    \"epoch_eval_train\": 20,\n",
        "    \"batch_train\": 32,\n",
        "    \"training_baseline\": True,\n",
        "    \"dataset\": \"MNIST Custom\"\n",
        "}\n",
        "\n",
        "args.update(args_dict)\n",
        "\n",
        "# train MNIST\n",
        "\n",
        "mnist_path = \"/content/drive/MyDrive/ECE1512/Project_B/MNIST Dataset/\"\n",
        "\n",
        "MNIST_dataset = get_dataset(\"MNIST Custom\", mnist_path)\n",
        "channel_mnist = MNIST_dataset[0]\n",
        "im_size_mnist = MNIST_dataset[1]\n",
        "num_classes_mnist = MNIST_dataset[2]\n",
        "class_names_mnist = MNIST_dataset[3]\n",
        "mean_mnist = MNIST_dataset[4]\n",
        "std_mnist = MNIST_dataset[5]\n",
        "dst_train_mnist = MNIST_dataset[6]\n",
        "dst_test_mnist = MNIST_dataset[7]\n",
        "testloader_mnist = MNIST_dataset[8]\n",
        "\n",
        "mnist_network = get_network(\"MLP\", channel_mnist, num_classes_mnist, im_size_mnist)\n",
        "\n",
        "# Create dummy input\n",
        "dummy_input = torch.randn(1, 1, 28, 28).to('cuda')\n",
        "\n",
        "# Use profiler to record FLOPS\n",
        "with profile(activities=[ProfilerActivity.CPU], record_shapes=True, with_flops = True) as prof:\n",
        "    with record_function(\"model_inference\"):\n",
        "        mnist_network(dummy_input)\n",
        "\n",
        "# Print the FLOPS\n",
        "print(prof.key_averages().table(sort_by=\"flops\", row_limit=-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPU2hKAF4ygH",
        "outputId": "3cfdd15d-a6ba-4923-91ac-7a97450d75e9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  Total KFLOPs  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                            aten::addmm        54.48%       1.010ms        64.46%       1.195ms     398.333us             3       236.032  \n",
            "                                        model_inference        19.69%     365.000us       100.00%       1.854ms       1.854ms             1            --  \n",
            "                                             aten::view         0.54%      10.000us         0.54%      10.000us      10.000us             1            --  \n",
            "                                           aten::linear         3.07%      57.000us        72.44%       1.343ms     447.667us             3            --  \n",
            "                                                aten::t         2.64%      49.000us         4.91%      91.000us      30.333us             3            --  \n",
            "                                        aten::transpose         1.02%      19.000us         2.27%      42.000us      14.000us             3            --  \n",
            "                                       aten::as_strided         1.24%      23.000us         1.24%      23.000us       7.667us             3            --  \n",
            "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         2.10%      39.000us         2.10%      39.000us      13.000us             3            --  \n",
            "                                       cudaLaunchKernel         9.01%     167.000us         9.01%     167.000us      33.400us             5            --  \n",
            "                                             aten::relu         1.67%      31.000us         7.34%     136.000us      68.000us             2            --  \n",
            "                                        aten::clamp_min         4.53%      84.000us         5.66%     105.000us      52.500us             2            --  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 1.854ms\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    \"dis_metric\": \"cos\",\n",
        "    \"lr_net\": 0.01,\n",
        "    \"epoch_eval_train\": 20,\n",
        "    \"batch_train\": 32,\n",
        "    \"training_baseline\": True,\n",
        "    \"dataset\": \"MHIST Custom\"\n",
        "}\n",
        "\n",
        "args.update(args_dict)\n",
        "\n",
        "# Train MHIST using ResNet18 to compare performance\n",
        "\n",
        "# set im_size to 32, 32 before running\n",
        "\n",
        "mhist_path = \"/content/drive/MyDrive/ECE1512/Project_B/MHIST Dataset/\"\n",
        "\n",
        "MHIST_dataset = get_dataset(\"MHIST Custom\", mhist_path)\n",
        "channel_mhist = MHIST_dataset[0]\n",
        "im_size_mhist = MHIST_dataset[1]\n",
        "num_classes_mhist = MHIST_dataset[2]\n",
        "class_names_mhist = MHIST_dataset[3]\n",
        "mean_mhist = MHIST_dataset[4]\n",
        "std_mhist = MHIST_dataset[5]\n",
        "dst_train_mhist = MHIST_dataset[6]\n",
        "dst_test_mhist = MHIST_dataset[7]\n",
        "testloader_mhist = MHIST_dataset[8]\n",
        "\n",
        "mhist_network = get_network(\"MLP\", channel_mhist, num_classes_mhist, im_size_mhist)\n",
        "\n",
        "evaluate_synset(1, mhist_network, dst_train_mhist, dst_train_mhist.targets, testloader_mhist, args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVl72iZLyRIu",
        "outputId": "07e188ac-b685-4556-c02e-c3fa2385afd5"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(977, 3, 32, 32)\n",
            "[2023-12-11 02:51:24] Evaluate_01: epoch = 0020 train time = 3 s train loss = 0.693711 train acc = 0.5000, test acc = 0.3685\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(MLP(\n",
              "   (fc_1): Linear(in_features=3072, out_features=128, bias=True)\n",
              "   (fc_2): Linear(in_features=128, out_features=128, bias=True)\n",
              "   (fc_3): Linear(in_features=128, out_features=2, bias=True)\n",
              " ),\n",
              " 0.5,\n",
              " 0.368474923234391)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#MLP MHIST FLOPS\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    \"dis_metric\": \"cos\",\n",
        "    \"lr_net\": 0.01,\n",
        "    \"epoch_eval_train\": 20,\n",
        "    \"batch_train\": 32,\n",
        "    \"training_baseline\": True,\n",
        "    \"dataset\": \"MHIST Custom\"\n",
        "}\n",
        "\n",
        "args.update(args_dict)\n",
        "\n",
        "# Train MHIST using ResNet18 to compare performance\n",
        "\n",
        "# set im_size to 32, 32 before running\n",
        "\n",
        "mhist_path = \"/content/drive/MyDrive/ECE1512/Project_B/MHIST Dataset/\"\n",
        "\n",
        "MHIST_dataset = get_dataset(\"MHIST Custom\", mhist_path)\n",
        "channel_mhist = MHIST_dataset[0]\n",
        "im_size_mhist = MHIST_dataset[1]\n",
        "num_classes_mhist = MHIST_dataset[2]\n",
        "class_names_mhist = MHIST_dataset[3]\n",
        "mean_mhist = MHIST_dataset[4]\n",
        "std_mhist = MHIST_dataset[5]\n",
        "dst_train_mhist = MHIST_dataset[6]\n",
        "dst_test_mhist = MHIST_dataset[7]\n",
        "testloader_mhist = MHIST_dataset[8]\n",
        "\n",
        "mhist_network = get_network(\"MLP\", channel_mhist, num_classes_mhist, im_size_mhist)\n",
        "\n",
        "# Create dummy input\n",
        "dummy_input = torch.randn(1, 3, 32, 32).to('cuda')\n",
        "\n",
        "# Use profiler to record FLOPS\n",
        "with profile(activities=[ProfilerActivity.CPU], record_shapes=True, with_flops = True) as prof:\n",
        "    with record_function(\"model_inference\"):\n",
        "        mhist_network(dummy_input)\n",
        "\n",
        "# Print the FLOPS\n",
        "print(prof.key_averages().table(sort_by=\"flops\", row_limit=-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vViLQuZC5Ibr",
        "outputId": "64410b7f-8c80-4338-cb5e-0e7b9a893e10"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(977, 3, 128, 128)\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  Total KFLOPs  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                            aten::addmm        54.43%     921.000us        61.11%       1.034ms     344.667us             3       819.712  \n",
            "                                        model_inference        23.17%     392.000us       100.00%       1.692ms       1.692ms             1            --  \n",
            "                                             aten::view         0.89%      15.000us         0.89%      15.000us      15.000us             1            --  \n",
            "                                           aten::linear         2.72%      46.000us        70.80%       1.198ms     399.333us             3            --  \n",
            "                                                aten::t         3.78%      64.000us         6.97%     118.000us      39.333us             3            --  \n",
            "                                        aten::transpose         1.48%      25.000us         3.19%      54.000us      18.000us             3            --  \n",
            "                                       aten::as_strided         1.71%      29.000us         1.71%      29.000us       9.667us             3            --  \n",
            "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.59%      10.000us         0.59%      10.000us       3.333us             3            --  \n",
            "                                       cudaLaunchKernel         7.15%     121.000us         7.15%     121.000us      24.200us             5            --  \n",
            "                                             aten::relu         1.60%      27.000us         5.14%      87.000us      43.500us             2            --  \n",
            "                                        aten::clamp_min         2.48%      42.000us         3.55%      60.000us      30.000us             2            --  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 1.692ms\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3 Cross architecture performance\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    \"dis_metric\": \"cos\",\n",
        "    \"lr_net\": 0.01,\n",
        "    \"epoch_eval_train\": 20,\n",
        "    \"batch_train\": 32,\n",
        "    \"training_baseline\": True,\n",
        "    \"dataset\": \"MNIST Custom\"\n",
        "}\n",
        "\n",
        "args.update(args_dict)\n",
        "\n",
        "# train MNIST\n",
        "\n",
        "mnist_path = \"/content/drive/MyDrive/ECE1512/Project_B/MNIST Dataset/\"\n",
        "\n",
        "MNIST_dataset = get_dataset(\"MNIST Custom\", mnist_path)\n",
        "channel_mnist = MNIST_dataset[0]\n",
        "im_size_mnist = MNIST_dataset[1]\n",
        "num_classes_mnist = MNIST_dataset[2]\n",
        "class_names_mnist = MNIST_dataset[3]\n",
        "mean_mnist = MNIST_dataset[4]\n",
        "std_mnist = MNIST_dataset[5]\n",
        "dst_train_mnist = MNIST_dataset[6]\n",
        "dst_test_mnist = MNIST_dataset[7]\n",
        "testloader_mnist = MNIST_dataset[8]\n",
        "\n",
        "mnist_network = get_network(\"ConvNetD4\", channel_mnist, num_classes_mnist, im_size_mnist)\n",
        "\n",
        "evaluate_synset(1, mnist_network, dst_train_mnist, dst_train_mnist.targets, testloader_mnist, args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZQ0b6VryrBN",
        "outputId": "2d0f5f5f-1d33-47cd-fa15-7a6758d36015"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-12-11 02:53:11] Evaluate_01: epoch = 0020 train time = 3 s train loss = 0.134468 train acc = 1.0000, test acc = 0.8930\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(ConvNet(\n",
              "   (features): Sequential(\n",
              "     (0): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3))\n",
              "     (1): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
              "     (2): ReLU(inplace=True)\n",
              "     (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "     (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "     (5): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
              "     (6): ReLU(inplace=True)\n",
              "     (7): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "     (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "     (9): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
              "     (10): ReLU(inplace=True)\n",
              "     (11): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "     (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "     (13): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
              "     (14): ReLU(inplace=True)\n",
              "     (15): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "   )\n",
              "   (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
              " ),\n",
              " 1.0,\n",
              " 0.893)"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3 Cross architecture performance\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    \"dis_metric\": \"cos\",\n",
        "    \"lr_net\": 0.01,\n",
        "    \"epoch_eval_train\": 20,\n",
        "    \"batch_train\": 32,\n",
        "    \"training_baseline\": True,\n",
        "    \"dataset\": \"MNIST Custom\"\n",
        "}\n",
        "\n",
        "args.update(args_dict)\n",
        "\n",
        "# train MNIST\n",
        "\n",
        "mnist_path = \"/content/drive/MyDrive/ECE1512/Project_B/MNIST Dataset/\"\n",
        "\n",
        "MNIST_dataset = get_dataset(\"MNIST Custom\", mnist_path)\n",
        "channel_mnist = MNIST_dataset[0]\n",
        "im_size_mnist = MNIST_dataset[1]\n",
        "num_classes_mnist = MNIST_dataset[2]\n",
        "class_names_mnist = MNIST_dataset[3]\n",
        "mean_mnist = MNIST_dataset[4]\n",
        "std_mnist = MNIST_dataset[5]\n",
        "dst_train_mnist = MNIST_dataset[6]\n",
        "dst_test_mnist = MNIST_dataset[7]\n",
        "testloader_mnist = MNIST_dataset[8]\n",
        "\n",
        "mnist_network = get_network(\"ConvNetD4\", channel_mnist, num_classes_mnist, im_size_mnist)\n",
        "\n",
        "# Create dummy input\n",
        "dummy_input = torch.randn(1, 1, 28, 28).to('cuda')\n",
        "\n",
        "# Use profiler to record FLOPS\n",
        "with profile(activities=[ProfilerActivity.CPU], record_shapes=True, with_flops = True) as prof:\n",
        "    with record_function(\"model_inference\"):\n",
        "        mnist_network(dummy_input)\n",
        "\n",
        "# Print the FLOPS\n",
        "print(prof.key_averages().table(sort_by=\"flops\", row_limit=-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjiMNye65lF2",
        "outputId": "3cfdf85b-8bf4-44c8-d864-3a48553aa1ca"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  Total KFLOPs  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                           aten::conv2d         0.32%      27.000us        79.64%       6.736ms       1.684ms             4    101449.728  \n",
            "                                            aten::addmm         2.00%     169.000us         2.42%     205.000us     205.000us             1        10.240  \n",
            "                                        model_inference         8.38%     709.000us       100.00%       8.458ms       8.458ms             1            --  \n",
            "                                      aten::convolution         1.56%     132.000us        79.32%       6.709ms       1.677ms             4            --  \n",
            "                                     aten::_convolution         1.74%     147.000us        77.76%       6.577ms       1.644ms             4            --  \n",
            "                                aten::cudnn_convolution        71.34%       6.034ms        74.14%       6.271ms       1.568ms             4            --  \n",
            "                                  cudaStreamIsCapturing         0.05%       4.000us         0.05%       4.000us       1.000us             4            --  \n",
            "                                  cudaStreamGetPriority         0.01%       1.000us         0.01%       1.000us       0.250us             4            --  \n",
            "                       cudaDeviceGetStreamPriorityRange         0.01%       1.000us         0.01%       1.000us       0.250us             4            --  \n",
            "                                       cudaLaunchKernel         4.06%     343.000us         4.06%     343.000us      10.088us            34            --  \n",
            "                                        cudaMemsetAsync         0.00%       0.000us         0.00%       0.000us       0.000us             1            --  \n",
            "                                          aten::reshape         0.14%      12.000us         0.22%      19.000us       4.750us             4            --  \n",
            "                                             aten::view         0.22%      19.000us         0.22%      19.000us       0.905us            21            --  \n",
            "                                             aten::add_         1.22%     103.000us         1.63%     138.000us      34.500us             4            --  \n",
            "                                       aten::group_norm         0.53%      45.000us         6.12%     518.000us     129.500us             4            --  \n",
            "                                aten::native_group_norm         3.41%     288.000us         5.59%     473.000us     118.250us             4            --  \n",
            "                                            aten::empty         1.25%     106.000us         1.25%     106.000us       5.300us            20            --  \n",
            "                                            aten::relu_         0.74%      63.000us         1.49%     126.000us      31.500us             4            --  \n",
            "                                       aten::clamp_min_         0.46%      39.000us         0.74%      63.000us      15.750us             4            --  \n",
            "                                       aten::avg_pool2d         0.98%      83.000us         1.28%     108.000us      27.000us             4            --  \n",
            "                                   cudaFuncSetAttribute         0.30%      25.000us         0.30%      25.000us       4.167us             6            --  \n",
            "                                    cudaLaunchKernelExC         0.45%      38.000us         0.45%      38.000us      12.667us             3            --  \n",
            "                                           aten::linear         0.40%      34.000us         3.01%     255.000us     255.000us             1            --  \n",
            "                                                aten::t         0.09%       8.000us         0.19%      16.000us      16.000us             1            --  \n",
            "                                        aten::transpose         0.07%       6.000us         0.09%       8.000us       8.000us             1            --  \n",
            "                                       aten::as_strided         0.02%       2.000us         0.02%       2.000us       2.000us             1            --  \n",
            "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.24%      20.000us         0.24%      20.000us      20.000us             1            --  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 8.458ms\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3D\n",
        "\n",
        "# Train MHIST using ConvNet 4 for comparison\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    \"dis_metric\": \"cos\",\n",
        "    \"lr_net\": 0.01,\n",
        "    \"epoch_eval_train\": 20,\n",
        "    \"batch_train\": 32,\n",
        "    \"training_baseline\": True,\n",
        "    \"dataset\": \"MHIST Custom\"\n",
        "}\n",
        "\n",
        "args.update(args_dict)\n",
        "\n",
        "# Train MHIST using ResNet18 to compare performance\n",
        "\n",
        "# set im_size to 32, 32 before running\n",
        "\n",
        "mhist_path = \"/content/drive/MyDrive/ECE1512/Project_B/MHIST Dataset/\"\n",
        "\n",
        "MHIST_dataset = get_dataset(\"MHIST Custom\", mhist_path)\n",
        "channel_mhist = MHIST_dataset[0]\n",
        "im_size_mhist = MHIST_dataset[1]\n",
        "num_classes_mhist = MHIST_dataset[2]\n",
        "class_names_mhist = MHIST_dataset[3]\n",
        "mean_mhist = MHIST_dataset[4]\n",
        "std_mhist = MHIST_dataset[5]\n",
        "dst_train_mhist = MHIST_dataset[6]\n",
        "dst_test_mhist = MHIST_dataset[7]\n",
        "testloader_mhist = MHIST_dataset[8]\n",
        "\n",
        "mhist_network = get_network(\"ConvNetD4\", channel_mhist, num_classes_mhist, im_size_mhist)\n",
        "\n",
        "evaluate_synset(1, mhist_network, dst_train_mhist, dst_train_mhist.targets, testloader_mhist, args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INPnnpVDyZwO",
        "outputId": "736f6a70-9c28-4f1c-ab90-b83d03da1b49"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(977, 3, 128, 128)\n",
            "[2023-12-11 02:52:34] Evaluate_01: epoch = 0020 train time = 5 s train loss = 0.705282 train acc = 0.5000, test acc = 0.4964\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(ConvNet(\n",
              "   (features): Sequential(\n",
              "     (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "     (1): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
              "     (2): ReLU(inplace=True)\n",
              "     (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "     (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "     (5): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
              "     (6): ReLU(inplace=True)\n",
              "     (7): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "     (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "     (9): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
              "     (10): ReLU(inplace=True)\n",
              "     (11): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "     (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "     (13): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
              "     (14): ReLU(inplace=True)\n",
              "     (15): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "   )\n",
              "   (classifier): Linear(in_features=8192, out_features=2, bias=True)\n",
              " ),\n",
              " 0.5,\n",
              " 0.49641760491299897)"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    \"dis_metric\": \"cos\",\n",
        "    \"lr_net\": 0.01,\n",
        "    \"epoch_eval_train\": 20,\n",
        "    \"batch_train\": 32,\n",
        "    \"training_baseline\": True,\n",
        "    \"dataset\": \"MHIST Custom\"\n",
        "}\n",
        "\n",
        "args.update(args_dict)\n",
        "\n",
        "# Train MHIST using ResNet18 to compare performance\n",
        "\n",
        "# set im_size to 32, 32 before running\n",
        "\n",
        "mhist_path = \"/content/drive/MyDrive/ECE1512/Project_B/MHIST Dataset/\"\n",
        "\n",
        "MHIST_dataset = get_dataset(\"MHIST Custom\", mhist_path)\n",
        "channel_mhist = MHIST_dataset[0]\n",
        "im_size_mhist = MHIST_dataset[1]\n",
        "num_classes_mhist = MHIST_dataset[2]\n",
        "class_names_mhist = MHIST_dataset[3]\n",
        "mean_mhist = MHIST_dataset[4]\n",
        "std_mhist = MHIST_dataset[5]\n",
        "dst_train_mhist = MHIST_dataset[6]\n",
        "dst_test_mhist = MHIST_dataset[7]\n",
        "testloader_mhist = MHIST_dataset[8]\n",
        "\n",
        "mhist_network = get_network(\"ConvNetD4\", channel_mhist, num_classes_mhist, im_size_mhist)\n",
        "\n",
        "# Create dummy input\n",
        "dummy_input = torch.randn(1, 3, 128, 128).to('cuda')\n",
        "\n",
        "# Use profiler to record FLOPS\n",
        "with profile(activities=[ProfilerActivity.CPU], record_shapes=True, with_flops = True) as prof:\n",
        "    with record_function(\"model_inference\"):\n",
        "        mhist_network(dummy_input)\n",
        "\n",
        "# Print the FLOPS\n",
        "print(prof.key_averages().table(sort_by=\"flops\", row_limit=-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xzgDPYB5sxX",
        "outputId": "3fa724ea-d4b4-4c66-d891-07ef35cdc808"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(977, 3, 128, 128)\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  Total KFLOPs  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                           aten::conv2d         0.53%      21.000us        53.72%       2.138ms     534.500us             4   1698693.120  \n",
            "                                            aten::addmm         5.90%     235.000us         6.96%     277.000us     277.000us             1        32.768  \n",
            "                                        model_inference        17.91%     713.000us       100.00%       3.980ms       3.980ms             1            --  \n",
            "                                      aten::convolution         1.76%      70.000us        53.19%       2.117ms     529.250us             4            --  \n",
            "                                     aten::_convolution         3.19%     127.000us        51.43%       2.047ms     511.750us             4            --  \n",
            "                                aten::cudnn_convolution        38.92%       1.549ms        44.72%       1.780ms     445.000us             4            --  \n",
            "                                  cudaStreamIsCapturing         0.13%       5.000us         0.13%       5.000us       1.250us             4            --  \n",
            "                                  cudaStreamGetPriority         0.03%       1.000us         0.03%       1.000us       0.250us             4            --  \n",
            "                       cudaDeviceGetStreamPriorityRange         0.03%       1.000us         0.03%       1.000us       0.250us             4            --  \n",
            "                                       cudaLaunchKernel         8.44%     336.000us         8.44%     336.000us       9.600us            35            --  \n",
            "                                          aten::reshape         0.28%      11.000us         0.45%      18.000us       4.500us             4            --  \n",
            "                                             aten::view         0.50%      20.000us         0.50%      20.000us       0.952us            21            --  \n",
            "                                             aten::add_         2.36%      94.000us         3.07%     122.000us      30.500us             4            --  \n",
            "                                       aten::group_norm         1.01%      40.000us        13.42%     534.000us     133.500us             4            --  \n",
            "                                aten::native_group_norm         7.41%     295.000us        12.41%     494.000us     123.500us             4            --  \n",
            "                                            aten::empty         3.02%     120.000us         3.02%     120.000us       6.000us            20            --  \n",
            "                                            aten::relu_         1.46%      58.000us         2.99%     119.000us      29.750us             4            --  \n",
            "                                       aten::clamp_min_         0.93%      37.000us         1.53%      61.000us      15.250us             4            --  \n",
            "                                       aten::avg_pool2d         3.44%     137.000us         4.10%     163.000us      40.750us             4            --  \n",
            "                                   cudaFuncSetAttribute         0.63%      25.000us         0.63%      25.000us       4.167us             6            --  \n",
            "                                    cudaLaunchKernelExC         0.90%      36.000us         0.90%      36.000us      12.000us             3            --  \n",
            "                                           aten::linear         0.33%      13.000us         7.69%     306.000us     306.000us             1            --  \n",
            "                                                aten::t         0.20%       8.000us         0.40%      16.000us      16.000us             1            --  \n",
            "                                        aten::transpose         0.15%       6.000us         0.20%       8.000us       8.000us             1            --  \n",
            "                                       aten::as_strided         0.05%       2.000us         0.05%       2.000us       2.000us             1            --  \n",
            "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.50%      20.000us         0.50%      20.000us      20.000us             1            --  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 3.980ms\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# APPLICATION\n",
        "\n",
        "# See separate application notebook"
      ],
      "metadata": {
        "id": "V5ygzvotzYAF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}